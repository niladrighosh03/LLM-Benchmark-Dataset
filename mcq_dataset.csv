pdf_name,chunk_number,total_chunks,pages,question,option_A,option_B,option_C,option_D,correct_answer
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",How does a large language model generate text?,By being trained on specific text to generate,By assigning probabilities to sequences of words and sampling possible next words,By learning to predict the next word only,By using a rule-based approach,B) By assigning probabilities to sequences of words and sampling possible next words
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",What type of knowledge does a large language model learn through pretraining?,Mathematical knowledge,Common sense knowledge,Visual recognition knowledge,Language knowledge,D) Language knowledge
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",Which method does a large language model use to learn a lot of useful language knowledge?,By being fine-tuned on specific tasks,By being trained on a lot of text,By using a rule-based approach,By using a neural network,B) By being trained on a lot of text
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which type of language model is referred to as 'autoregressive' or 'left-to-right'?,Decoder-only model,Encoder-only model,Encoder-decoder model,Transformer model,A) Decoder-only model
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which language model type gets bidirectional context and can condition on future words?,Decoder-only model,Encoder-only model,Encoder-decoder model,Transformer model,B) Encoder-only model
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which term is used to describe decoder-only models in the context of language models?,Causal language models,Autoregressive language models,Left-to-right language models,Encoder-decoder language models,A) Causal language models
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which language model type is best suited for generating text in a left-to-right manner?,Decoder-only model,Encoder-only model,Encoder-decoder model,Transformer model,A) Decoder-only model
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which model type is used for generating text but cannot condition on future words?,Decoder-only model,Encoder-only model,Transformer model,Recurrent neural network,A) Decoder-only model
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which model type gets bidirectional context and can condition on future words?,Decoder-only model,Encoder-only model,Transformer model,Encoder-decoder model,B) Encoder-only model
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which model type is popular for machine translation tasks?,Decoder-only model,Encoder-only model,Transformer model,Encoder-decoder model,D) Encoder-decoder model
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which model type is used to train models by predicting words from surrounding words on both sides?,Decoder-only model,Encoder-only model,Masked Language Models,BERT family,C) Masked Language Models
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Which model type is used for generating text left-to-right?,Decoder-only model,Encoder-only model,Transformer model,RNN model,A) Decoder-only model
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Which type of language model can't condition on future words?,Decoder-only model,Encoder-only model,Transformer model,RNN model,B) Encoder-only model
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Which model type gets bidirectional context and can condition on future?,Decoder-only model,Encoder-only model,Transformer model,RNN model,C) Transformer model
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9","Which term is another name for causal LLMs, autoregressive LLMs, and left-to-right LLMs?",Decoder-only models,Encoder-only models,Transformer models,Language models,D) Language models
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11","Given the sentence 'I like Jackie Chan', which word does a transformer-based language model predict comes next based on the context?",positive,negative,sentiment,summary,A) positive
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",Which task can be cast as word prediction in NLP?,Text summarization,Question answering,Sentiment analysis,Text generation,B) Question answering
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",What is the role of the encoder in an encoder-decoder model?,Generates words left-to-right,Gets bidirectional context,Conditions on future words,Builds strong representations,D) Builds strong representations
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",Which token is used to indicate that a summary should be generated in text summarization?,tl;dr,summary,question,answer,A) tl;dr
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13","Given the sentence 'I like Jackie Chan', which word is more probable to come next based on the sentiment analysis?",positive,negative,sentiment,Chan,A) positive
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",Which token should be added after 'Who wrote the book ‘‘The Origin of Species’' for question answering using a language model?,Charles,question,Darwin,answer,A) Charles
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",What is the role of the token 'tl;dr' in text summarization using a language model?,It is a token for suggesting an answer in question answering,It is a token for indicating the start of a long text summarization text,It is a token for indicating the end of a long text summarization text and requesting a shorter summary,It is a token for indicating the start of a question in question answering,C) It is a token for indicating the end of a long text summarization text and requesting a shorter summary
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",Which model architecture is used for modern language models?,RNN,CNN,Transformer,LSTM,C) Transformer
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",Which type of system is used for language modeling in modern approaches?,Stack of neural networks called transformers,Recurrent neural networks,Convolutional neural networks,Long Short-Term Memory networks,A) Stack of neural networks called transformers
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",Which term is used to describe the modern approach for language modeling?,Transformer-based language modeling,Recurrent neural network language modeling,Convolutional neural network language modeling,Long Short-Term Memory network language modeling,A) Transformer-based language modeling
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",Which neural network architecture is the foundation of modern language models?,Stack of neural networks called transformers,Recurrent neural networks,Convolutional neural networks,Long Short-Term Memory networks,A) Stack of neural networks called transformers
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",Which term is used to describe the large neural network models used for language modeling?,Transformers,Language models,Neural networks,Stacks of neural networks,B) Language models
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Who proposed the idea of replacing RNNs with self-attention in the Transformer architecture?,Ashish Vaswani,Noam Shazeer,Jakob Uszkoreit,Niki Parmar,A) Ashish Vaswani
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Which researchers designed and implemented the first Transformer models?,Ashish Vaswani and Illia Polosukhin,Noam Shazeer and Llion Jones,Jakob Uszkoreit and Niki Parmar,Lukasz Kaiser and Aidan Gomez,A) Ashish Vaswani and Illia Polosukhin
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Which attention mechanism was proposed by Noam Shazeer in the Transformer architecture?,Scaled dot-product attention,Multi-head attention,Parameter-free position representation,Self-attention,A) Scaled dot-product attention
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",In which year was the Transformer architecture proposed?,2015,2017,2018,2019,B) 2017
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which event marked the beginning of the use of GPUs in deep learning?,The discovery of Static Word Embeddings in 2013,The introduction of Multi-Task Learning in 2008,The emergence of GPUs in 2004,The development of Transformers in 2017,C) The emergence of GPUs in 2004
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19","Which technology was re-discovered in 2013, after its initial discovery in 1990?",Multi-Task Learning,Neural Language Model,Static Word Embeddings,Attention,C) Static Word Embeddings
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which technology was introduced after the emergence of Transformers in 2017?,Contextual Word Embeddings and Pretraining,Prompting,Stacked Transformer Blocks,Logits,A) Contextual Word Embeddings and Pretraining
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which component of a Transformer model processes the input tokens and generates the input encoding?,Language Modeling Head,Input Encoding,Stacked Transformer Blocks,Next token,B) Input Encoding
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21","Given the context of the lecture, which type of word embeddings are static?",Contextual embeddings,Static embeddings (word2vec),Transformer block embeddings,Input token embeddings,B) Static embeddings (word2vec)
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",Why is it important for word embeddings to reflect the meaning of a word in different contexts?,To make the model more complex,To improve the model's ability to understand the meaning of words,To reduce the number of input tokens,To make the model faster,B) To improve the model's ability to understand the meaning of words
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",Which method is used to compute contextual embeddings according to the lecture?,Static embedding lookup,Word2vec,Attention mechanism,Transformer blocks,C) Attention mechanism
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21","Consider the sentence 'The chicken didn't cross the road because it was too tired'. In the context of the lecture, which word's meaning is being represented in a static embedding?",Chicken,Road,Tired,Because,C) Tired
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27","Given the self-attention mechanism, which equation calculates the score between a current focus of attention and a preceding context element?","score(xi, xj) = xi·xj","aij = softmax(score(xi, xj))",ai = Xj·iaijxj,"score(xi, xj) = qi·kj","D) score(xi, xj) = qi·kj"
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27","In the self-attention mechanism, what is the role of the query vector qi?",It is used to project each input vector into its role as a value,It is used to compute the output for the current focus of attention,It is used to compare each input to all preceding inputs,It is used to normalize the scores to provide a probability distribution,C) It is used to compare each input to all preceding inputs
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",Which of the following equations calculates the output value ai for the current focus of attention?,ai = Xj·iaijxj,"aij = softmax(score(xi, xj))","score(xi, xj) = qi·kj",ai = Xjiaijvj,D) ai = Xjiaijvj
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27","In the self-attention mechanism, what is the dimensionality of the key and query vectors?","dk and dv are both 512, d is 1⇥d","dk and dv are both 64, d is 1⇥d","dk and dv are both 512, d is 1⇥dk","dk and dv are both 64, d is 1⇥dv","B) dk and dv are both 64, d is 1⇥d"
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29","Given the self-attention mechanism, which vector from the input sequence is used as the query in the dot product calculation with the key vectors?",Current focus element's key vector,Current focus element's value vector,Preceding input's key vector,Preceding input's value vector,C) Preceding input's key vector
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29","In the self-attention mechanism, which vectors are normalized using a softmax function to create a vector of weights?",Query vectors,Key vectors,Value vectors,Input vectors,B) Key vectors
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29",Which of the following vectors is used to compute the output for the current focus of attention in the self-attention mechanism?,Query vector,Key vector,Value vector,Softmax weight vector,C) Value vector
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29","In the self-attention mechanism, which vectors have the same dimensionality as the inputs and outputs of transformers?",Key and query vectors,Key and value vectors,Query and value vectors,Input and output vectors,A) Key and query vectors
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33","Given the attention head in transformers, which role does the query vector play during the attention process?",It represents the current element being compared to the preceding inputs.,It represents the preceding input being compared to the current element.,It is the weighted sum of the prior element's key and value.,It is the projection of each input vector into a representation of its role as a value.,A) It represents the current element being compared to the preceding inputs.
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",Which equation is used to compute the dot product between the query vector and the key vector in the attention process?,Eq.9.10,Eq.9.11,Eq.9.12,Eq.9.13,B) Eq.9.11
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",What is the purpose of scaling the dot product in the attention process by a factor related to the size of the embeddings?,To avoid numerical issues and loss of gradients during training.,To increase the similarity between the query and key vectors.,To decrease the dimensionality of the query and key vectors.,To normalize the dot product into a probability distribution.,A) To avoid numerical issues and loss of gradients during training.
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",Which equation is used to compute the output for the current element ai in the attention process?,Eq.9.9,Eq.9.10,Eq.9.11,Eq.9.13,D) Eq.9.13
notes/LLM_cs124_week7_2025.pdf,17,"33, 34, 35","Given the attention head in transformers, which role does the weight matrix WQ play?",It represents the current element being compared to the preceding inputs.,It represents the preceding input being compared to the current element to determine a similarity weight.,It projects each input vector into a representation of its role as a key.,It projects each input vector into a representation of its role as a value.,C) It projects each input vector into a representation of its role as a key.
notes/LLM_cs124_week7_2025.pdf,17,"33, 34, 35","In the attention process, which matrix is used to compute the dot product between the current element's query vector and the preceding element's key vector?",WK,WQ,WV,WVQ,A) WK
notes/LLM_cs124_week7_2025.pdf,17,"33, 34, 35",Which equation in the attention process involves the softmax calculation?,Eq.9.11,Eq.9.12,Eq.9.13,Eq.9.14,A) Eq.9.11
notes/LLM_cs124_week7_2025.pdf,17,"33, 34, 35",Which matrix is used to compute the weighted sum of the prior elements' value vectors?,WQ,WK,WV,WVQ,C) WV
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37","Given the transformer model, which matrix is used to reshape the output of the attention head?",WQ,WK,WV,WO,D) WO
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",Which dimension does the transformer model use for the key and query vectors?,dk,dv,d,1,A) dk
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the shape of the output of the attention head in the transformer model?,[1 × d],[1 × dv],[dv × d],[d × dv],B) [1 × dv]
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",Which matrix in the transformer model is used to project each input vector into its representation as a key?,WQ,WK,WV,WO,B) WK
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",Which method enriches the representation of a token by incorporating contextual information?,Summary Attention,Transformers Attention,Word Embedding,Long Short-Term Memory,B) Transformers Attention
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",What is the main function of the Transformer Block in the context of enriched representation?,It applies summary attention to each word's context.,It passes the enriched representation up layer by layer.,It initializes the word embeddings.,It calculates the final output of the model.,B) It passes the enriched representation up layer by layer.
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",Which method is used to calculate the enriched representation for each word based on its context?,Transformers Attention,Summary Attention,Long Short-Term Memory,Recurrent Neural Network,A) Transformers Attention
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39","Considering the lecture content, which model does not contribute to the enriched representation of a word?",Transformers,Summary Attention,Word Embedding,Long Short-Term Memory,C) Word Embedding
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",Which model is responsible for passing the enriched representation up layer by layer?,Transformers,Summary Attention,Word Embedding,Long Short-Term Memory,A) Transformers
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",Which component of the Transformer model applies Multi-Head Attention?,Language Modeling Head,Input Encoding,Layer Norm,Layer NormMultiHeadAttention,D) Layer NormMultiHeadAttention
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",Which part of the Transformer model applies Layer Norm after each sub-layer?,Input tokens,Language Modeling Head,Layer Norm,Layer NormMultiHeadAttention,C) Layer Norm
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",Which part of the Transformer model processes the residual stream?,Input tokens,Language Modeling Head,Layer Norm,The residual stream: each token gets passed up and modified,D) The residual stream: each token gets passed up and modified
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",Which component of the Transformer model is responsible for the feedforward part?,Layer Norm,Input Encoding,Layer NormMultiHeadAttention,Layer Norm + Feedforward,D) Layer Norm + Feedforward
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",Which layer in a transformer block normalizes the input vector twice?,Attention layer,Feedforward layer,Layer norm before attention,Layer norm after feedforward,C) Layer norm before attention
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the dimensionality of the hidden layer in a feedforward network in a transformer model?,Same as the model dimensionality,Larger than the model dimensionality,Smaller than the model dimensionality,Equal to the model dimensionality,B) Larger than the model dimensionality
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",Which layer in a transformer block computes the residual stream?,Attention layer,Feedforward layer,Layer norm before attention,Layer norm after feedforward,B) Feedforward layer
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the purpose of the residual stream in a transformer block?,To add the input of a component to its output,To normalize the vector,To process the input token's embedding,To pass the input vector up and modify it,A) To add the input of a component to its output
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45","Given an embedding vector x of dimensionality d, which values are calculated during layer normalization to normalize the vector?",Mean and standard deviation over all elements of the vector,Mean and standard deviation over all elements of the layer,Mean and standard deviation over all tokens in the context,Gain and offset values for the vector,A) Mean and standard deviation over all elements of the vector
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",Which component of a transformer block takes input from other tokens in the context?,Layer Normalization,Multi-Head Attention,Feed-Forward Neural Network,Gain and Offset Parameters,B) Multi-Head Attention
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What is the result of the layer normalization process for a single vector x?,A new vector with zero mean and a standard deviation of one,A new vector with a mean of zero and a standard deviation of one,A new vector with a mean of one and a standard deviation of zero,A new vector with a mean of one and a standard deviation of one,A) A new vector with zero mean and a standard deviation of one
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",Which learnable parameters are introduced in the standard implementation of layer normalization?,Mean and standard deviation parameters,Gain and offset parameters,Multi-Head Attention parameters,Feed-Forward Neural Network parameters,B) Gain and offset parameters
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47","Given a transformer block, which component takes input from other tokens in the context?",Layer Norm,Multi-Head Attention,Feedforward Network,Layer Normalization of Embedding,B) Multi-Head Attention
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",Which equation represents the computation of the gain and offset values in layer normalization?,LayerNorm(x) = g(x - µ)s + b,t1i = LayerNorm(xi),"t2i = MultiHeadAttention(t1i, ...)",t3i = t2i + xi,A) LayerNorm(x) = g(x - µ)s + b
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",Which component of a transformer block computes the mean and standard deviation for layer normalization?,Layer Norm,Multi-Head Attention,Feedforward Network,Layer Normalization of Embedding,D) Layer Normalization of Embedding
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47","Given a transformer block, which component computes the high-dimensional embeddings for the current token and neighboring tokens?",Layer Norm,Multi-Head Attention,Feedforward Network,Layer Normalization of Embedding,B) Multi-Head Attention
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49","Given a Transformer model with an input sequence of length 5, what is the shape of the embedding matrix X?",[5 × 2d],[5 × d],[N × d],[d × 5],C) [N × d]
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",Which of the following shapes represents the embedding for a single input token and its positional information?,[1× d],[d×1],[1× 2d],[2d×1],C) [1× 2d]
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",Consider a Transformer model with an input sequence of length 10. Which shape does the embedding matrix X have?,[10 × d],[d × 10],[10 × 2d],[2d × 10],A) [10 × d]
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49","Given a Transformer model with an input sequence of length 5, what shape does the embedding for a single token have?",[5 × d],[d × 5],[1× d],[d×1],C) [1× d]
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","Given the string 'Thanks for all the', the vocabulary indices for each word are [5, 4000, 10532, 2224]. Which rows of the embedding matrix E should be selected to obtain the embeddings for these words?","Rows 5, 4000, 10532, 2225","Rows 5, 4000, 10532, 2224","Rows 5, 4000, 10533, 2224","Rows 5, 4000, 10532, 2223","B) Rows 5, 4000, 10532, 2224"
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","Given the string 'Thanks for all the', the vocabulary indices for each word are [5, 4000, 10532, 2224]. Which shape does the embedding matrix E have?",[|V| ×  (d + 1)],[|V| ×  d],[d × |V|],[|V| × |V|],B) [|V| ×  d]
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55","Given a [1 x d] vector output from a transformer block, how does the language modeling head convert it to a probability distribution over the vocabulary?",By directly using the vector as probabilities,By multiplying the vector with an unembedding matrix and then applying softmax,By adding the vector to an identity matrix and then applying softmax,By subtracting the vector from an identity matrix and then applying softmax,B) By multiplying the vector with an unembedding matrix and then applying softmax
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",Which step in the language modeling head process converts the [1 x d] vector to a [1 x |V|] vector of logits?,Softmax,Unembedding matrix,Transformer block output,Concatenation of unembedding matrix and softmax,B) Unembedding matrix
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55","Given a [1 x d] vector output from a transformer block, which step in the language modeling head process converts it to a probability distribution over the vocabulary?",Unembedding matrix,Softmax,Transformer block output,Concatenation of transformer block output and softmax,D) Concatenation of transformer block output and softmax
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",Which matrix is used to map a [1 x d] vector to a [1 x |V|] vector of logits in the language modeling head?,Embedding matrix,Unembedding matrix,Position embedding matrix,Softmax matrix,B) Unembedding matrix
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57","Given a [1 x d] vector, which layer of the language modeling head maps it to a probability distribution over the vocabulary?",Softmax layer,TransformerBlock layer,Unembedding layer,Embedding layer,C) Unembedding layer
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",Which matrix maps a [1 x d] vector to a [1 x |V|] vector of logits in the language modeling head?,Unembedding matrix,Embedding matrix,TransformerBlock,Softmax,A) Unembedding matrix
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57","Given a [1 x |V|] vector of logits, which layer of the language modeling head maps it to a probability distribution?",Unembedding layer,TransformerBlock layer,Softmax layer,Language Model Head,C) Softmax layer
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",Which matrix maps a [|V| x d] embedding matrix to a [d x |V|] unembedding matrix?,ETE,ET^T,E^T,E,B) ET^T
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59","Given the Transformer model architecture, which layer maps the output of the final transformer layer to a probability distribution over words in the vocabulary?",Unembedding layer,TransformerBlock layer,Language modeling head,Embedding layer,C) Language modeling head
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",Which matrix is used to perform the reverse mapping from an embedding to a vector over the vocabulary in the Transformer model?,Embedding matrix,Unembedding matrix,TransformerBlock matrix,Softmax matrix,B) Unembedding matrix
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the primary function of the unembedding layer in the Transformer model?,To map from a one-hot vector over the vocabulary to an embedding,To map from an embedding to a vector over the vocabulary,To perform a softmax operation on logits,To project from the output of the final transformer layer to the logit vector,B) To map from an embedding to a vector over the vocabulary
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",Which layer in the Transformer model takes the output of the final transformer layer and produces a probability distribution over words in the vocabulary?,Embedding layer,TransformerBlock layer,Language modeling head,Unembedding layer,C) Language modeling head
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61","Given the transformer architecture, which layer is responsible for producing the probability distribution over words in the vocabulary?",Input token Language Model Head,Layer 1,Unembedding layer,Attention layer,A) Input token Language Model Head
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",Which matrix is used to map from the logit vector to a probability distribution over words in the vocabulary?,Embedding matrix,Unembedding matrix,Softmax function,Transpose of the embedding matrix,D) Transpose of the embedding matrix
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the primary function of the unembedding layer in the transformer architecture?,Maps from a one-hot vector over the vocabulary to an embedding,Maps from an embedding to a vector over the vocabulary,Maps from the output of the final transformer layer to a probability distribution over words in the vocabulary,Maps from the logit vector to a probability distribution over words in the vocabulary,B) Maps from an embedding to a vector over the vocabulary
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",Which layer in the transformer architecture takes the output of the final transformer layer and uses it to predict the upcoming word?,Input token Language Model Head,Layer 1,Unembedding layer,Language Model Head,D) Language Model Head
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",Why is pretraining a transformer model on large amounts of text important for its performance on new tasks?,It helps the model learn position embeddings.,It allows the model to memorize specific texts.,It enables the model to perform numerical reasoning.,It improves the model's ability to compare texts. ,A) It helps the model learn position embeddings.
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",Which step in the process of using a pretrained transformer model for a new task involves applying the language model head?,Pretraining the model on large amounts of text,Applying the position embeddings,Fine-tuning the model on the new task,Removing the language model head,C) Fine-tuning the model on the new task
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",Which of the following is a valid reason for pretraining a transformer model on enormous amounts of text?,To enable the model to perform edge cases,To help the model learn position embeddings,To improve the model's ability to perform definition recall,To allow the model to memorize specific texts,B) To help the model learn position embeddings
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",Which part of a transformer model is responsible for learning the meaning of words and their relationships in context?,The position embeddings,The language model head,The self-attention mechanism,The feed-forward neural network,B) The language model head
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65","Given a language model trained using self-supervised learning, which component is responsible for outputting the probability distribution over the vocabulary of possible next words?",Transformer model,Language modeling head,Corpus of text,Gradient descent algorithm,B) Language modeling head
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",Which part of the self-supervised learning process for language models involves asking the model to predict the next word and training it using gradient descent to minimize the error in this prediction?,Pretraining,Applying the model to new tasks,Predicting the next word,Training using gradient descent,C) Predicting the next word
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the term used to describe the process of training a language model using only the next word as the label?,Supervised learning,Self-supervised learning,Unsupervised learning,Reinforcement learning,B) Self-supervised learning
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",Which of the following is a correct description of the output of a language model during self-supervised training?,A probability distribution over the entire corpus of text,A probability distribution over the vocabulary of possible next words,A sequence of correct next words for the entire corpus,A sequence of incorrect next words for the entire corpus,B) A probability distribution over the vocabulary of possible next words
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",Which loss function is used in language modeling to assign a high probability to the true word?,Cross-entropy loss,Mean squared error loss,Absolute error loss,Root mean squared error loss,A) Cross-entropy loss
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",Which loss calculation method is used in teacher forcing?,Loss is calculated based on the correct word and the previous words,Loss is calculated based on the model's prediction and the previous words,Loss is calculated based on the model's prediction and the next word,Loss is calculated based on the model's prediction and the context,A) Loss is calculated based on the correct word and the previous words
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",Which loss function is similar to cross-entropy loss but is used in regression and neural networks?,Mean squared error loss,Absolute error loss,Cross-entropy loss,Root mean squared error loss,B) Absolute error loss
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67","In teacher forcing, what is the next word ignored and what is used instead?",The model's prediction is ignored and the correct word is used,The correct word is ignored and the model's prediction is used,The previous word is ignored and the next word is used,The context is ignored and the next word is used,A) The model's prediction is ignored and the correct word is used
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",Which corpus is mainly used to train Language Model (LLM) models?,Common Crawl: snapshots of the entire web produced by the non-profit Common Crawl,Colossal Clean Crawled Corpus (C4),Filtered patent text documents from the web,News sites and Wikipedia,B) Colossal Clean Crawled Corpus (C4)
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",Which filtering is applied to the Colossal Clean Crawled Corpus (C4) to remove unwanted data?,Removal of patent text documents and adult content,Removal of news sites and Wikipedia,Removal of boilerplate and toxicity,Removal of Common Crawl snapshots,C) Removal of boilerplate and toxicity
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",Which part of the transformer model computes the loss for the next token?,Language Modeling Head,Input Encoding,Stacked Transformer Blocks,Logits,A) Language Modeling Head
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",Which transformer block is responsible for ignoring the model's prediction for the next token?,Language Modeling Head,Input Encoding,Stacked Transformer Blocks,Teacher Forcing,D) Teacher Forcing
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Which issue prevents large language models from being pretrained on all available text on the web?,Data consent from website owners,Copyright issues,Privacy concerns regarding IP addresses and phone numbers,Lack of computational resources,B) Copyright issues
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Which legal doctrine in the US might allow for the use of copyrighted text for pretraining large language models?,Fair use,Public domain,Creative Commons,Digital Millennium Copyright Act,A) Fair use
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Which concern arises when large language models are pretrained on text from websites?,Data security,Data privacy,Data accuracy,Data availability,B) Data privacy
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Which of the following is a potential issue with pretraining large language models on text from the web?,Lack of diversity in the data,Lack of context in the data,Lack of accuracy in the data,Lack of privacy concerns,B) Lack of context in the data
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75","Given a large language model, which method is used to measure the quality of the model by calculating the inverse probability of the model assigning to an unseen test set, normalized by the test set length?",Continued pre-training,Parameter-efficient finetuning (PEFT),Sentiment classification using a classification head,Perplexity evaluation,D) Perplexity evaluation
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75","Which method is used to measure the quality of a large language model by calculating the inverse probability of the model assigning to an unseen test set, normalized by the test set length?",Pretraining,Finetuning,Perplexity evaluation,Masked language model training,C) Perplexity evaluation
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",Which method is used to train a language model to be a sentiment classifier by adding extra neural circuitry after the top layer of the model?,Continued pre-training,Parameter-efficient finetuning (PEFT),Sentiment classification using a classification head,Supervised finetuning (SFT),C) Sentiment classification using a classification head
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",Which method is used to train a language model to follow text instructions and produce desired responses?,Continued pre-training,Parameter-efficient finetuning (PEFT),Sentiment classification using a classification head,Supervised finetuning (SFT),D) Supervised finetuning (SFT)
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75","Which method is used to measure the quality of a large language model by calculating the inverse probability of the model assigning to an unseen test set, given that the test set length is 10 tokens? The model assigns a probability of 0.0001 to the test set.",Perplexity evaluation with a perplexity of 1000,Perplexity evaluation with a perplexity of 101,Perplexity evaluation with a perplexity of 0.0001,Perplexity evaluation with a perplexity of 10,A) Perplexity evaluation with a perplexity of 1000
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77","Given a large language model, which method is used to measure the quality of the model by calculating the inverse probability of the test set, normalized by the test set length?",Continued pre-training,Parameter-efficient finetuning,Sentiment classification,Perplexity evaluation,D) Perplexity evaluation
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77","Which method is used to evaluate the quality of a large language model by measuring the inverse probability of the test set, normalized by the number of words?",Continued pre-training,Parameter-efficient finetuning,Sentiment classification,Perplexity evaluation,D) Perplexity evaluation
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",Which method is used to measure the quality of a large language model by predicting the words in an instruction prompt iteratively and training it to produce the desired response?,Continued pre-training,Parameter-efficient finetuning,Sentiment classification,Supervised finetuning,D) Supervised finetuning
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",Which method is used to measure the quality of a large language model by predicting the words in an instruction prompt iteratively and freezing the entire pretrained model?,Continued pre-training,Parameter-efficient finetuning,Sentiment classification,Supervised finetuning,C) Sentiment classification
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Which metric is used to evaluate the quality of language models by measuring the probability of the word sequence?,Perplexity,Accuracy,Fairness,Size,A) Perplexity
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Which factor is NOT typically considered when evaluating large language models?,Perplexity,Size,Fairness,Energy efficiency,D) Energy efficiency
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Which metric is used to measure the decrease in performance for language from or about certain groups in large language models?,Perplexity,Fairness,Size,Energy usage,B) Fairness
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Which factor is a concern for large language models due to their high energy usage?,Perplexity,Size,Fairness,Energy efficiency,D) Energy efficiency
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",Which phenomenon can lead large language models to generate incorrect or fictitious information?,Hallucination due to copyright infringement,Hallucination due to lack of context,Hallucination due to model size limitation,Hallucination due to power consumption,A) Hallucination due to copyright infringement
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",Which term refers to the generation of incorrect or fictitious information by large language models?,Hallucination,Infringement,Context,Limitation,A) Hallucination
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",Which issue can arise when a large language model generates incorrect or fictitious information?,Infringement of copyright,Lack of context,Hallucination,Limitation of model size,C) Hallucination
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",Which term describes the situation when a large language model generates incorrect or fictitious information?,Hallucination due to model size,Infringement of copyright,Hallucination due to lack of context,Limitation of model capabilities,A) Hallucination due to model size
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",Which of the following is NOT a common topic in online privacy discussions?,Data collection and usage by companies,Encryption and secure communication,Social media and its impact on privacy,Copyright and intellectual property,D) Copyright and intellectual property
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",Which topic is NOT typically discussed under 'Toxicity and Abuse' in online communities?,Harassment and cyberbullying,Privacy and data protection,Copyright infringement and plagiarism,Netiquette and online etiquette,C) Copyright infringement and plagiarism
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",Which type of online harm is most closely related to the spread of misinformation?,Toxicity and Abuse,Fraud,Phishing,Misinformation,D) Misinformation
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",Which harm can large language models inadvertently cause due to their generation of misinformation?,Toxicity and Abuse,Fraud,Phishing,Misinformation,D) Misinformation
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",Which harm can large language models be used to intentionally cause?,Toxicity and Abuse,Fraud,Phishing,Misinformation,B) Fraud
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",Which harm is NOT directly related to large language models?,Toxicity and Abuse,Fraud,Phishing,Misinformation,C) Phishing
notes/LLM_cs124_week7_2025.pdf,44,"87, 88, 89","Given a simple 1-layer neural network with initial weights w1=2, w2=-1, b=1, and no activation function, what is the output of the forward pass for the input (x1, x2) = (4, -3)?",6,12,8,14,B) 12
notes/LLM_cs124_week7_2025.pdf,44,"87, 88, 89","Considering the same neural network as above, and a squared loss function, what is the loss for the training tuple (x1, x2, ytrue) = (4, -3, 10)?",1,9,16,25,C) 16
notes/LLM_cs124_week7_2025.pdf,44,"87, 88, 89","Suppose the learning rate η for the neural network is .01. After one weight update, which weight will have a smaller value: w1 or w2?",w1,w2,Both w1 and w2,It depends on the loss,A) w1
notes/LLM_cs124_week7_2025.pdf,44,"87, 88, 89","Given the loss function L=(ytrue−y)2, which value of y would result in the smallest loss for ytrue=10 and the current network output y=12?",8,10,12,14,A) 8
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","Given the forward pass computation for y = 2x1 - x2 + 1 with input (x1, x2) = (4, -3) and initial weights w1=2, w2=-1, b=1, what is the value of y?",10,12,14,16,B) 12
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91",Which nodes in the computation graph are the parameters of the model?,"w1, x1, x2","w1, w2, b","x1, x2, y","y, ytrue, Loss","B) w1, w2, b"
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","Given the forward pass computation for y = 2x1 - x2 + 1 with input (x1, x2) = (4, -3) and initial weights w1=2, w2=-1, b=1, what is the value of the loss (L) if ytrue = 10?",0,4,100,1000,B) 4
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95","Given the backpropagation equation for computing the gradient of the loss with respect to weight w1x1, which term is multiplied by -1?",h1,w1x1,ytrue,2y,B) w1x1
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95","In the backpropagation equation for computing the gradient of the loss with respect to weight w1x1, which term is subtracted from 2y?",h1,w1x1,ytrue,2y,C) ytrue
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95",Which term in the backpropagation equation for computing the gradient of the loss with respect to weight w1x1 is equal to the input x1?,h1,w1x1,y,x1,B) w1x1
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95","In the backpropagation equation for computing the gradient of the loss with respect to weight w1x1, which term represents the bias term bh1?",h1,w1x1,ytrue,bh1,D) bh1
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97","Given the backpropagation equation, which term represents the weight between the second hidden layer and the output layer?",w1x1,h1 + h2 + b,w2x2,2(ytrue-y),C) w2x2
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97","In the backpropagation equation, which term represents the error between the true output and the predicted output?",h1 + h2 + b,w1x1,2(ytrue-y),w2x2,C) 2(ytrue-y)
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97","Given the backpropagation equation, which term represents the weight between the input layer and the first hidden layer?",w1x1,h1 + h2 + b,w2x2,2(ytrue-y),A) w1x1
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97","In the backpropagation equation, which term represents the weight between the first hidden layer and the second hidden layer?",w1x1,h1 + h2 + b,w2x2,h1,B) h1 + h2 + b
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","Given the backpropagation equation, what is the value of dw1 in terms of w1, x1, y, ytrue, and b?",w1 + 2(ytrue - y) * h1 + b,w1 - 1.84 * x1 + 0.96 * w2 - 0.88 * b,w1 - 1.84 * x2 + 0.96 * w2 + 0.88 * b,w1 + 1.84 * x1 + 0.96 * w2 + 1.2 * b,A) w1 + 2(ytrue - y) * h1 + b
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","Given the backpropagation equation, what is the value of dw2 in terms of w2, x2, y, ytrue, and b?",w2 + 12(ytrue - y) * h2 + b,w2 - 4 * x1 - 12 * b,w2 - 4 * x2 + 12 * b,w2 + 4 * x1 + 12 * b,A) w2 + 12(ytrue - y) * h2 + b
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","Given the backpropagation equation, what is the value of db in terms of w1, w2, x1, x2, y, and ytrue?",b + 2(ytrue - y) * h2 + 4 * x1 - 4 * x2,b - 2(ytrue - y) * h1 + 4 * x1 + 4 * x2,b - 2(ytrue - y) * h1 + 4 * x1 - 4 * x2,b + 2(ytrue - y) * h1 + 4 * x1 + 4 * x2,B) b - 2(ytrue - y) * h1 + 4 * x1 + 4 * x2
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","Given the backpropagation weights update rules, which weight will have the smallest change in value after an update?",w1 = 1.84,w2 = -0.88,b = 0.96,w1 = 0.96,D) w1 = 0.96
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","If the learning rate η is increased, which weight will experience a larger update?",w1 = 1.84,w2 = -1,b = 1,w1 = -3,A) w1 = 1.84
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","Given the backpropagation weights update rules, which weight will have the largest change in value after an update?",w1 = 1.84,w2 = -12,b = 1,w1 = -3,B) w2 = -12
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","If the input features x1 and x2 are multiplied by a factor of 2, what will be the new value of w1 in the backpropagation weight update rule?",w1 = 3.68,w1 = 7.36,w1 = 1.84,w1 = 0.92,B) w1 = 7.36
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","Given a multi-head attention layer with 8 heads, what is the shape of the output from each head?",[1 x d],[1 x hdv],[hdv x d],[dv x hdv],B) [1 x hdv]
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","Which matrices are used to project the inputs into separate key, value, and query embeddings for each head in a multi-head attention layer?","WK, WQ, WV","WQ, WK, WV","WK, WV, WQ","WV, WQ, WK","A) WK, WQ, WV"
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","What is the shape of the key, query, and value embeddings for each head in a multi-head attention layer?",[d x dk],[dk x d],[d x dv],[dv x dk],A) [d x dk]
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","Given a multi-head attention layer with 8 heads, what is the shape of the output from the concatenation of all heads?",[1 x d],[1 x hdv],[8 x hdv],[hdv x d],C) [8 x hdv]
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105","Given a multi-head attention layer with A heads, what is the shape of the output from each head?",1⇥dv,1⇥hdv,hdv x dv,1⇥d,A) 1⇥dv
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",Which matrices are multiplied to produce the QK| matrix in parallelized attention?,XWQ and XWK,XWQ and XWV,XWK and XWV,XWQ and XWQ,A) XWQ and XWK
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the shape of the mask matrix Min used for masking future words in attention?,N⇥N,N⇥d,1⇥N,N⇥(N+1),D) N⇥(N+1)
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",Which of the following shapes is the output of a single attention head in parallelized attention?,hdv x d,1⇥d,1⇥dv,N⇥d,C) 1⇥dv
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107","Given a single attention head, which matrices are multiplied to produce matrices Q, K, and V?","X, WQ, WK, WV","X, WQ, W, V","X, W, WQ, WK","X, W, WK, WV","A) X, WQ, WK, WV"
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",Which portion of the masked QK|matrix is set to zero in language modeling?,Lower-triangular portion,Upper-triangular portion,Diagonal portion,Randomly selected elements,B) Upper-triangular portion
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107","In multi-head attention, the output of each head has dimensionality:",N × d,N × dk,N × dv,N × hdv,C) N × dv
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",The final linear projection in multi-head attention is of shape: ,Adv⇥d,Adv⇥dk,Adv⇥dv,Adv⇥hdv,A) Adv⇥d
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",What is the purpose of masking in the self-attention computation?,To eliminate knowledge of words that follow in the sequence in language modeling,To make attention computation linear in the length of the input,To add weight to the comparisons between query and key values,To eliminate the need for dot products between tokens in the input,A) To eliminate knowledge of words that follow in the sequence in language modeling
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",Which portion of the QK|matrix is masked in the self-attention computation?,Lower triangle,Upper triangle,Diagonal,Entire matrix,B) Upper triangle
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",What is the shape of the output of a single attention head in multi-head attention?,N⇥d,N⇥dv,N⇥dk,N⇥hdv,B) N⇥dv
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",What is the role of the mask matrix in the self-attention computation?,It sets elements in the upper-triangular portion of the QK|matrix to 0,It sets elements in the lower-triangular portion of the QK|matrix to 0,It sets elements in the diagonal of the QK|matrix to 0,It sets elements in the QK|matrix to 1,A) It sets elements in the upper-triangular portion of the QK|matrix to 0
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",Which matrix is zeroed out in the upper-triangular portion for masking out future words in attention computation?,QK| matrix,QKT masked matrix,Q matrix,K matrix,A) QK| matrix
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What shape does the output of each attention head have in multi-head attention?,N x dk x NN x dv,N x dk x NN x dk,N x dv x NN x dk,N x dk x NN x dvd,B) N x dk x NN x dk
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111","Which matrix is multiplied by the inputs to produce Q, K, and V matrices in multi-head attention?",X matrix,WQ matrix,WK matrix,WV matrix,A) X matrix
notes/LLM_cs124_week7_2025.pdf,56,"111, 112, 113","Given a sequence of tokens, where does the transformer obtain the input embeddings from?",Output of the MultiHeadAttention function,Output of the FFN function,Input of the LayerNorm function,"Input of the transformer block, separately computed as input token and positional embeddings","D) Input of the transformer block, separately computed as input token and positional embeddings"
notes/LLM_cs124_week7_2025.pdf,56,"111, 112, 113",What shape does the input X and output H of a transformer block have?,[N⇥(d⇥|V|)],[N⇥d],[N⇥(d⇥N)],[N⇥(d⇥d)],B) [N⇥d]
notes/LLM_cs124_week7_2025.pdf,56,"111, 112, 113",Which function computes the self-attention output of shape [N⇥d]?,MultiHeadAttention,LayerNorm,SelfAttention,FFN,C) SelfAttention
notes/LLM_cs124_week7_2025.pdf,56,"111, 112, 113",What is the role of the embedding matrix E in the transformer model?,It computes the input token embeddings and input positional embeddings,It applies the MultiHeadAttention function in parallel to each embedding vector,It applies the FFN function in parallel to each embedding vector,It normalizes the input and output of each transformer block,A) It computes the input token embeddings and input positional embeddings
notes/LLM_cs124_week7_2025.pdf,57,"113, 114","Given a sequence of tokens, where does the transformer obtain the input embeddings from?",The embedding matrix E is multiplied with the input tokens to obtain the input embeddings,The embedding matrix E is added to the input tokens to obtain the input embeddings,The embedding matrix E is subtracted from the input tokens to obtain the input embeddings,The embedding matrix E is divided by the input tokens to obtain the input embeddings,A) The embedding matrix E is multiplied with the input tokens to obtain the input embeddings
notes/LLM_cs124_week7_2025.pdf,57,"113, 114","Given an input token string, how are the token embeddings selected from the embedding matrix E?",By multiplying the embedding matrix E with the one-hot vector of the token index,By adding the embedding matrix E to the one-hot vector of the token index,By subtracting the embedding matrix E from the one-hot vector of the token index,By dividing the embedding matrix E by the one-hot vector of the token index,A) By multiplying the embedding matrix E with the one-hot vector of the token index
notes/LLM_cs124_week7_2025.pdf,57,"113, 114","Given a sequence of tokens, what is the shape of the input matrix X and the output matrix H in a transformer block?",Both X and H have shape [N⇥(d⇥|V|)],Both X and H have shape [N⇥d⇥|V|],Both X and H have shape [N⇥|V|⇥d],Both X and H have shape [N⇥d],D) Both X and H have shape [N⇥d]
notes/LLM_cs124_week7_2025.pdf,57,"113, 114","In the context of a transformer block, what does the FFN represent?",A function that applies parallel attention to each embedding vector in the window,A function that applies parallel normalization to each embedding vector in the window,A function that applies parallel feed-forward neural network processing to each embedding vector in the window,A function that applies parallel multiplication to each embedding vector in the window,C) A function that applies parallel feed-forward neural network processing to each embedding vector in the window
