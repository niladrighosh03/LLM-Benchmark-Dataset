{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415a6249",
   "metadata": {},
   "source": [
    "### Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3966751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_pdf_pages(pdf_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Extracts text from each page of a PDF.\n",
    "    Returns a list where each element is text from one page.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    pages = []\n",
    "\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        pages.append(text if text else \"\")\n",
    "\n",
    "    return pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7822ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_page_chunks(\n",
    "    pages: list,\n",
    "    chunk_size: int = 3,\n",
    "    overlap: int = 1\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Creates page chunks with overlap.\n",
    "    \n",
    "    Example:\n",
    "    chunk_size=3, overlap=1\n",
    "    Pages: [1,2,3] → [3,4,5] → ...\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap\n",
    "\n",
    "    chunk_number = 1\n",
    "    for start in range(0, len(pages), step):\n",
    "        end = start + chunk_size\n",
    "        chunk_pages = pages[start:end]\n",
    "\n",
    "        if not chunk_pages:\n",
    "            break\n",
    "\n",
    "        chunks.append({\n",
    "            \"chunk_number\": chunk_number,\n",
    "            \"pages\": list(range(start + 1, min(end + 1, len(pages) + 1))),\n",
    "            \"text\": \"\\n\".join(chunk_pages)\n",
    "        })\n",
    "\n",
    "        chunk_number += 1\n",
    "\n",
    "        if end >= len(pages):\n",
    "            break\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def save_chunks(chunks: list, output_path: str):\n",
    "    \"\"\"\n",
    "    Saves chunks to a JSON file.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "def load_chunks(path: str) -> list:\n",
    "    \"\"\"\n",
    "    Loads stored PDF chunks.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c08334",
   "metadata": {},
   "source": [
    "### Use LLM to generate MCQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b3bcc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mcq_prompt(text: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are an expert university-level instructor and question setter.\n",
    "\n",
    "Generate  high-quality multiple-choice questions (MCQs)\n",
    "STRICTLY based on the lecture content below.\n",
    "\n",
    "CRITICAL RULES (NO EXCEPTIONS):\n",
    "1. Use ONLY the provided lecture content.\n",
    "2. Each question must be conceptually meaningful.\n",
    "3. Each question must have EXACTLY 4 options.\n",
    "4. EXACTLY one option must be correct.\n",
    "5. DO NOT add explanations, comments, or extra text.\n",
    "6. OUTPUT MUST BE VALID JSON ONLY.\n",
    "\n",
    "OUTPUT FORMAT (STRICT JSON ARRAY):\n",
    "[\n",
    "  {{\n",
    "    \"question\": \"question text\",\n",
    "    \"options\": {{\n",
    "      \"A\": \"option text\",\n",
    "      \"B\": \"option text\",\n",
    "      \"C\": \"option text\",\n",
    "      \"D\": \"option text\"\n",
    "    }},\n",
    "    \"answer\": \"A\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "EXAMPLE OUTPUT (FORMAT ONLY):\n",
    "[\n",
    "  {{\n",
    "    \"question\": \"What is gradient descent?\",\n",
    "    \"options\": {{\n",
    "      \"A\": \"An optimization algorithm\",\n",
    "      \"B\": \"A loss function\",\n",
    "      \"C\": \"A neural network layer\",\n",
    "      \"D\": \"A regularization method\"\n",
    "    }},\n",
    "    \"answer\": \"A\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "LECTURE CONTENT:\n",
    "<<<\n",
    "{text}\n",
    ">>>\n",
    "\n",
    "RETURN ONLY THE JSON ARRAY. NO MARKDOWN. NO TEXT OUTSIDE JSON.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483c3957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 14:41:55.508734: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-26 14:41:55.523042: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-26 14:41:55.540110: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-26 14:41:55.545069: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-26 14:41:55.557930: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-26 14:41:56.709672: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471b04b0945343b29453816ce8dacfdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b470691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mistral_generate(tokenizer, model, prompt: str, max_tokens: int = 1024):\n",
    "    messages = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "    inputs = tokenizer(messages, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id     # deterministic output\n",
    "    )\n",
    "\n",
    "    # Slice to get only the generated tokens (exclude prompt)\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952819fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def clean_json_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the LLM response to ensure it's valid JSON.\n",
    "    \"\"\"\n",
    "    # Remove markdown code fences\n",
    "    cleaned = re.sub(r\"```(?:json)?\", \"\", response).strip()\n",
    "    \n",
    "    # Try to find the JSON array list\n",
    "    start = cleaned.find(\"[\")\n",
    "    end = cleaned.rfind(\"]\")\n",
    "    \n",
    "    if start != -1 and end != -1:\n",
    "        cleaned = cleaned[start : end + 1]\n",
    "        \n",
    "    return cleaned\n",
    "\n",
    "def generate_mcqs_from_chunks(\n",
    "    chunks: list,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    csv_path: str,\n",
    "    pdf_name: str\n",
    "):\n",
    "    \"\"\"Generate MCQs and write to CSV immediately after each MCQ\"\"\"\n",
    "    \n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    # Write CSV header first\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"pdf_name\",\n",
    "            \"chunk_number\",\n",
    "            \"total_chunks\",\n",
    "            \"pages\",\n",
    "            \"question\",\n",
    "            \"option_A\",\n",
    "            \"option_B\",\n",
    "            \"option_C\",\n",
    "            \"option_D\",\n",
    "            \"correct_answer\"\n",
    "        ])\n",
    "    \n",
    "    total_mcqs = 0\n",
    "    skipped_mcqs = 0\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_number = chunk.get(\"chunk_number\", i + 1)  # Use chunk_number from JSON or fallback\n",
    "        print(f\"\\nProcessing chunk {chunk_number}/{total_chunks}...\")\n",
    "\n",
    "        try:\n",
    "            prompt = build_mcq_prompt(\n",
    "                text=chunk[\"text\"]\n",
    "            )\n",
    "\n",
    "            response = mistral_generate(tokenizer, model, prompt)\n",
    "\n",
    "            # --- JSON SAFETY ---\n",
    "            try:\n",
    "                cleaned_response = clean_json_response(response)\n",
    "                mcq_list = json.loads(cleaned_response)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[WARN] JSON parse failed for chunk {chunk_number}\")\n",
    "                print(f\"ERROR: {e}\")\n",
    "                print(f\"Skipping chunk {chunk_number} and continuing...\")\n",
    "                continue\n",
    "\n",
    "            # Ensure mcq_list is actually a list\n",
    "            if not isinstance(mcq_list, list):\n",
    "                print(f\"[WARN] Response is not a list for chunk {chunk_number}\")\n",
    "                print(f\"Skipping chunk {chunk_number} and continuing...\")\n",
    "                continue\n",
    "\n",
    "            # Write each MCQ immediately to CSV\n",
    "            with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                \n",
    "                for mcq_idx, mcq in enumerate(mcq_list):\n",
    "                    try:\n",
    "                        # Validate MCQ structure\n",
    "                        if not isinstance(mcq, dict):\n",
    "                            print(f\"  [SKIP] MCQ {mcq_idx+1} is not a dict, skipping...\")\n",
    "                            skipped_mcqs += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Check required fields\n",
    "                        if \"question\" not in mcq or \"options\" not in mcq or \"answer\" not in mcq:\n",
    "                            print(f\"  [SKIP] MCQ {mcq_idx+1} missing required fields, skipping...\")\n",
    "                            skipped_mcqs += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Validate options\n",
    "                        options = mcq[\"options\"]\n",
    "                        if not isinstance(options, dict) or not all(key in options for key in [\"A\", \"B\", \"C\", \"D\"]):\n",
    "                            print(f\"  [SKIP] MCQ {mcq_idx+1} has invalid options structure, skipping...\")\n",
    "                            skipped_mcqs += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Validate answer key\n",
    "                        answer_key = mcq[\"answer\"]\n",
    "                        if answer_key not in options:\n",
    "                            print(f\"  [SKIP] MCQ {mcq_idx+1} has invalid answer key '{answer_key}', skipping...\")\n",
    "                            skipped_mcqs += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Format correct answer as \"D) Option Text\"\n",
    "                        answer_text = options[answer_key]\n",
    "                        formatted_answer = f\"{answer_key}) {answer_text}\"\n",
    "                        \n",
    "                        # Convert pages list to string\n",
    "                        pages_str = \", \".join(map(str, chunk[\"pages\"]))\n",
    "                        \n",
    "                        writer.writerow([\n",
    "                            pdf_name,\n",
    "                            chunk_number,\n",
    "                            pages_str,\n",
    "                            mcq[\"question\"],\n",
    "                            options[\"A\"],\n",
    "                            options[\"B\"],\n",
    "                            options[\"C\"],\n",
    "                            options[\"D\"],\n",
    "                            formatted_answer\n",
    "                        ])\n",
    "                        \n",
    "                        total_mcqs += 1\n",
    "                        # print(f\"  ✓ Generated MCQ #{total_mcqs}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"  [ERROR] Failed to process MCQ {mcq_idx+1}: {e}\")\n",
    "                        print(f\"  Skipping this MCQ and continuing...\")\n",
    "                        skipped_mcqs += 1\n",
    "                        continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process chunk {chunk_number}: {e}\")\n",
    "            print(f\"Skipping chunk {chunk_number} and continuing...\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✅ Completed!\")\n",
    "    print(f\"   Generated: {total_mcqs} MCQs\")\n",
    "    print(f\"   Skipped: {skipped_mcqs} MCQs\")\n",
    "    print(f\"   Processed: {total_chunks} chunks\")\n",
    "    return total_mcqs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a8e09",
   "metadata": {},
   "source": [
    "#### Inference the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "191a7298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 114 pages\n",
      "Created 57 chunks\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract pages from the PDF\n",
    "pdf_name = \"LLM_cs124_week7_2025.pdf\"\n",
    "pages = extract_pdf_pages(pdf_name)\n",
    "\n",
    "# Step 2: Chunk (3 pages, 1-page overlap)\n",
    "chunks = create_page_chunks(\n",
    "    pages,\n",
    "    chunk_size=3,\n",
    "    overlap=1\n",
    ")\n",
    "\n",
    "# Step 3: Store to JSON\n",
    "save_chunks(chunks, \"pdf_chunks.json\")\n",
    "\n",
    "print(f\"Extracted {len(pages)} pages\")\n",
    "print(f\"Created {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29c0c1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1/57...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Generated MCQ #1\n",
      "  ✓ Generated MCQ #2\n",
      "  ✓ Generated MCQ #3\n",
      "  ✓ Generated MCQ #4\n",
      "\n",
      "Processing chunk 2/57...\n",
      "  ✓ Generated MCQ #5\n",
      "  ✓ Generated MCQ #6\n",
      "  ✓ Generated MCQ #7\n",
      "  ✓ Generated MCQ #8\n",
      "\n",
      "Processing chunk 3/57...\n",
      "  ✓ Generated MCQ #9\n",
      "  ✓ Generated MCQ #10\n",
      "  ✓ Generated MCQ #11\n",
      "  ✓ Generated MCQ #12\n",
      "\n",
      "Processing chunk 4/57...\n",
      "  ✓ Generated MCQ #13\n",
      "  ✓ Generated MCQ #14\n",
      "  ✓ Generated MCQ #15\n",
      "  ✓ Generated MCQ #16\n",
      "\n",
      "Processing chunk 5/57...\n",
      "  ✓ Generated MCQ #17\n",
      "  ✓ Generated MCQ #18\n",
      "  ✓ Generated MCQ #19\n",
      "  ✓ Generated MCQ #20\n",
      "\n",
      "Processing chunk 6/57...\n",
      "  [SKIP] MCQ 1 has invalid answer key 'A, B, C', skipping...\n",
      "  ✓ Generated MCQ #21\n",
      "  ✓ Generated MCQ #22\n",
      "  ✓ Generated MCQ #23\n",
      "\n",
      "Processing chunk 7/57...\n",
      "  ✓ Generated MCQ #24\n",
      "  ✓ Generated MCQ #25\n",
      "\n",
      "Processing chunk 8/57...\n",
      "  ✓ Generated MCQ #26\n",
      "  ✓ Generated MCQ #27\n",
      "  ✓ Generated MCQ #28\n",
      "  ✓ Generated MCQ #29\n",
      "\n",
      "Processing chunk 9/57...\n",
      "  ✓ Generated MCQ #30\n",
      "  ✓ Generated MCQ #31\n",
      "  ✓ Generated MCQ #32\n",
      "  ✓ Generated MCQ #33\n",
      "  ✓ Generated MCQ #34\n",
      "  ✓ Generated MCQ #35\n",
      "  ✓ Generated MCQ #36\n",
      "  ✓ Generated MCQ #37\n",
      "  ✓ Generated MCQ #38\n",
      "  ✓ Generated MCQ #39\n",
      "\n",
      "Processing chunk 10/57...\n",
      "  ✓ Generated MCQ #40\n",
      "  ✓ Generated MCQ #41\n",
      "  ✓ Generated MCQ #42\n",
      "  ✓ Generated MCQ #43\n",
      "\n",
      "Processing chunk 11/57...\n",
      "[WARN] JSON parse failed for chunk 11\n",
      "ERROR: Expecting ',' delimiter: line 13 column 66 (char 468)\n",
      "Skipping chunk 11 and continuing...\n",
      "\n",
      "Processing chunk 12/57...\n",
      "[WARN] JSON parse failed for chunk 12\n",
      "ERROR: Invalid control character at: line 8 column 84 (char 427)\n",
      "Skipping chunk 12 and continuing...\n",
      "\n",
      "Processing chunk 13/57...\n",
      "  ✓ Generated MCQ #44\n",
      "  ✓ Generated MCQ #45\n",
      "  ✓ Generated MCQ #46\n",
      "  ✓ Generated MCQ #47\n",
      "\n",
      "Processing chunk 14/57...\n",
      "  ✓ Generated MCQ #48\n",
      "  ✓ Generated MCQ #49\n",
      "  ✓ Generated MCQ #50\n",
      "  ✓ Generated MCQ #51\n",
      "\n",
      "Processing chunk 15/57...\n",
      "  ✓ Generated MCQ #52\n",
      "  ✓ Generated MCQ #53\n",
      "  ✓ Generated MCQ #54\n",
      "  ✓ Generated MCQ #55\n",
      "\n",
      "Processing chunk 16/57...\n",
      "  ✓ Generated MCQ #56\n",
      "  ✓ Generated MCQ #57\n",
      "  ✓ Generated MCQ #58\n",
      "  ✓ Generated MCQ #59\n",
      "\n",
      "Processing chunk 17/57...\n",
      "  ✓ Generated MCQ #60\n",
      "  ✓ Generated MCQ #61\n",
      "  ✓ Generated MCQ #62\n",
      "  ✓ Generated MCQ #63\n",
      "\n",
      "Processing chunk 18/57...\n",
      "  ✓ Generated MCQ #64\n",
      "  ✓ Generated MCQ #65\n",
      "  ✓ Generated MCQ #66\n",
      "  ✓ Generated MCQ #67\n",
      "\n",
      "Processing chunk 19/57...\n",
      "  ✓ Generated MCQ #68\n",
      "  ✓ Generated MCQ #69\n",
      "  ✓ Generated MCQ #70\n",
      "  ✓ Generated MCQ #71\n",
      "\n",
      "Processing chunk 20/57...\n",
      "  ✓ Generated MCQ #72\n",
      "  ✓ Generated MCQ #73\n",
      "  ✓ Generated MCQ #74\n",
      "  ✓ Generated MCQ #75\n",
      "\n",
      "Processing chunk 21/57...\n",
      "  ✓ Generated MCQ #76\n",
      "  ✓ Generated MCQ #77\n",
      "  ✓ Generated MCQ #78\n",
      "  ✓ Generated MCQ #79\n",
      "\n",
      "Processing chunk 22/57...\n",
      "  ✓ Generated MCQ #80\n",
      "  ✓ Generated MCQ #81\n",
      "  ✓ Generated MCQ #82\n",
      "  ✓ Generated MCQ #83\n",
      "\n",
      "Processing chunk 23/57...\n",
      "[WARN] JSON parse failed for chunk 23\n",
      "ERROR: Invalid control character at: line 36 column 29 (char 1220)\n",
      "Skipping chunk 23 and continuing...\n",
      "\n",
      "Processing chunk 24/57...\n",
      "  ✓ Generated MCQ #84\n",
      "  ✓ Generated MCQ #85\n",
      "  ✓ Generated MCQ #86\n",
      "  ✓ Generated MCQ #87\n",
      "\n",
      "Processing chunk 25/57...\n",
      "  ✓ Generated MCQ #88\n",
      "  ✓ Generated MCQ #89\n",
      "  ✓ Generated MCQ #90\n",
      "  ✓ Generated MCQ #91\n",
      "\n",
      "Processing chunk 26/57...\n",
      "  ✓ Generated MCQ #92\n",
      "  ✓ Generated MCQ #93\n",
      "  ✓ Generated MCQ #94\n",
      "  ✓ Generated MCQ #95\n",
      "\n",
      "Processing chunk 27/57...\n",
      "  ✓ Generated MCQ #96\n",
      "  ✓ Generated MCQ #97\n",
      "  ✓ Generated MCQ #98\n",
      "  ✓ Generated MCQ #99\n",
      "\n",
      "Processing chunk 28/57...\n",
      "  ✓ Generated MCQ #100\n",
      "  ✓ Generated MCQ #101\n",
      "  ✓ Generated MCQ #102\n",
      "  ✓ Generated MCQ #103\n",
      "\n",
      "Processing chunk 29/57...\n",
      "  ✓ Generated MCQ #104\n",
      "  ✓ Generated MCQ #105\n",
      "  ✓ Generated MCQ #106\n",
      "  ✓ Generated MCQ #107\n",
      "\n",
      "Processing chunk 30/57...\n",
      "  ✓ Generated MCQ #108\n",
      "  ✓ Generated MCQ #109\n",
      "  ✓ Generated MCQ #110\n",
      "  ✓ Generated MCQ #111\n",
      "\n",
      "Processing chunk 31/57...\n",
      "  ✓ Generated MCQ #112\n",
      "  ✓ Generated MCQ #113\n",
      "  ✓ Generated MCQ #114\n",
      "  ✓ Generated MCQ #115\n",
      "\n",
      "Processing chunk 32/57...\n",
      "  ✓ Generated MCQ #116\n",
      "  ✓ Generated MCQ #117\n",
      "  ✓ Generated MCQ #118\n",
      "  ✓ Generated MCQ #119\n",
      "\n",
      "Processing chunk 33/57...\n",
      "  ✓ Generated MCQ #120\n",
      "  ✓ Generated MCQ #121\n",
      "  ✓ Generated MCQ #122\n",
      "  ✓ Generated MCQ #123\n",
      "\n",
      "Processing chunk 34/57...\n",
      "  ✓ Generated MCQ #124\n",
      "  ✓ Generated MCQ #125\n",
      "  ✓ Generated MCQ #126\n",
      "  ✓ Generated MCQ #127\n",
      "\n",
      "Processing chunk 35/57...\n",
      "  ✓ Generated MCQ #128\n",
      "  ✓ Generated MCQ #129\n",
      "  ✓ Generated MCQ #130\n",
      "  ✓ Generated MCQ #131\n",
      "\n",
      "Processing chunk 36/57...\n",
      "  ✓ Generated MCQ #132\n",
      "  ✓ Generated MCQ #133\n",
      "  ✓ Generated MCQ #134\n",
      "\n",
      "Processing chunk 37/57...\n",
      "  ✓ Generated MCQ #135\n",
      "  ✓ Generated MCQ #136\n",
      "  ✓ Generated MCQ #137\n",
      "  ✓ Generated MCQ #138\n",
      "\n",
      "Processing chunk 38/57...\n",
      "  ✓ Generated MCQ #139\n",
      "  ✓ Generated MCQ #140\n",
      "  ✓ Generated MCQ #141\n",
      "  ✓ Generated MCQ #142\n",
      "\n",
      "Processing chunk 39/57...\n",
      "  ✓ Generated MCQ #143\n",
      "  ✓ Generated MCQ #144\n",
      "  ✓ Generated MCQ #145\n",
      "  ✓ Generated MCQ #146\n",
      "\n",
      "Processing chunk 40/57...\n",
      "  ✓ Generated MCQ #147\n",
      "  ✓ Generated MCQ #148\n",
      "\n",
      "Processing chunk 41/57...\n",
      "[WARN] JSON parse failed for chunk 41\n",
      "ERROR: Extra data: line 2 column 1 (char 3)\n",
      "Skipping chunk 41 and continuing...\n",
      "\n",
      "Processing chunk 42/57...\n",
      "  ✓ Generated MCQ #149\n",
      "  ✓ Generated MCQ #150\n",
      "\n",
      "Processing chunk 43/57...\n",
      "  ✓ Generated MCQ #151\n",
      "  ✓ Generated MCQ #152\n",
      "  ✓ Generated MCQ #153\n",
      "  ✓ Generated MCQ #154\n",
      "\n",
      "Processing chunk 44/57...\n",
      "  ✓ Generated MCQ #155\n",
      "  ✓ Generated MCQ #156\n",
      "  ✓ Generated MCQ #157\n",
      "  ✓ Generated MCQ #158\n",
      "\n",
      "Processing chunk 45/57...\n",
      "  ✓ Generated MCQ #159\n",
      "  ✓ Generated MCQ #160\n",
      "  ✓ Generated MCQ #161\n",
      "  ✓ Generated MCQ #162\n",
      "\n",
      "Processing chunk 46/57...\n",
      "  ✓ Generated MCQ #163\n",
      "  ✓ Generated MCQ #164\n",
      "  ✓ Generated MCQ #165\n",
      "  ✓ Generated MCQ #166\n",
      "\n",
      "Processing chunk 47/57...\n",
      "  ✓ Generated MCQ #167\n",
      "  ✓ Generated MCQ #168\n",
      "  ✓ Generated MCQ #169\n",
      "  ✓ Generated MCQ #170\n",
      "\n",
      "Processing chunk 48/57...\n",
      "  ✓ Generated MCQ #171\n",
      "  ✓ Generated MCQ #172\n",
      "  ✓ Generated MCQ #173\n",
      "  ✓ Generated MCQ #174\n",
      "\n",
      "Processing chunk 49/57...\n",
      "  ✓ Generated MCQ #175\n",
      "  ✓ Generated MCQ #176\n",
      "  ✓ Generated MCQ #177\n",
      "  ✓ Generated MCQ #178\n",
      "  ✓ Generated MCQ #179\n",
      "\n",
      "Processing chunk 50/57...\n",
      "  ✓ Generated MCQ #180\n",
      "  ✓ Generated MCQ #181\n",
      "  ✓ Generated MCQ #182\n",
      "  ✓ Generated MCQ #183\n",
      "\n",
      "Processing chunk 51/57...\n",
      "  ✓ Generated MCQ #184\n",
      "  ✓ Generated MCQ #185\n",
      "  ✓ Generated MCQ #186\n",
      "  ✓ Generated MCQ #187\n",
      "\n",
      "Processing chunk 52/57...\n",
      "  ✓ Generated MCQ #188\n",
      "  ✓ Generated MCQ #189\n",
      "  ✓ Generated MCQ #190\n",
      "  ✓ Generated MCQ #191\n",
      "\n",
      "Processing chunk 53/57...\n",
      "  ✓ Generated MCQ #192\n",
      "  ✓ Generated MCQ #193\n",
      "  ✓ Generated MCQ #194\n",
      "  ✓ Generated MCQ #195\n",
      "\n",
      "Processing chunk 54/57...\n",
      "  ✓ Generated MCQ #196\n",
      "  ✓ Generated MCQ #197\n",
      "  ✓ Generated MCQ #198\n",
      "  ✓ Generated MCQ #199\n",
      "\n",
      "Processing chunk 55/57...\n",
      "  ✓ Generated MCQ #200\n",
      "  ✓ Generated MCQ #201\n",
      "  ✓ Generated MCQ #202\n",
      "  ✓ Generated MCQ #203\n",
      "\n",
      "Processing chunk 56/57...\n",
      "  ✓ Generated MCQ #204\n",
      "  ✓ Generated MCQ #205\n",
      "  ✓ Generated MCQ #206\n",
      "  ✓ Generated MCQ #207\n",
      "\n",
      "Processing chunk 57/57...\n",
      "  ✓ Generated MCQ #208\n",
      "  ✓ Generated MCQ #209\n",
      "  ✓ Generated MCQ #210\n",
      "  ✓ Generated MCQ #211\n",
      "\n",
      "✅ Completed!\n",
      "   Generated: 211 MCQs\n",
      "   Skipped: 1 MCQs\n",
      "   Processed: 57 chunks\n"
     ]
    }
   ],
   "source": [
    "pdf_name = \"LLM_cs124_week7_2025.pdf\"\n",
    "\n",
    "chunks = load_chunks(\"pdf_chunks.json\")\n",
    "\n",
    "# Generate MCQs and write to CSV on the go\n",
    "total_mcqs = generate_mcqs_from_chunks(\n",
    "    chunks,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    csv_path=\"mcq_dataset.csv\",\n",
    "    pdf_name=pdf_name\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salesbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
