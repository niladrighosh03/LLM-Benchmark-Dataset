pdf_name,chunk_number,total_chunks,pages,question,option_A,option_B,option_C,option_D,correct_answer
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3","When a simple n-gram language model and a large language model generate text, which of the following statements is true?",Both models generate text by sampling possible next words.,Both models only use pretrained data for text generation.,Only the large language model generates text by sampling possible next words.,Only the simple n-gram model uses learning to guess the next word.,A) Both models generate text by sampling possible next words.
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",What is a fundamental difference between simple n-gram language models and large language models?,Large language models are trained solely on n-grams.,Simple n-gram models are trained on larger datasets than large language models.,"Large language models learn to guess the next word, while simple n-gram models do not.","Simple n-gram models use learning techniques, while large language models do not.","C) Large language models learn to guess the next word, while simple n-gram models do not."
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",How do large language models acquire useful language knowledge?,Through pretraining on large datasets.,Only by explicitly programming linguistic rules.,By sampling random sequences of text.,Through learning grammar and syntax rules after initial training.,A) Through pretraining on large datasets.
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3","In the context of language models, what is the main purpose of sampling possible next words?",To exclusively determine the length of a text.,To assess the grammatical correctness of a sentence.,To generate coherent and contextually relevant text.,To count the frequency of specific words in a corpus.,C) To generate coherent and contextually relevant text.
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",What is true about the training data of large language models?,It consists solely of structured data.,"It is limited to a small, curated dataset.",It consists of large amounts of text from diverse sources.,It includes only text from academic publications.,C) It consists of large amounts of text from diverse sources.
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which type of large language model (LLM) is known for generating text by predicting words from left to right?,Encoders,Encoder-Decoders,Decoders,BERT family,C) Decoders
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5","In large language models, which architecture allows for conditioning on future context?",Decoders,Encoders,Encoder-Decoders,Flan-T5,B) Encoders
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which of the following pretraining approaches is specific to decoder-only models like GPT?,Pretraining on bidirectional context,Pretraining for autoregressive language modeling,Pretraining for encoder-decoder tasks,Pretraining using HuBERT,B) Pretraining for autoregressive language modeling
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which neural architecture is typically associated with generating text and cannot condition on future words?,Encoders,Encoder-Decoders,Decoders,Llama,C) Decoders
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5","In the context of large language models, what does the term 'bidirectional context' refer to?",The ability to generate text from left to right,The ability to predict future words based on past context,The capacity to condition on both past and future context,The ability to train models using the Flan-T5 architecture,C) The capacity to condition on both past and future context
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which model architecture allows for conditioning on future context during training?,Decoder-only models,MLM or BERT family models,Encoder-Decoder models,Causal LLMs,B) MLM or BERT family models
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",What is a key characteristic of decoder-only models like GPT?,They are trained on supervised data for classification tasks.,They are incapable of generating text.,They generate text by predicting words left to right.,They map sequences from one form to another.,C) They generate text by predicting words left to right.
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which of the following is the best description of Encoder-Decoder models?,They are exclusively used for generating text.,They are primarily used for machine translation and speech recognition.,They can only predict words from past context.,They are trained by predicting words from surrounding words on one side.,B) They are primarily used for machine translation and speech recognition.
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",What pretraining approach is typically used by Models in the BERT family?,Masked Language Model (MLM),Language Modeling,Sequence-to-Sequence Prediction,Causal Masking,A) Masked Language Model (MLM)
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which model architecture is specifically designed for tasks that involve mapping from one sequence to another?,Decoder-only models,Encoder-only models,MLM or BERT family models,Encoder-Decoder models,D) Encoder-Decoder models
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What is a common characteristic of decoder-only models in language processing tasks?,They can condition on future words in a sequence,"They generate text from left to right, predicting one word at a time",They encapsulate bidirectional context for each word,They combine linguistic patterns from multiple languages,"B) They generate text from left to right, predicting one word at a time"
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Which of the following is NOT a synonym for decoder-only models?,Autoregressive LLMs,Causal LLMs,Encoder-Decoder Architectures,Left-to-right LLMs,C) Encoder-Decoder Architectures
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9","In the context of pretraining language models, which architecture is typically associated with the ability to condition on future words?",Encoder-only models,Decoder-only models,Encoder-Decoder models,Purely bidirectional models,C) Encoder-Decoder models
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Which statement best describes the pretraining approach for decoder-only Language Learning Models (LLMs)?,They are pretrained to predict future words based on previous context,They are pretrained to predict words without any sequential order,They are pretrained to encode semantic information from both directions,They are pretrained to translate languages without understanding context,A) They are pretrained to predict future words based on previous context
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What is a major use case scenario for Encoder-Decoder models as highlighted in the lecture?,Generating random word sequences,Machine translation and speech recognition,Calculating numerical computations,Encoding images into a vector space,B) Machine translation and speech recognition
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",What type of pretraining is typically used for transformer-based large language models?,Prefix Text Completion,Encoder-Only Pretraining,Decoder-Only Pretraining,Both Encoder and Decoder Pretraining,C) Decoder-Only Pretraining
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11","In the context of language modeling, which token is used to suggest that an answer should come next in a question answering task?",Q:,A:,P:,E:,B) A:
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",Which token is typically used in language models to prompt the generation of a text summary?,tl;dr;,Summary;,Concise;,Brief;,A) tl;dr;
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",What type of language model architecture is known for its ability to condition on future words?,Decoder-only models,Encoder-only models,Encoder-Decoder models,Causal LLMs,B) Encoder-only models
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",What is the sentiment of the sentence 'I like Jackie Chan'?,positive,negative,neutral,unknown,A) positive
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13","In the context of the sentence 'Who wrote The Origin of Species', which name is more likely to appear next?",Charles,Darwin,Smith,Johnson,A) Charles
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13","For the task of text summarization, what token is used after a long text to indicate the start of a summary?",tl;dr,summary,short,brief,A) tl;dr
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15","In the context of language modeling, what is the primary function of a transformer within a large language model (LLM)?",To create a stack of neural networks that process word sequences,To compute the probability of the next word given a prefix string,To transform the output of one neural network into another,To reduce the dimensionality of text data before processing,B) To compute the probability of the next word given a prefix string
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",Which architecture is primarily used in modern large language models for processing language?,Recurrent Neural Networks (RNNs),Convolutional Neural Networks (CNNs),Transformers,Autoencoders,C) Transformers
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",What is the primary goal of a system in language modeling according to the lecture content?,To analyze the grammar of the input text,To generate new text based on a given context,To compute the probability that a word follows a given prefix string of words,To translate text from one language to another,C) To compute the probability that a word follows a given prefix string of words
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Which of the following best describes the architecture of the Transformer model proposed in the lecture content?,A network based on recurrent neural networks,"A network based on self-attention, without recurrence and convolution",A network based on convolutional neural networks,A network based on gradient descent optimization,"B) A network based on self-attention, without recurrence and convolution"
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What is the primary reason for replacing RNNs with self-attention in the Transformer model?,To increase the complexity of the model,To improve the speed of training and increase parallelizability,To reduce the overall accuracy of the model,To simplify the model for educational purposes,B) To improve the speed of training and increase parallelizability
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Which metric did the Transformer model achieve on the WMT 2014 English-to-German translation task according to the lecture content?,28.4 BLEU,41.8 BLEU,35.2 BLEU,26.1 BLEU,A) 28.4 BLEU
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Who among the following contributed to the design and implementation of the first Transformer models?,Ashish Vaswani,Aidan N. Gomez,Noam Shazeer,All of the above,D) All of the above
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17","According to the lecture content, what was a significant outcome of applying the Transformer model to English constituency parsing?",The model failed to generalize well to other tasks,The model showed good generalization to other tasks,The model required significant modifications to be applied to other tasks,The model had limited success when trained with large datasets,B) The model showed good generalization to other tasks
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19","In the evolution of natural language processing models, what significant milestone occurred in 2017?",Introduction of Static Word Embeddings,Widespread adoption of GPUs,Invention of the Transformer architecture,Development of Attention mechanisms,C) Invention of the Transformer architecture
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which of the following components is NOT a part of the Transformer architecture as described in the lecture?,StackedTransformerBlocks,InputEncodings,LanguageModelingHead,Recurrent Neural Networks,D) Recurrent Neural Networks
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19","Based on the lecture, what is the role of 'E1+', 'E2+', 'E3+', 'E4+', 'E5+' in the Transformer architecture?",They represent different types of Attention mechanisms.,They denote sequential stages in the training of the model.,They signify the addition of input embeddings through embedding layers.,They are placeholders for hyperparameter settings.,C) They signify the addition of input embeddings through embedding layers.
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19","According to the timeline provided, when did 'Prompting' become a recognized technique in NLP?",2012,2015,2019,2022,C) 2019
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",What does the term 'static embeddings' refer to in the context of word representations?,Embeddings that change based on the word's context,Embeddings that are fixed and do not change with context,Embeddings that are generated through attention mechanisms,Embeddings that are only applicable to sentences,B) Embeddings that are fixed and do not change with context
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21","In the context of Natural Language Processing, why are static embeddings like word2vec considered limited?",They require too much computational power,They cannot represent numerical values,They do not account for the changing meaning of words in different contexts,They are not language-specific,C) They do not account for the changing meaning of words in different contexts
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",What is a primary advantage of contextual embeddings over static embeddings?,They are simpler to compute,They provide a unique vector representation for each word in every context,They are always smaller in size,They can be used without understanding the language,B) They provide a unique vector representation for each word in every context
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",Which mechanism is often used to compute contextual embeddings?,Recurrent Neural Networks,Attention mechanisms,Convolutional Neural Networks,Feedforward Neural Networks,B) Attention mechanisms
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",What is the primary purpose of contextual embeddings in understanding the meaning of a word?,To assign a unique vector to each word irrespective of context,To represent the same meaning of a word across different contexts,To create different representations for the same word based on its surrounding words,To eliminate the need for attention mechanisms in language models,C) To create different representations for the same word based on its surrounding words
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23","In the sentence 'The chicken didn't cross the road because it was too wide', what does 'it' most likely refer to?",The chicken,The road,The act of crossing,The time of day,B) The road
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",How does attention contribute to the construction of contextual embeddings?,By giving equal weight to all neighboring words,By selectively integrating information from neighboring words,By attending to non-neighboring words exclusively,By removing the influence of surrounding words,B) By selectively integrating information from neighboring words
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",What is the underlying intuition behind the concept of contextual embeddings?,All words should have a fixed representation,The meaning of a word is constant across different contexts,The meaning of a word should vary depending on its context,Attention mechanisms are irrelevant to word embeddings,C) The meaning of a word should vary depending on its context
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25","In the context of attention mechanisms, what does it mean when a word 'attends to' neighboring words?",It means the word completely ignores its neighboring words.,It means the word is selectively integrating information from its neighboring words.,It means the word replaces the neighboring words in its context.,It means the word is physically closer to the neighboring words.,B) It means the word is selectively integrating information from its neighboring words.
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",How is the attention mechanism in a neural network model best described?,A method for randomly selecting tokens to ignore.,A method for doing a weighted sum of vectors based on their importance.,A method for duplicating tokens at each layer.,A method for predicting the next token in a sequence.,B) A method for doing a weighted sum of vectors based on their importance.
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",Which of the following best illustrates the role of attention in a neural network layer?,It assigns equal importance to all input tokens.,It focuses on the most relevant input tokens for computing the embedding for a token.,It removes the less frequent input tokens from consideration.,It translates all input tokens into a universal embedding.,B) It focuses on the most relevant input tokens for computing the embedding for a token.
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",What does the 'self-attention distribution' in an attention layer indicate?,It shows the distribution of self-attention scores across different parts of the input.,It represents the physical distances between tokens.,It indicates the frequency of each token in the input sequence.,It displays the chronological order of tokens.,A) It shows the distribution of self-attention scores across different parts of the input.
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","In the context of an attention head, which role does the 'query' vector represent?",A preceding input being compared to the current element,The current element being compared to the preceding inputs,A value of a preceding element that gets weighted and summed,The self-attention distribution columns corresponding to input tokens,B) The current element being compared to the preceding inputs
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31",What does the 'value' vector represent in an actual attention head?,A preceding input being compared to the current element,The current element being compared to the preceding inputs,A value of a preceding element that gets weighted and summed,The self-attention distribution columns corresponding to input tokens,C) A value of a preceding element that gets weighted and summed
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","In a layer k+1 self-attention distribution, what does each column represent?",The query vectors for input tokens,The key vectors for input tokens,The value vectors for input tokens,The self-attention distribution columns corresponding to input tokens,D) The self-attention distribution columns corresponding to input tokens
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","If 'The chick...' sentence is being processed by a self-attention mechanism, which component would be responsible for determining the similarity between 'The chick...' and 'didn’t cross'?","Query, as it represents the current element being compared","Key, as it represents a preceding input being compared","Value, as it is the weighted and summed value of a preceding element","Self-attention distribution columns, as they represent input tokens","B) Key, as it represents a preceding input being compared"
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33","What is the purpose of the weight matrices WQ, WK, and WV in the context of a transformer's attention head?","To project each input vector into representations for its role as a query, key, or value",To compute the similarity between different input vectors,To normalize the similarity scores into a probability distribution,To compute the weighted sum of the input vectors,"A) To project each input vector into representations for its role as a query, key, or value"
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33","In a transformer's attention mechanism, what does the softmax function primarily help achieve?","It helps project input vectors into query, key, or value representations",It assigns weights to the similarity scores between input vectors,It normalizes the similarity scores into a probability distribution,It computes the weighted sum of the input vectors,C) It normalizes the similarity scores into a probability distribution
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33","In the attention mechanism of transformers, what is the role of the query vector?",To represent the current element being compared to preceding inputs,To represent the preceding input that is being compared to the current element,To represent the value of a preceding element that gets weighted and summed,To compute the similarity between different input vectors,A) To represent the current element being compared to preceding inputs
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33","According to the lecture, how does a transformer compute the output for the current element?","By summing the values of the prior elements, each weighted by the similarity of its key to the current element's query",By multiplying the values of the prior elements with the current element's query,By normalizing the similarity scores between the current element and all preceding elements,By averaging the values of all prior elements,"A) By summing the values of the prior elements, each weighted by the similarity of its key to the current element's query"
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",Which matrix in the transformer model is responsible for reshaping the output of the attention head to match the model dimensionality?,WQ,WK,WV,WO,D) WO
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37","In the context of transformer models, what does the matrix WO primarily do?",Projects the input vector into a query representation,Projects the input vector into a key representation,Projects the input vector into a value representation,Reshapes the output of the attention head to match model dimensionality,D) Reshapes the output of the attention head to match model dimensionality
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the purpose of using multiple attention heads in a transformer model?,To process different aspects of the relationships among inputs,To reduce the dimensionality of the input vectors,To increase the dimensionality of the output vectors,To simplify the mathematical calculations required,A) To process different aspects of the relationships among inputs
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37","In a transformer model, what does the dimensionality 'd' typically represent?",The length of the input sequence,The number of attention heads,The model dimensionality,The number of layers in the transformer block,C) The model dimensionality
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the shape of the output vector after applying the WO matrix in a transformer model?,[1⇥dk],[1⇥dv],[1⇥d],[1⇥dk⇥dv],C) [1⇥d]
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",What is the outcome of incorporating contextual information in token representation as per the lecture?,The embedding for each word will remain static across different contexts,The embedding for each word will differ depending on the context,Contextual information will make the token representation less accurate,The attention mechanism will be rendered ineffective,B) The embedding for each word will differ depending on the context
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39","According to the lecture, how does the enriched representation in attention mechanism propagate?",It propagates down layer by layer,It does not propagate,It propagates up layer by layer,It propagates only within the same layer,C) It propagates up layer by layer
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","In the context of the Transformer architecture, what is the purpose of the residual stream?",To provide an alternative path for the gradients during backpropagation.,To bypass the normalization layer.,To directly pass the input tokens to the output without any modification.,To increase the computational complexity of the model.,A) To provide an alternative path for the gradients during backpropagation.
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",Which component of the Transformer model is primarily responsible for integrating information from different positions in the input sequence?,Layer Norm,MultiHeadAttention,Feedforward,InputEncoding,B) MultiHeadAttention
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","What does the '+' symbol represent in the notation 'E1+', 'E2+', etc., within the context of the Transformer architecture?",Exponential growth of the model's complexity.,The addition of new input tokens at each layer.,The residual connections that add the input of a layer to its output.,The incremental increase in the number of attention heads.,C) The residual connections that add the input of a layer to its output.
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","In the Transformer model's stacked blocks, what is the primary function of the 'Layer Norm' component?",To normalize the input sequence length.,To stabilize the training process by normalizing the inputs to each sublayer.,To reduce the dimensionality of the input data.,To encode the position of each input token.,B) To stabilize the training process by normalizing the inputs to each sublayer.
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","Based on the Transformer architecture, how does the 'logits' layer function within the model?",It calculates the loss during training.,It is a final dense layer that produces the probability distribution over the vocabulary.,It is used for input encoding.,It represents the intermediate outputs of the attention mechanism.,B) It is a final dense layer that produces the probability distribution over the vocabulary.
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43","In the context of a transformer block, what is the purpose of the residual stream?",To provide a direct path for the input to reach the output without any modifications,"To carry the output of various layers back to their input for addition, ensuring that the input is not lost during processing",To replace the input with the output of the feedforward layer,To carry the output of the attention layer exclusively to the final output,"B) To carry the output of various layers back to their input for addition, ensuring that the input is not lost during processing"
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",Which component in a transformer block is responsible for dimensionality expansion?,Layer Normalization,MultiHead Attention,Feedforward layer,Residual Stream,C) Feedforward layer
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the dimensionality relationship between the model dimensionality 'd' and the hidden layer dimensionality 'dff' in the feedforward network of a transformer block?,dff is smaller than d,dff is equal to d,dff is larger than d,There is no consistent relationship between d and dff,C) dff is larger than d
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",Which layer normalization order occurs in the pre-norm version of a transformer block's architecture depicted in Figure 9.6?,Before the attention layer only,After the feedforward layer only,Before and after the attention layer,Before the attention and feedforward layer,D) Before the attention and feedforward layer
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What process does layer normalization (LayerNorm) apply to a vector in a transformer block?,LayerNorm applies a linear transformation to the vector.,LayerNorm applies a z-score normalization to the vector.,LayerNorm applies an attention mechanism to the vector.,LayerNorm applies a feedforward neural network to the vector.,B) LayerNorm applies a z-score normalization to the vector.
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",Which component of a transformer block directly incorporates information from neighboring tokens?,LayerNorm,MultiHeadAttention,Feedforward,Residual Streams,B) MultiHeadAttention
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45","In a transformer block, what is the role of the attention mechanism?",It normalizes the vector.,It calculates mean and standard deviation.,It moves information from the residual stream of a neighboring token into the current stream.,It applies a feedforward neural network.,C) It moves information from the residual stream of a neighboring token into the current stream.
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",Which of the following is NOT a component of a transformer block?,LayerNorm,MultiHeadAttention,Feedforward,Recurrent Neural Network,D) Recurrent Neural Network
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What are g and b parameters in layer normalization?,They are the mean and standard deviation of the vector.,They are the gain and offset values introduced in the standard implementation of layer normalization.,They are the input and output vectors of the transformer block.,They are the components of the feedforward neural network.,B) They are the gain and offset values introduced in the standard implementation of layer normalization.
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",What is the purpose of layer normalization within a transformer block?,To normalize the input to have zero mean and a standard deviation of one,To improve the convergence of the deep neural network,To apply gain and offset values to the embeddings,To reduce the dimensionality of the input vector,A) To normalize the input to have zero mean and a standard deviation of one
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47","In the context of a transformer block, what is the function of the two learnable parameters, g and b?",They represent the gain and offset values applied after layer normalization,They are used to calculate the mean and standard deviation of the input vector,They are used to reduce the dimensionality of the input vector,They represent the weights and biases of the feedforward neural network,A) They represent the gain and offset values applied after layer normalization
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",What is the input to the layer normalization in a transformer block?,An entire transformer layer,A single token embedding vector,A vector of dimensionality d,A vector of dimensionality 1⇥d,B) A single token embedding vector
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",What is the final computation inside a transformer block before the output is produced?,LayerNorm(x) + MultiHeadAttention(x),LayerNorm(x) + FFN(x),MultiHeadAttention(LayerNorm(x)) + x,LayerNorm(MultiHeadAttention(LayerNorm(x))) + FFN(LayerNorm(x)),D) LayerNorm(MultiHeadAttention(LayerNorm(x))) + FFN(LayerNorm(x))
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",Which component within a transformer block processes information from neighboring tokens?,LayerNorm,MultiHeadAttention,FFN,LayerNorm + FFN,B) MultiHeadAttention
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49","In the context of Transformer Blocks, what is the shape of matrix X that contains token and position embeddings for each word in the context?",[N × d],[d × N],[N × N],[1 × d],A) [N × d]
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",How is each embedding of shape [1× d] created in a Transformer Block?,By concatenating a token embedding and a positional embedding,By multiplying a token embedding with a positional embedding,By adding a token embedding to a positional embedding,By subtracting a token embedding from a positional embedding,C) By adding a token embedding to a positional embedding
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",What is the purpose of position embeddings in the Transformer model?,To provide a unique identifier for each word in the context,To represent the relative or absolute position of tokens in the sequence,To encode the semantic meaning of each word,To act as a placeholder for unknown words in the context,B) To represent the relative or absolute position of tokens in the sequence
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",Which component of the Transformer model is responsible for creating embeddings for each word in the context?,The Language Model Head,The Token Embeddings,The Position Embeddings,The Transformer Block,B) The Token Embeddings
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",What is the main function of the Language Model Head in the context of Transformers?,To generate the final output of the Transformer model,To create position embeddings for each token,To calculate the attention scores between tokens,To encode the input text into a fixed-size vector,A) To generate the final output of the Transformer model
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","In the context of token and position embeddings, which of the following best describes the shape of the embedding matrix E?",[N × d],[|V | × d],[d × |V |],[1 × d],B) [|V | × d]
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","When tokenizing the string 'Thanks for all the' with BPE and converting into vocab indices, what shape does the resulting vector w have?",[1 × d],[|V | × 1],[4 × 1],[N × 1],C) [4 × 1]
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","Given a tokenized string converted into vocab indices, how is the corresponding word embedding obtained from matrix E?",Select the corresponding column from E for each index,Select the corresponding row from E for each index,Multiply the index with each row in E,Add the index to each row in E,B) Select the corresponding row from E for each index
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","If the word 'Word' has vocabulary index 1024 and the embedding matrix E is of shape [|V | × d], which of the following correctly represents the embedding for 'Word'?",Row 1024 of E,Column 1024 of E,Row |V | of E,Column |V | of E,A) Row 1024 of E
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53","In the context of transformer models, what do the composite embeddings X represent?",The sum of word embeddings and position embeddings,The product of word embeddings and position embeddings,The difference between word embeddings and position embeddings,The concatenation of word embeddings and position embeddings,A) The sum of word embeddings and position embeddings
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",Which method of position encoding in transformers involves learning embeddings for each integer up to a maximum length?,Relative position encoding,Absolute position encoding,Random position encoding,Sequential position encoding,B) Absolute position encoding
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53","When tokenizing with BPE and converting into vocab indices, which of the following sequences represents the correct process?","Combining tokens into subwords, assigning unique identifiers","Assigning unique identifiers, splitting into subwords","Splitting into words, assigning unique identifiers","Assigning unique identifiers, combining tokens into words","A) Combining tokens into subwords, assigning unique identifiers"
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",What is the goal of learning a position embedding matrix Epos in transformer models?,To encode the order of words in a sequence,To encode the semantic meaning of words,To reduce the dimensionality of the input data,To encode the syntactic structure of sentences,A) To encode the order of words in a sequence
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53","In the given example of composite embeddings X, which components are summed to form X?",Token embeddings and position embeddings,Token embeddings and word embeddings,Position embeddings and sentence embeddings,Word embeddings and sentence embeddings,A) Token embeddings and position embeddings
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",What is the primary role of the language modeling head in a transformer block?,To map a [1 x d] vector to a [1 x |V|] probability distribution over the vocabulary,To generate the next word in a sequence,To compress the input sequence into a dense representation,To encode positional information into the input sequence,A) To map a [1 x d] vector to a [1 x |V|] probability distribution over the vocabulary
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",Which sequence of operations correctly describes the function of the language modeling head in terms of vector transformation?,Embedding matrix -> Softmax -> Logits,Logits -> Softmax -> Embedding matrix,Softmax -> Embedding matrix -> Logits,Embedding matrix -> Logits -> Softmax,D) Embedding matrix -> Logits -> Softmax
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55","In the context of transformers, what does the 'unembedding matrix' primarily accomplish?",It maps a [1 x d] vector to a [1 x |V|] vector of logits,It compresses the input sequence into a smaller dimension,It adds positional information to the input sequence,It normalizes the input sequence for processing by the transformer block,A) It maps a [1 x d] vector to a [1 x |V|] vector of logits
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",Which of the following is NOT a step involved in turning the output of a transformer block into a probability distribution over the vocabulary?,Applying the embedding matrix,Using softmax to convert logits into probabilities,Mapping the [1 x d] vector to a [1 x |V|] vector of logits,Normalizing the [1 x |V|] vector of probabilities,D) Normalizing the [1 x |V|] vector of probabilities
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What is the primary function of the language modeling head in a neural network?,To map a [1 x d] vector to a [1 x |V|] probability distribution over the vocabulary,To act as a database for storing vocabulary,To generate new vocabulary items on the fly,To optimize the network's weights through backpropagation,A) To map a [1 x d] vector to a [1 x |V|] probability distribution over the vocabulary
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",Which sequence of transformations is used by the language modeling head to produce word probabilities?,Softmax followed by an embedding layer,An embedding layer followed by softmax,A transformer block followed by softmax,Softmax followed by a transformer block,C) A transformer block followed by softmax
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",How does the unembedding matrix relate to the embedding matrix in the context of the language modeling head?,They are identical and interchangeable,The unembedding matrix has the same shape as the embedding matrix,The unembedding matrix has a shape of [|V| x d] and is used to map logits to probabilities,The unembedding matrix has a shape of [d x |V|] and is used to reverse the operation of the embedding matrix,D) The unembedding matrix has a shape of [d x |V|] and is used to reverse the operation of the embedding matrix
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What is the role of the softmax function in the language modeling head?,To linearly transform the input vector into a probability distribution,To map logits to a probability distribution over the vocabulary,To compress the input vector into fewer dimensions,To increase the sparsity of the output vector,B) To map logits to a probability distribution over the vocabulary
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57","In the context of a language modeling head, what does the [1 x |V|] vector represent after applying softmax?",The final hidden layer's output before the softmax,The logits produced by the unembedding matrix,The probability distribution over the vocabulary for a given input,The embeddings of the input tokens before transformation,C) The probability distribution over the vocabulary for a given input
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59","In the context of a Transformer language model, what is the primary function of the 'unembedding layer'?",To project the output token embedding at position N to the logit vector,To perform softmax over the logits to generate word probabilities,To randomly sample a word from the vocabulary based on logit scores,To tie the weight matrix with the embedding matrix at the input stage of the transformer,A) To project the output token embedding at position N to the logit vector
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",Which of the following best describes the relationship between the embedding matrix E and the unembedding layer in a Transformer language model?,The unembedding layer and the embedding matrix are completely separate with no relation.,The unembedding layer is a learned weight matrix distinct from the embedding matrix.,"The unembedding layer uses the same weights as the embedding matrix, but in the reverse order.",The embedding matrix is derived from the unembedding layer.,"C) The unembedding layer uses the same weights as the embedding matrix, but in the reverse order."
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59","In a Transformer language model, the softmax layer is used to do what?",Convert logits into embeddings,Turn the logits into probabilities over the vocabulary,Project the output embedding to the logit vector,Tie the weight matrix with the embedding matrix,B) Turn the logits into probabilities over the vocabulary
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59","What is the shape of the logit vector in a Transformer language model language head, given the vocabulary size is |V| and the embedding dimension is d?",|V| x d,1 x d,1 x |V|,d x |V|,C) 1 x |V|
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the primary function of the language modeling head in a transformer language model?,To compute the probability of a word given counts of its occurrence with the prior words,To convert logits into a probability distribution over the vocabulary,To tie the weight matrix to the embedding matrix,To perform attention layer normalization,B) To convert logits into a probability distribution over the vocabulary
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61","In a transformer language model, how does the unembedding layer function?",It maps from a one-hot vector over the vocabulary to an embedding,It projects from the output token embedding to the logit vector,It uses the transpose of the embedding matrix to map from an embedding to a vector over the vocabulary,It performs reverse mapping from an embedding to a one-hot vector over the vocabulary,C) It uses the transpose of the embedding matrix to map from an embedding to a vector over the vocabulary
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the shape of the logit vector in a transformer language model?,1 x d,|V| x d,1 x |V|,d x |V|,C) 1 x |V|
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",How is weight tying used in a transformer language model?,It uses the same weights for two different matrices in the model,It optimizes the embedding matrix for both input and output mapping,It performs the reverse mapping from an embedding to a one-hot vector over the vocabulary,It performs linear layer normalization,A) It uses the same weights for two different matrices in the model
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",What is the initial step in pretraining large language models?,Training on specific tasks,Applying to new tasks,Fine-tuning the model,Pretraining a transformer model on large amounts of text,D) Pretraining a transformer model on large amounts of text
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63","In the context of Large Language Models, what is the role of the Language Model Head?",It is used for image recognition tasks,It is a component of the transformer model architecture,It is used for generating text,It is responsible for optimizing the pretraining phase,C) It is used for generating text
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",Which of the following best describes the 'big idea' behind the performance of language models?,Pretraining a transformer model on enormous amounts of data,Applying the model to new tasks without any pretraining,Fine-tuning the model on specific tasks,Using smaller models for faster computation,A) Pretraining a transformer model on enormous amounts of data
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",Which step in the self-supervised training algorithm for transformers involves using the next word as the label?,Taking a corpus of text,Training the model using gradient descent,Asking the model to predict the next word,Minimizing the error in this prediction,C) Asking the model to predict the next word
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What does the output of a language model (LLM) represent in the context of transformer models?,A binary classification over the vocabulary,A probability distribution over the vocabulary,The index of the next word in the sequence,The list of all possible next words,B) A probability distribution over the vocabulary
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65","In the context of transformer models, what is the purpose of the loss function?",To predict the next word in the sequence,To minimize the error in the next word's prediction,To determine the probability distribution over the vocabulary,To apply gradient descent in the transformer model,B) To minimize the error in the next word's prediction
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What component of a transformer model is responsible for the language modeling head?,Self-supervised training algorithm,Transformer stack,Pretraining process,Corpus of text,B) Transformer stack
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69","In the context of training a transformer language model, what does the term 'teacher forcing' refer to?","Using the correct token as input for the next position, ignoring the model's prediction",Forcing the model to learn from only the most recent token,Correcting the model's predictions by providing human feedback,Iterating the training process until the model's predictions align with the teacher's,"A) Using the correct token as input for the next position, ignoring the model's prediction"
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",Which corpus is primarily used for training large language models (LLMs) and includes billions of web pages?,Common Crawl,WordNet,British National Corpus,Google Books Corpus,A) Common Crawl
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",What is a key characteristic of the Colossal Clean Crawled Corpus (C4) used in training LLMs?,It exclusively contains patent text documents,"It is a small, manually curated dataset",It has been filtered to remove undesirable content,It includes only news articles and Wikipedia entries,C) It has been filtered to remove undesirable content
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69","In the sequence of operations for a transformer language model, what does the 'E1+' notation signify?",The addition of the first token embedding,The completion of the encoding of the first token,The error correction for the first token prediction,The initial epoch of training for the first token,B) The completion of the encoding of the first token
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",Which dataset is primarily used for pretraining LLMs as mentioned in the lecture?,Common Crawl,Wikipedia,News Sites,Patent Text Documents,A) Common Crawl
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What is the approximate size of the Colossal Clean Crawled Corpus (C4) used in pretraining models?,75 billion tokens,100 billion tokens,156 billion tokens,200 billion tokens,C) 156 billion tokens
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What type of content does the Colossal Clean Crawled Corpus (C4) filter to remove?,Scientific Research,Boilerplate and adult content,Scholarly Articles,E-commerce listings,B) Boilerplate and adult content
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71","According to the lecture, which of the following best describes the primary benefit of pretraining language models on large text corpora?",Improving the model's performance on text compression,Enhancing the model's ability to generate random text,Providing the model with extensive knowledge and understanding,Reducing the model's training time significantly,C) Providing the model with extensive knowledge and understanding
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71","Based on the information provided, which of the following entities is NOT explicitly mentioned as a component of the datasets used for pretraining LLMs?",Wikipedia,Colossal Clean Crawled Corpus (C4),Common Crawl,Social Media Sites,D) Social Media Sites
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",What is a potential legal challenge when scraping text from the web for pretraining language models?,The data is always public domain,The fair use doctrine may not permit extensive use,All web content is licensed for educational use,Copyright laws do not apply to online content,B) The fair use doctrine may not permit extensive use
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73","When pretraining large language models, which factor is NOT a concern regarding web-scraped data?",Inclusion of private IP addresses,Website owners' consent for crawling,The algorithm's efficiency in processing data,Copyrighted text being part of the datasets,C) The algorithm's efficiency in processing data
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Which of the following is a valid reason for website owners to prevent their sites from being scraped?,To ensure faster internet speeds for their visitors,To avoid their content being used in large language models,To reduce the server load from automated bots,To prevent the spread of their website's information,B) To avoid their content being used in large language models
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Why might scraping text from the web for pretraining language models raise privacy concerns?,All text on the internet is considered public information,It can lead to the release of private IP addresses and phone numbers,The process is completely anonymized and secure,Web scraping is universally considered private,B) It can lead to the release of private IP addresses and phone numbers
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73","In the context of pretraining large language models, which statement best captures the complexity of the data used?",Only educational websites' content is used,The data is always anonymized before use,Knowledge from a wide variety of texts is incorporated,Texts are selected from a single genre to maintain consistency,C) Knowledge from a wide variety of texts is incorporated
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75","In the context of Large Language Models, what does the term 'Perplexity' refer to?",The uncertainty of a language model,The inverse probability that a model assigns to an unseen text set,The speed of a language model in predicting text,The size of a language model,B) The inverse probability that a model assigns to an unseen text set
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",What is the common method used to reduce the time and cost when retraining a large language model?,Re-training all parameters of the model,Freezing some of the parameters and training only a subset,Increasing the size of the model,Using a smaller dataset for retraining,B) Freezing some of the parameters and training only a subset
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75","In the context of large language models, what is the purpose of 'Supervised Fine-Tuning' (SFT)?",To train the model to follow text instructions,To increase the size of the model,To decrease the perplexity of the model,To improve the speed of the model,A) To train the model to follow text instructions
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",Which method is used to create a dataset for Supervised Fine-Tuning?,Collecting unlabelled data and predicting labels,Using data from the pre-training phase,Creating supervised responses to each command,Generating random prompts and responses,C) Creating supervised responses to each command
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",What does 'Parameter-Efficient Fine-Tuning' (PEFT) mainly involve in the context of large language models?,Freezing all parameters of the model,Selectively updating specific parameters,Increasing the size of the model,Decreasing the size of the model,B) Selectively updating specific parameters
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",What is the purpose of supervised fine-tuning (SFT) in the context of large language models?,To retrain all parameters of the model with new data,To train the model to follow text instructions and produce desired responses,To freeze some of the parameters and train only a subset of them,To classify and label data for a specific task,B) To train the model to follow text instructions and produce desired responses
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77","In the context of large language models, what does perplexity measure?",The accuracy of the model in classifying text,"The inverse probability that the model assigns to the unseen test set, normalized by the test set length",The number of parameters in the model,The speed at which the model processes new data,"B) The inverse probability that the model assigns to the unseen test set, normalized by the test set length"
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",Which of the following best describes the concept of parameter-efficient fine-tuning (PEFT)?,Retraining all parameters of the model with new data,Freezing all parameters of the model and not updating them,Freezing some of the parameters and training only a subset of them,Adding extra neural circuitry to the model for classification tasks,C) Freezing some of the parameters and training only a subset of them
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",Why is perplexity used as a metric to evaluate language models?,Because it measures the raw probability of the test set,"Because it is per-word, normalized by length, and reflects the quality of the model in predicting unseen text",Because it solely depends on the size of the test set,Because it is faster to compute than raw probabilities,"B) Because it is per-word, normalized by length, and reflects the quality of the model in predicting unseen text"
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",What does a lower perplexity score indicate about a language model?,The model is less accurate with the data,The model's tokenizer is inadequate,The model has a higher probability of generating the word sequence,The model requires more energy to train,C) The model has a higher probability of generating the word sequence
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Why is perplexity not always the best metric for comparing language models?,It only measures the model's size,It is sensitive to length and tokenization differences,It does not account for energy usage,It cannot measure fairness,B) It is sensitive to length and tokenization differences
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79","In the context of Large Language Models, what does minimizing perplexity equate to?",Maximizing the model's energy consumption,Maximizing the length of the word sequence,Maximizing the probability of the word sequence,Maximizing the number of GPUs required for training,C) Maximizing the probability of the word sequence
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79","When comparing the performance of Large Language Models, what factor is NOT directly related to perplexity?",Size of the model,Tokenization used by the model,Gendered and racial stereotypes in benchmarks,Probability of word sequences,C) Gendered and racial stereotypes in benchmarks
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",What is an important consideration when using perplexity to evaluate language models?,The color scheme of the model's interface,The tokenizer consistency across models being compared,The model's ability to generate text in multiple languages,The number of publications citing the model,B) The tokenizer consistency across models being compared
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81","In the context of large language models, what does the term 'hallucination' refer to?",The model's ability to generate human-like text,The model's tendency to fabricate information not present in the source data,The model's capability to learn from large datasets,The model’s process of copying text from the source material,B) The model's tendency to fabricate information not present in the source data
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",Which of the following best illustrates a potential harm associated with large language models?,Increased efficiency in data processing,Improved accuracy in language translation,Generation of misleading or factually incorrect information,Enhanced ability to understand human emotions,C) Generation of misleading or factually incorrect information
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81","When considering the implications of copyright laws on large language models, which statement is correct?",Large language models can easily circumvent copyright laws due to their inherent design,Copyright laws do not apply to the outputs generated by large language models,Large language models may inadvertently reproduce copyrighted material when generating new content,The use of large language models is fully exempt from copyright infringement concerns,C) Large language models may inadvertently reproduce copyrighted material when generating new content
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83","In the context of intellectual property, which of the following terms indicates that the use of copyrighted material without permission is potentially permissible?",Fair Use,Public Domain,Trademark Law,Copyright Infringement,A) Fair Use
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",Which concept refers to the practice of protecting personal data from unauthorized access and ensuring confidentiality?,Toxicity and Abuse,Privacy,Copyright,Fair Use,B) Privacy
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83","When considering the ethical aspects of digital content, which principle primarily deals with preventing harm and offensive behavior online?",Fair Use,Privacy,Copyright,Toxicity and Abuse,D) Toxicity and Abuse
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",Which of the following scenarios is an example of misinformation rather than phishing?,An email asks for personal information by falsely claiming to be from a bank.,A social media post spreads false information about a celebrity.,A message pretends to be from a tech company offering a fake software update.,A website claims to sell medication at an unbelievably low price to attract visitors.,B) A social media post spreads false information about a celebrity.
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85","In the context of large language models, what harm could result from the spread of misinformation?",Users may be unable to differentiate between real and fake news.,Language models could become outdated.,Language models may have increased processing speeds.,There will be an increase in the number of language models available.,A) Users may be unable to differentiate between real and fake news.
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",How might fraud be facilitated by the use of large language models?,Large language models can generate believable fake reviews.,Large language models can detect and prevent all cases of fraud.,Fraud is not related to large language models.,"Large language models can only analyze text, not generate it.",A) Large language models can generate believable fake reviews.
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85","Considering the risks associated with large language models, which of the following would be most effective in mitigating the harm of phishing attempts?",Increasing the complexity of the language model algorithms.,Training the language model to recognize and flag phishing content.,Disabling the language model's text generation feature.,Restricting the language model's access to the internet.,B) Training the language model to recognize and flag phishing content.
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87",What is the primary goal during the training of a neural network?,Increase the complexity of the model,Minimize the loss function between true and estimated values,Maximize the number of layers in the network,Reduce the accuracy to prevent overfitting,B) Minimize the loss function between true and estimated values
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87",Which of the following best describes the role of weight updates in neural network training?,Increase the weight value to improve model accuracy,Decrease the weight value to reduce computational complexity,Update weights to decrease the loss function by adjusting according to their contribution to the loss,Keep weights constant to maintain network stability,C) Update weights to decrease the loss function by adjusting according to their contribution to the loss
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87","In the context of backpropagation, what does the term 'chain rule' refer to?",A technique to increase the speed of forward propagation,A method to compute partial derivatives necessary for weight updates,A rule to chain multiple neural networks together,A formula used to calculate the loss function directly,B) A method to compute partial derivatives necessary for weight updates
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87",What is the significance of the learning rate (η) in neural network training?,It determines the number of epochs the network will train for,It controls the magnitude of weight updates during backpropagation,It specifies the number of layers in the neural network,It is the threshold for stopping training when loss is minimized,B) It controls the magnitude of weight updates during backpropagation
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","Given the initial weights w1=2, w2=-1, and bias b=1 for a single neuron, what is the output y after one forward pass with inputs (x1, x2) = (4, -3)?",5,12,7,14,B) 12
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","If the true output ytrue for the given inputs is 10, what is the computed loss L after the forward pass with the calculated output in the previous question?",2,4,16,8,B) 4
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91",Which of the following options correctly describes the first step in the backward pass process?,Update the weights and bias using the loss,Compute the derivative of the loss function,Create a computation graph including intermediate variables,Directly compute the gradients of the parameters,C) Create a computation graph including intermediate variables
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","If the output of the neuron y is less than the true value ytrue, what will be the sign of the loss L?",Positive,Negative,Zero,Cannot be determined,A) Positive
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93","In the backward pass, which intermediate node represents the result of the first weighted input, w1 multiplied by x1?",h1,h2,b,L,A) h1
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93",Which of the following nodes is necessary to compute the gradient of the loss with respect to w2?,h1,b,h2,L,C) h2
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93","In the context of backpropagation, what is the purpose of the node 'b' in the computation graph?",It represents the first weighted input.,It represents the sum of the weighted inputs and the bias.,It is used to calculate the loss.,It represents the second weighted input.,B) It represents the sum of the weighted inputs and the bias.
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93","In backpropagation, what is the significance of the node 'L' in the computation graph?",It represents the predicted output before loss calculation.,It represents the intermediate node before the final output.,It represents the loss function evaluated using the true labels and the predictions.,It is a placeholder for the next layer's input.,C) It represents the loss function evaluated using the true labels and the predictions.
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95",Which term represents the derivative of the loss function with respect to the bias term bh1 in the backpropagation process?,2(ytrue - y),x1,h2,2y - 2ytrue,A) 2(ytrue - y)
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95","In the context of backpropagation, how is the gradient of the loss L with respect to the output y calculated?",2(y - ytrue),w1x1,h2 + bh1,w2x2,A) 2(y - ytrue)
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95","If the loss L is defined as (ytrue - y)^2, what is the derivative of L with respect to y?",2(y - ytrue),2(ytrue - y),(y - ytrue)^2,-2(y - ytrue),A) 2(y - ytrue)
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","Given the loss function L=(ytrue-y)^2, what is the derivative of L with respect to y?",2(ytrue-y),(ytrue-y)^2,ytrue-y,2y,A) 2(ytrue-y)
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","In backpropagation, if η is the learning rate, w1 is the weight, x1 is the input, and the computed gradient of the loss with respect to w1 is 16, what is the new value of w1 after applying one step of gradient descent?",1.84,2.16,2,1.6,A) 1.84
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","When performing backpropagation, the loss gradient with respect to weight w2 is −12. If the learning rate η is 0.01, what is the update to w2?",-0.88,1.12,-0.12,0.88,A) -0.88
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","Based on the lecture content, what is the updated value of bias b after one step of gradient descent with learning rate η=0.01 and the computed gradient of the loss with respect to b being 4?",0.96,1.04,1,0.04,A) 0.96
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","In the backpropagation step, what is the updated value of w1 after one iteration with a learning rate of 0.01, given the partial derivative of the loss with respect to w1 is 16x1, where x1 is 4?",1.84,2.00,1.68,2.16,A) 1.84
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","Based on the lecture content, what is the calculated loss (L) if the true output (ytrue) is 10 and the predicted output (y) is 10.96?",0.096,0.01,0.96,1.00,A) 0.096
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","After applying the backpropagation algorithm with a learning rate of 0.01, what is the updated value of b?",0.96,0.88,0.92,0.86,A) 0.96
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","In the given neural network example, what is the gradient of the loss function with respect to w2, denoted as 𝜕𝐿𝜕𝑤2, when the loss has a dependency on w2 through the term -12?",-12,4,12,0.88,A) -12
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101",What does the term 'backprop' refer to within the context of the lecture content?,A method for visualizing neural network decisions,A technique for pruning unnecessary connections in a neural network,A process for propagating errors backwards through a neural network to update weights,A strategy for initializing weights in a neural network,C) A process for propagating errors backwards through a neural network to update weights
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103",What is the purpose of using multiple attention heads in a transformer model?,"To attend to the context for different purposes, allowing each head to learn different linguistic relationships or patterns",To increase the dimensionality of the key and query embeddings,To reduce the computational complexity of the self-attention layer,To decrease the number of weight layers required in the model,"A) To attend to the context for different purposes, allowing each head to learn different linguistic relationships or patterns"
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","In a multi-head attention layer, what do the weight layers WQi, WKi, and WVi represent?","They represent the input, output, and value embeddings respectively","They are used to project the inputs into separate key, value, and query embeddings for each head",They are used to concatenate the outputs from each head,They are used to compute the softmax of the attention scores,"B) They are used to project the inputs into separate key, value, and query embeddings for each head"
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103",What is the dimensionality of the key and query embeddings in the original transformer paper?,64,512,8,dv,A) 64
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103",How is the output of a multi-head attention layer computed?,By concatenating the outputs of each head and projecting down to the desired output dimensionality,By summing the outputs of each head,By computing the softmax of the attention scores for each head,By averaging the outputs of each head,A) By concatenating the outputs of each head and projecting down to the desired output dimensionality
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the purpose of the mask matrix in the multi-head attention calculation?,To eliminate the knowledge of words that follow in the sequence,To increase the dimensionality of the embeddings,To add noise to the attention scores,To prevent overfitting during training,A) To eliminate the knowledge of words that follow in the sequence
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105","How many weight layers are used for key, query, and value embeddings in each head of the multi-head attention mechanism?",8,N,A,1,A) 8
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",Which of the following best describes the effect of using multiple attention heads in the multi-head attention layer?,"Each head attends to the context differently, enabling the model to capture different linguistic relationships or patterns in the context",Multiple heads reduce the computational complexity of the attention mechanism,Multiple heads are used to increase the dimensionality of the input embeddings,Multiple heads allow the model to focus on a single contextual relationship at a time,"A) Each head attends to the context differently, enabling the model to capture different linguistic relationships or patterns in the context"
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the shape of the output vector ai after the multi-head attention calculation?,1 x dv,1 x d,1 x hdv,1 x dv x h,C) 1 x hdv
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",How does the mask matrix prevent the self-attention mechanism from attending to future tokens in language modeling tasks?,By setting the upper-triangular portion of the QK| matrix to zero,By multiplying the softmax output by the mask matrix,By adding noise to the attention scores,By reducing the dimensionality of the embeddings,A) By setting the upper-triangular portion of the QK| matrix to zero
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",What is the purpose of the input token embedding and input positional embedding in the transformer model?,To represent the initial representation of each input token and its position in the sequence,To normalize the embedding vectors in parallel,To increase the dimensionality of the embedding vectors,To apply the same FFN in parallel to each embedding vector,A) To represent the initial representation of each input token and its position in the sequence
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",How is the embedding for a specific input token determined in the transformer model?,By multiplying the one-hot vector of the token with the embedding matrix,By adding an input positional embedding to the input token embedding,By feeding the token to a feed-forward neural network,By applying layer normalization to the token embedding,A) By multiplying the one-hot vector of the token with the embedding matrix
notes/LLM_cs124_week7_2025.pdf,57,"113, 114","In the transformer model, what is the purpose of the one-hot vector representation of tokens?",To represent the initial representation of each input token,To normalize the embedding vectors in parallel,To select the corresponding rows from the embedding matrix,To increase the dimensionality of the embedding vectors,C) To select the corresponding rows from the embedding matrix
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",What is the shape of the embedding matrix E in the transformer model?,|V|⇥d,N⇥d,N⇥|V|,d⇥|V|,A) |V|⇥d
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",What is the purpose of Layer Normalization in the self-attention computation of the transformer model?,To increase the dimensionality of the embedding vectors,To add an input positional embedding to the input token embedding,To normalize the embedding vectors in parallel,To select the corresponding rows from the embedding matrix,C) To normalize the embedding vectors in parallel
