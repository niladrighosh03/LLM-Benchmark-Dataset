pdf_name,chunk_number,total_chunks,pages,question,option_A,option_B,option_C,option_D,correct_answer
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",What is a primary function of both simple n-gram language models and large language models?,Assigning probabilities to word sequences,Encrypting text data,Translating languages,Summarizing text,A) Assigning probabilities to word sequences
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",How are large language models different from simple n-gram models in terms of training?,Large language models use encryption during training,Large language models are trained to guess the next word,Simple n-gram models learn language knowledge from text,Simple n-gram models generate text by sampling possible next words,B) Large language models are trained to guess the next word
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",Which statement best describes the training data for large language models?,Large language models are trained on text from a single source,Large language models only use synthesized data for training,Large language models are trained on a diverse set of texts,Large language models use small datasets for faster training,C) Large language models are trained on a diverse set of texts
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",What is a key difference between simple n-gram models and large language models regarding text generation?,Simple n-gram models can generate longer texts,Large language models do not rely on text generation,Large language models generate text by sampling possible next words,Simple n-gram models are trained on more text data,C) Large language models generate text by sampling possible next words
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3","Considering their training, why are large language models more effective in understanding language compared to simple n-gram models?",Because they can encrypt text,Because they are trained on diverse text data,Because they only generate short sequences,Because they are trained on synthesized data,B) Because they are trained on diverse text data
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which of the following models is referred to as a decoder-only language model?,BERT,GPT,Flan-T5,Whisper,B) GPT
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",What is the significant characteristic of decoder-only models?,Can condition on future words,Generates language from left to right,Provides bidirectional context,Both B and C,B) Generates language from left to right
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",How are language models like GPT and Claude typically pretrained?,Predicting the next word in a sequence,Conditioning on future words,Classifying text,Translating languages,A) Predicting the next word in a sequence
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which of the following architectures can condition on future words during pretraining?,Decoders,Encoders,Encoder-Decoders,All of the above,B) Encoders
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",What is a common term used to describe Autoregressive LLMs?,Predictive Language Models,Causal LLMs,Contextual Encoders,Bidirectional LLMs,B) Causal LLMs
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which of the following architectures cannot condition on future words when generating text?,Decoders,Encoders,Encoder-Decoders,Masked Language Models,A) Decoders
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7","In the context of Large Language Models (LLMs), what does 'Causal LLMs' refer to?",Models that predict future words based on past context,Models that use bidirectional context for prediction,Models that condition on future words for prediction,Models that map from one sequence to another,A) Models that predict future words based on past context
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",How do Encoder-Decoder architectures typically differ from traditional LLMs in their application?,They can only generate text from left to right,They are primarily used for classification tasks,They are often used for tasks that involve mapping from one sequence to another,They cannot use bidirectional context,C) They are often used for tasks that involve mapping from one sequence to another
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which of the following is a popular architecture for building strong representations through bidirectional context?,Decoders,Encoders,Masked Language Models,Encoder-Decoders,B) Encoders
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",What is a primary use case for Encoder-Decoder architectures in the field of natural language processing?,Classification tasks,Language modeling,Machine translation,Word prediction without considering future words,C) Machine translation
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Which of the following best describes the predominant direction of word prediction in 'decoder-only models'?,Right-to-left,Left-to-right,Bottom-up,Top-down,B) Left-to-right
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9","In the context of Encoder-Decoder architectures, what is the primary advantage of using an encoder?",It can generate text from scratch,It processes input sequentially only,It can condition on future words in the sequence,It requires less computational power,C) It can condition on future words in the sequence
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Pretraining for neural architectures is influenced by which of the following factors?,The color of the model,The type of pretraining tasks,The size of the input data,The brand of the GPU used,B) The type of pretraining tasks
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9","When referring to LLMs like ChatGPT or Claude, which training approach is highlighted?",Simultaneous bidirectional training,"Causal, autoregressive training",Randomized training,Frequency-based training,"B) Causal, autoregressive training"
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Which type of model architecture is typically associated with the capability to generate language models?,Encoder-only models,Decoder-only models,Encoder-Decoder models,All of the above,D) All of the above
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",What is the sentiment of the sentence 'I like Jackie Chan' based on the language model's prediction?,Positive,Negative,Neutral,Ambiguous,A) Positive
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",Who is the probable author of the book 'The Origin of Species' according to the language model?,Charles Darwin,Albert Einstein,Isaac Newton,Galileo Galilei,A) Charles Darwin
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13","In the context of sentiment analysis, which word is more likely to follow the phrase 'The sentiment of the sentence 'I like Jackie Chan''?",Positive,Negative,Neutral,Confused,A) Positive
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",What kind of model architecture is used for large language modeling in the lecture?,Recurrent Neural Networks (RNNs),Transformers,Convolutional Neural Networks (CNNs),Autoencoders,B) Transformers
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",Which token suggests that a summary should follow in the context of text summarization?,tl;dr,FYI,IMHO,TBD,A) tl;dr
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",Which type of neural network stacks are primarily used in modern language models?,Convolutional Neural Networks,Recurrent Neural Networks,Transformers,Radial Basis Function Networks,C) Transformers
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",What is the primary function of a language model (LM) in computational linguistics?,To perform syntax highlighting,To compute the probability of a word following a given prefix string,To translate languages in real-time,To correct spelling mistakes,B) To compute the probability of a word following a given prefix string
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15","In the context of Large Language Models (LLMs), what is the purpose of the 'prefix string'?",To serve as a unique identifier for documents,To define the beginning of a new paragraph,To indicate the start of a new sentence,To provide context for predicting the next word in a sequence,D) To provide context for predicting the next word in a sequence
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",Which of the following is a key characteristic of Transformers in the context of language modeling?,They rely exclusively on convolutional layers,They are built using only recurrent layers,They are designed to handle sequential data without the need for recurrence,They are primarily used for image recognition tasks,C) They are designed to handle sequential data without the need for recurrence
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",The term 'stacks of neural networks' in language modeling primarily refers to which architectural concept?,A linear progression of neural network layers,A deep learning model where each layer is independent,A combination of different neural network architectures in sequence,A hierarchical structure of multiple neural network models,D) A hierarchical structure of multiple neural network models
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Who proposed replacing RNNs with self-attention in the Transformer model?,Noam Shazeer,Jakob Uszkoreit,Ashish Vaswani,Illia Polosukhin,B) Jakob Uszkoreit
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What does the Transformer model primarily dispense with?,Recurrence and Convolution,Encoder-Decoder mechanism,Attention mechanisms,Feedforward networks,A) Recurrence and Convolution
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What was the BLEU score achieved by the Transformer model on the WMT 2014 English-to-German translation task?,26.4 BLEU,28.4 BLEU,30.4 BLEU,32.4 BLEU,B) 28.4 BLEU
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Which person was crucially involved in designing and implementing the first Transformer models?,Ashish Vaswani,Niki Parmar,Llion Jones,Jakob Uszkoreit,A) Ashish Vaswani
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",How many GPU days did it take the Transformer model to achieve state-of-the-art on the WMT 2014 English-to-French translation task?,1.5 GPU days,2.5 GPU days,3.5 GPU days,4.5 GPU days,C) 3.5 GPU days
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which year marked the initial use of GPUs in the development of language models?,2003,2006,2012,2018,B) 2006
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",What is the primary function of Attention in the context of Transformers?,To reduce the size of the input data,To focus on different parts of the input sequence,To encode the input sequence into a fixed-size vector,To replace the need for Recurrent Neural Networks,B) To focus on different parts of the input sequence
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19","In the context of neural network architectures, what did the 'StackedTransformerBlocks' primarily contribute to?",Improving the efficiency of static word embeddings,Enhancing the performance of multi-task learning,Facilitating the development of the Transformer architecture,Increasing the computational power of GPUs,C) Facilitating the development of the Transformer architecture
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which of the following years is associated with the rediscovery of Static Word Embeddings?,2013,2015,2017,2019,A) 2013
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",What was the significant advancement in 2018 that influenced the development of language models?,The introduction of Contextual Word Embeddings and Pretraining,The invention of the Attention mechanism,The widespread adoption of GPUs,The emergence of Stacked Transformer Blocks,A) The introduction of Contextual Word Embeddings and Pretraining
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",What does the term 'static embeddings' imply in the context of word meanings?,Word meanings are fixed and do not change with context.,Word meanings change dynamically with context.,Word meanings are derived from sentence structure.,Word meanings are determined by the length of the word.,A) Word meanings are fixed and do not change with context.
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",Why might 'static embeddings' like word2vec be problematic?,They do not reflect changes in word meaning across different contexts.,They require too much computational power.,They only work for very long words.,They cannot handle new words.,A) They do not reflect changes in word meaning across different contexts.
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",What distinguishes contextual embeddings from static embeddings?,Contextual embeddings are computationally easier to generate.,Contextual embeddings represent a word's meaning as fixed across all contexts.,Contextual embeddings provide a unique vector representation for a word based on its surrounding words.,Contextual embeddings are only used in language modeling tasks.,C) Contextual embeddings provide a unique vector representation for a word based on its surrounding words.
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21","In the statement 'The chicken didn't cross the road because it was too tired', what issue illustrates the limitation of static embeddings?","The word 'it' can refer to either the chicken or the road in different contexts, but static embeddings cannot capture this ambiguity.",The length of the sentence is too long for static embeddings to handle.,Static embeddings cannot process the word 'cross' correctly.,Static embeddings do not understand the concept of tiredness.,"A) The word 'it' can refer to either the chicken or the road in different contexts, but static embeddings cannot capture this ambiguity."
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",How are contextual embeddings computed according to the lecture content?,By applying attention mechanisms within transformer blocks.,By using a fixed vector for each word regardless of context.,Through simple averaging of surrounding word embeddings.,By encoding words in isolation without considering context.,A) By applying attention mechanisms within transformer blocks.
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",Why are contextual embeddings important for understanding the meaning of a word?,They provide a static representation of a word's meaning.,They ensure that each word has a unique vector that changes with context.,"They only focus on the word itself, without considering its neighbors.",They represent the frequency of a word's usage in a corpus.,B) They ensure that each word has a unique vector that changes with context.
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23","In the sentence 'The chicken didn't cross the road because it was too tired,' what does 'it' most likely attend to?",The neighboring words 'chicken' and 'road',The neighboring word 'wide',Only the word 'cross',The words in the previous sentence,A) The neighboring words 'chicken' and 'road'
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",How does attention contribute to the computation of contextual embeddings?,By giving equal weight to all words in a sentence,By emphasizing the frequency of word appearances,By selectively integrating information from neighboring words,By ignoring the neighbors and focusing on distant words,C) By selectively integrating information from neighboring words
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",Which of the following statements is true regarding contextual embeddings?,Each word in a sentence has a fixed representation regardless of context.,Contextual embeddings are less accurate than traditional embeddings.,The representation of a word can change depending on the surrounding words.,Contextual embeddings disregard the order of words in a sentence.,C) The representation of a word can change depending on the surrounding words.
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23","Given the statement 'The chicken didn't cross the road because it was too wide,' what does this suggest about the contextual embedding of 'it'?",It likely represents the chicken as 'it' is the subject of the sentence.,"It likely represents the road, as 'wide' is more commonly associated with roads than chickens.",It has no significant meaning as 'it' is too ambiguous.,"It refers to the act of crossing, not an object.","B) It likely represents the road, as 'wide' is more commonly associated with roads than chickens."
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25","In the context of attention mechanisms, what is the primary function of the tokens at layer k+1?",They are used for computing the final output only.,They represent the attention distribution across input tokens.,They store the original input tokens without any modifications.,They replace the self-attention mechanism.,B) They represent the attention distribution across input tokens.
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",How does the attention mechanism in neural networks relate to the concept of 'attending to' neighboring words?,It ignores neighboring words to focus on the current word.,It randomly selects which neighboring words to attend to.,It selectively integrates information from neighboring words to form an embedding.,It attends to neighboring words based on their lengths only.,C) It selectively integrates information from neighboring words to form an embedding.
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",What does the 'self-attention distribution' in a neural network layer represent?,The distribution of attention across different layers in the network.,The weighted importance of each token relative to others within the same layer.,The uniform distribution of attention applied to all tokens.,The distribution of tokens that are not attended to at all.,B) The weighted importance of each token relative to others within the same layer.
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25","In the analogy of 'The chick did not cross the road because it was too tired', how does the concept of attention apply?",Attention determines which part of the sentence is grammatically correct.,Attention helps in understanding the context by focusing on relevant words.,Attention modifies the meaning of 'road' and 'tired'.,Attention is not applicable in this analogy.,B) Attention helps in understanding the context by focusing on relevant words.
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",Which statement accurately reflects the role of attention mechanisms in computing vector representations of tokens?,Attention mechanisms replace the need for vector representations of tokens.,Attention mechanisms distribute vectors uniformly across all tokens.,Attention mechanisms compute vector representations by ignoring context.,"Attention mechanisms compute vector representations through a weighted sum of vectors, taking context into account.","D) Attention mechanisms compute vector representations through a weighted sum of vectors, taking context into account."
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31",What role does the 'query' play in an attention head?,Represents a preceding input being compared to the current element,As the current element being compared to the preceding inputs,A value of a preceding element that gets weighted and summed,A type of operation in the attention mechanism,B) As the current element being compared to the preceding inputs
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31",Which of the following best describes the 'key' in the context of attention mechanisms?,A function that determines the weight of an element,A preceding input that is being compared to the current element to determine a similarity,The current element being compared in the self-attention mechanism,The output of the attention head,B) A preceding input that is being compared to the current element to determine a similarity
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","In the context of attention mechanisms, what is the role of the 'value'?",A type of operation in the attention mechanism,A preceding input that is being compared to the current element,A value of a preceding element that gets weighted and summed,The current element being compared in the self-attention mechanism,C) A value of a preceding element that gets weighted and summed
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","In the given lecture content, which layer is described as having a self-attention distribution?",Layer k-1,Layer k,Layer k+1,No specific layer,C) Layer k+1
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","Based on the lecture content, what does the repetition of 'Thechickendidn’tcrosstheroadbecauseitwastootired' signify in the context of attention mechanisms?",It represents the values being processed by the attention head,It is an example of the query being compared to other elements,It is a malfunction in the attention distribution,It illustrates the input tokens for the attention mechanism,D) It illustrates the input tokens for the attention mechanism
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33","What is the purpose of the weight matrices WQ, WK, and WV in a transformer's self-attention head?",They are used to compute the dot product between query and key vectors.,"They are used to project each input vector into a representation of its role as query, key, and value.",They are used to compute the softmax weight.,They are used to compute the value of the prior elements.,"B) They are used to project each input vector into a representation of its role as query, key, and value."
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",What is the role of the softmax operation in the transformer's self-attention mechanism?,It computes the weighted sum of the value vectors.,It normalizes the similarity scores into a probability distribution.,It projects each input vector into a representation of its role.,It computes the dot product between the query and key vectors.,B) It normalizes the similarity scores into a probability distribution.
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",Which factor is used to scale the dot product in the transformer's self-attention mechanism to avoid numerical issues?,The number of input vectors.,The dimensionality of the query and key vectors.,The number of attention heads.,The dimensionality of the value vectors.,B) The dimensionality of the query and key vectors.
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33","In the context of a transformer's self-attention mechanism, what is the role of a 'query' vector?",It is used to compute the dot product with all key vectors.,It is projected into a representation of its role as a value.,It is used to compute the softmax weight.,It is used to compute the value of the prior elements.,A) It is used to compute the dot product with all key vectors.
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",Which of the following best describes the process of computing the value of the third output 'a3' in a sequence using self-attention?,"Weighted sum of the values of the prior elements, each weighted by the similarity of its key to the query from the current element.","Sum of all query, key, and value vectors.","Dot product between the query vector and all key vectors, followed by softmax normalization.","Projection of each input vector into a representation of its role as query, key, and value.","A) Weighted sum of the values of the prior elements, each weighted by the similarity of its key to the query from the current element."
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the purpose of using multiple attention heads in transformer models?,To represent different aspects of the relationships among inputs,To reduce the model size and computational cost,To simplify the attention process,To enhance the model's ability to memorize inputs,A) To represent different aspects of the relationships among inputs
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37","What do the weight matrices WQ, WK, and WV represent in a single attention head?","These matrices represent the dimensionality of the inputs, keys, and values","These matrices are used to project each input vector into a representation of its role as a key, query, or value","These matrices are used to reduce the dimensionality of the inputs, keys, and values",These matrices are used to randomly initialize the attention head parameters,"B) These matrices are used to project each input vector into a representation of its role as a key, query, or value"
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37","In the context of self-attention, what does the term 'softmax' refer to?",A type of activation function used in neural networks,A type of weight initialization method,A type of regularization technique,A function used to convert the dot product of queries and keys into a probability distribution,D) A function used to convert the dot product of queries and keys into a probability distribution
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37","In the equation for calculating the value of a3 using causal self-attention, what does the term 'a_i' represent?",The value of the i-th element in the sequence,The query vector for the i-th element in the sequence,The key vector for the i-th element in the sequence,The weighted sum of value vectors for the i-th element in the sequence,D) The weighted sum of value vectors for the i-th element in the sequence
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the main purpose of using the softmax function in the attention mechanism?,To normalize the dot product scores between queries and keys,To initialize the attention weights,To reduce the dimensionality of the input vectors,To introduce randomness into the attention process,A) To normalize the dot product scores between queries and keys
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",Which of the following best describes the result of using attention in token representation?,Each word has a static embedding regardless of context.,Token representations become contextually enriched and thus vary with context.,Attention eliminates the need for token representations.,Token representations become less important in the model.,B) Token representations become contextually enriched and thus vary with context.
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39","In the context of Transformers, what is the primary function of the Transformer Block?",To reduce the dimensionality of the input data.,To convert token representations into a final output.,To apply attention mechanisms and pass enriched representations.,To randomly initialize token representations for variability.,C) To apply attention mechanisms and pass enriched representations.
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",How does the attention mechanism in Transformers affect the representation of tokens across layers?,It prevents tokens from passing through multiple layers.,It keeps the token representation the same through all layers.,It allows token representations to be contextually enriched in each layer.,It randomly alters token representations in each layer.,C) It allows token representations to be contextually enriched in each layer.
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",What is the outcome of contextually enriching token representations in Transformers?,Tokens are disregarded in later layers.,Token embeddings become less informative over time.,"Each token's embedding is unique to its context, enhancing understanding.",The context of tokens becomes irrelevant to the model.,"C) Each token's embedding is unique to its context, enhancing understanding."
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39","Considering the role of attention mechanisms in Transformers, which statement is true?",Attention mechanisms simplify the model by ignoring context.,Attention mechanisms replace the need for layered processing.,Attention mechanisms ensure that token representations are independent of context.,Attention mechanisms enable a dynamic and context-dependent representation of tokens.,D) Attention mechanisms enable a dynamic and context-dependent representation of tokens.
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","In the context of Transformer blocks, what does 'E1+', 'E2+', 'E3+', 'E4+', and 'E5+' represent?",Error tokens,Encoded input tokens,Embedding layers,Encoded output tokens,C) Embedding layers
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",How are the tokens processed in the residual stream of a Transformer block?,Each token is processed in isolation without modification,Each token gets passed up and modified,Each token is passed down and modified,Each token is replaced with a fixed value,B) Each token gets passed up and modified
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",What is the purpose of 'Layer Norm' in the Transformer block?,Normalizing the input tokens,Normalizing the output tokens,Normalizing the hidden state before passing it to the feedforward network,Normalizing the residual stream,C) Normalizing the hidden state before passing it to the feedforward network
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","In the context of Transformer blocks, what does 'logitslogitslogitslogitslogitsTransformer language model' represent?",The output of the final layer in the Transformer,The input to the Transformer language model,The initial weights of the Transformer,The intermediate representation of the input tokens,A) The output of the final layer in the Transformer
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",Which component in the Transformer block process is responsible for attention mechanisms?,Layer Norm,Feedforward network,MultiHeadAttention,InputEncoding,C) MultiHeadAttention
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43","In the transformer block architecture, what is the purpose of the residual stream?",To normalize vectors,To add the input of a component to its output,To pass through a feedforward layer,To provide non-linearity,B) To add the input of a component to its output
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43","What is the dimensionality of the hidden layer in the feedforward network of a transformer block, relative to the model dimensionality?",Smaller than the model dimensionality,Equal to the model dimensionality,Larger than the model dimensionality,Not related to the model dimensionality,C) Larger than the model dimensionality
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43","In the transformer block architecture, where does layer normalization occur?",Before the attention and feedforward layers,After the attention and feedforward layers,Only before the attention layer,Only after the feedforward layer,A) Before the attention and feedforward layers
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",Which layer is responsible for adding non-linearity to the transformer block?,LayerNorm,MultiHeadAttention,Feedforward layer,Residual stream,C) Feedforward layer
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",Which component of the transformer block processes the input vector xi before it is added back into the residual stream?,LayerNorm,MultiHeadAttention,Feedforward layer,Both A and B,D) Both A and B
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45","In the context of transformer blocks, for which component does multi-head attention consider information from other tokens?",Layer Norm,MultiHeadAttention,FeedForward,Residual Stream,D) Residual Stream
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What do Elhage et al. (2021) suggest about the attention heads in transformer blocks?,They ignore neighboring tokens,They move information from neighboring tokens to the current token,They only focus on the current token,They process tokens independently,B) They move information from neighboring tokens to the current token
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",How is the output of LayerNorm(x) represented mathematically?,LayerNorm(x) = g(x - µ) + b,LayerNorm(x) = g(x + µ) + b,LayerNorm(x) = (x - µ) / s,LayerNorm(x) = g((x - µ) / s) + b,D) LayerNorm(x) = g((x - µ) / s) + b
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",Which component in the transformer block adds the output from attention to its token's embedding stream?,Layer Norm,MultiHeadAttention,FeedForward,Residual Stream,B) MultiHeadAttention
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45","In the transformer block equation, what does 't5i' represent?",LayerNorm(xi),"MultiHeadAttention(LayerNorm(xi), ⇥x11, ..., x1N)","LayerNorm(MultiHeadAttention(LayerNorm(xi), ⇥x11, ..., x1N)) + xi","FeedForward(LayerNorm(MultiHeadAttention(LayerNorm(xi), ⇥x11, ..., x1N)) + xi) + (LayerNorm(xi) + MultiHeadAttention(LayerNorm(xi), ⇥x11, ..., x1N))","D) FeedForward(LayerNorm(MultiHeadAttention(LayerNorm(xi), ⇥x11, ..., x1N)) + xi) + (LayerNorm(xi) + MultiHeadAttention(LayerNorm(xi), ⇥x11, ..., x1N))"
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47","In the context of Transformer blocks, what does layer normalization compute first?",Standard deviation of the vector,Mean of the vector,Gain and offset values,Normalized vector,B) Mean of the vector
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",What is the primary purpose of using layer normalization in Transformer blocks?,To introduce gain and offset values,To keep the values of a hidden layer within a range that facilitates gradient-based training,To compute the mean and standard deviation of the vector,To shift the embeddings space,B) To keep the values of a hidden layer within a range that facilitates gradient-based training
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",Which component in a Transformer block takes as input information from other tokens?,Layer Normalization,Feedforward,MultiHeadAttention,Residual Streams,C) MultiHeadAttention
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",What is the dimensionality of all vectors in a Transformer block?,It varies depending on the layer,The same as the dimensionality of the input,The same as the dimensionality of the output,The same as the dimensionality of the embedding vectors,D) The same as the dimensionality of the embedding vectors
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47","In the standard implementation of layer normalization, what do the learnable parameters 'g' and 'b' represent?",Mean and standard deviation,Gain and offset values,Input and output dimensions,Layer normalization and multi-head attention,B) Gain and offset values
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",Which of the following best describes the purpose of position embeddings in a Transformer block?,To provide a unique identifier for each token in the sequence,To encode the relative or absolute position of tokens within the sequence,To replace the need for token embeddings,To increase the dimensionality of the token embeddings,B) To encode the relative or absolute position of tokens within the sequence
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",What is the resulting shape of the matrix X after adding token and positional embeddings in a Transformer block?,[N x d],[d x N],[N x N],[N^2 x d],A) [N x d]
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49","In the context of Transformer blocks, what does the dimension 'd' represent in the shape [N x d] of matrix X?",The number of tokens in the sequence,The dimensionality of the token embeddings,The total number of parameters in the model,The length of the positional embeddings,B) The dimensionality of the token embeddings
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",Which statement is true regarding the embeddings used in Transformer blocks?,Token and positional embeddings are combined by multiplying them together,"Token embeddings represent the meaning of the word, while positional embeddings provide information about the word's position","Only one type of embedding is used, either token or positional, as both provide the same information","Token embeddings are used to determine the word position, while positional embeddings provide the word meaning","B) Token embeddings represent the meaning of the word, while positional embeddings provide information about the word's position"
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",What is the primary role of the Language Model Head in the context of Transformer blocks?,To generate token embeddings,To predict the next word in a sequence,To reduce the dimensionality of the embeddings,To replace the position embeddings,B) To predict the next word in a sequence
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53","In tokenization using BPE, which sequence of vocab indices corresponds to the input string 'Thanks for all the'?","[5, 4000, 10532, 2224]","[5, 1000, 10532, 2224]","[10532, 4000, 2224, 5]","[2224, 5, 4000, 10532]","A) [5, 4000, 10532, 2224]"
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",What is the role of the language modeling head in a transformer block?,Combine word and position embeddings,Map a [1 x d] vector to a probability distribution over the vocabulary,Encode the position of words in a sequence,Generate the next word in a sequence,B) Map a [1 x d] vector to a probability distribution over the vocabulary
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",How does a transformer block output a probability distribution over the vocabulary?,Using a softmax function directly on the [1 x d] vector,"Mapping the [1 x d] vector to a [1 x |V|] vector of logits using the unembedding matrix, then applying softmax",Multiplying the [1 x d] vector by the word embeddings,Applying the position embeddings to the [1 x d] vector,"B) Mapping the [1 x d] vector to a [1 x |V|] vector of logits using the unembedding matrix, then applying softmax"
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",Which of the following steps is involved in translating a transformer block's output into a probability distribution over the vocabulary?,Applying the position embeddings to the output vector,"Mapping the output vector to a vector of logits using the unembedding matrix, then applying softmax",Combining the output vector with word embeddings,Directly applying softmax to the output vector,"B) Mapping the output vector to a vector of logits using the unembedding matrix, then applying softmax"
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",What does the term 'unembedding matrix' refer to in the context of a language modeling head?,A matrix that reverses the process of embedding words into a vector space,A matrix used to combine word and position embeddings,A matrix that maps a [1 x d] vector to a [1 x |V|] vector of logits,A matrix that directly generates probabilities from a [1 x d] vector,C) A matrix that maps a [1 x d] vector to a [1 x |V|] vector of logits
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55","In the context of transformer models, what is the primary purpose of the softmax function within the language modeling head?",To generate the next word in a sequence,To map a [1 x d] vector to a [1 x |V|] vector of probabilities,To encode the position of words in a sequence,To combine word and position embeddings into a single vector,B) To map a [1 x d] vector to a [1 x |V|] vector of probabilities
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What is the function of the unembedding matrix in the language modeling head?,It maps from a [1 x d] vector to a [1 x |V|] vector of probabilities.,It maps from a [1 x d] vector to a [1 x |V|] vector of logits.,It applies softmax to a [1 x |V|] vector of logits.,It directly outputs a probability distribution over the vocabulary.,B) It maps from a [1 x d] vector to a [1 x |V|] vector of logits.
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What is the shape of the unembedding matrix E in the language modeling head?,[1 x |V|],[|V| x d],[d x |V|],It varies based on the dataset size.,C) [d x |V|]
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57","In the language modeling head, what follows the application of the unembedding matrix?",The probabilities are directly computed.,The softmax function is applied to the vector of logits.,An additional linear transformation is applied.,Logits are squared to emphasize the most probable tokens.,B) The softmax function is applied to the vector of logits.
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",Which component in the language modeling head is responsible for creating a probability distribution over the vocabulary?,The unembedding matrix,The embeddings layer,The softmax function,The TransformerBlock layer,C) The softmax function
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What is the purpose of applying softmax after the unembedding matrix in the language modeling head?,To convert the vector of logits into a vector of probabilities.,To increase the dimensionality of the data.,To reduce the dimensionality of the data.,To map the vector of logits back to the original embedding space.,A) To convert the vector of logits into a vector of probabilities.
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the shape of the unembedding layer's output in a transformer language model?,[|V| x d],[1 x |V|],[d x |V|],[1 x d],B) [1 x |V|]
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59","In the context of a transformer language model, what does the softmax layer accomplish?",Converts embeddings into logits,Performs weight tying,Turns logits into probabilities over vocabulary,Projects from hLN to a logit vector,C) Turns logits into probabilities over vocabulary
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",How does the unembedding layer relate to the embedding matrix in a transformer model?,It projects from an embedding to a logit vector,It is used to map from a one-hot vector to an embedding,It is the transpose of the embedding matrix,It performs the reverse mapping from an embedding to a vector over the vocabulary,D) It performs the reverse mapping from an embedding to a vector over the vocabulary
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What vocabulary size does the output of the unembedding layer represent?,The size of the transformer's context window,The size of the transformer's embedding layer,The size of the vocabulary V,The number of transformer blocks,C) The size of the vocabulary V
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the primary purpose of the language modeling head in a transformer language model?,To project from hLN to a logit vector,To map from an embedding to a logit vector,To map the output embedding of tokenN to a probability distribution over words,To perform weight tying between the embedding and unembedding layers,C) To map the output embedding of tokenN to a probability distribution over words
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the primary function of the language modeling head in a transformer model?,Project from the output embedding to a probability distribution over words,Turn the logits into probabilities over the vocabulary,Map the input token embedding to its corresponding vocabulary entry,Perform attention operations on the input sequence,A) Project from the output embedding to a probability distribution over words
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61","In the context of transformer language models, what does 'context' typically refer to?",The size of the input sequence,The number of layers in the transformer,The amount of data the model was trained on,The vocabulary size of the model,A) The size of the input sequence
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",How does the language modeling head map from the output embedding to vocabulary probabilities?,By using a softmax layer on the logits produced by a linear layer,By directly outputting the probabilities without any intermediate layers,By using a feedforward layer followed by an attention layer,By randomly selecting probabilities for each word in the vocabulary,A) By using a softmax layer on the logits produced by a linear layer
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the role of the 'E' matrix in a transformer language model?,It transforms logits into probability distributions,It is tied to the transpose of the matrix to perform unembedding,It is the output of the last transformer layer,It is used to perform attention operations,B) It is tied to the transpose of the matrix to perform unembedding
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61","In transformer models, what does the softmax layer convert the logits into?",Logits for the next token,Probabilities over the vocabulary,Embedding vectors for each token,Context vectors for the transformer layers,B) Probabilities over the vocabulary
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",What is the initial step in pretraining Large Language Models?,Fine-tuning the model on a specific task,Pretraining the transformer model on large text corpora,Applying the model to new tasks immediately,Incorporating position embeddings,B) Pretraining the transformer model on large text corpora
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",Which component of the Transformer model aids in understanding the order of words in a sentence?,Attention Mechanism,Language Model Head,Position Embeddings,Transformer Layers,C) Position Embeddings
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",What is the primary purpose of a Language Model Head in a Transformer model?,To generate position embeddings,To pretrain the model on large text corpora,To facilitate the transformation of input to output,To predict the likelihood of a sequence of words,D) To predict the likelihood of a sequence of words
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",How does the process of pretraining Large Language Models typically conclude?,By applying the model to new tasks,By incorporating position embeddings,By fine-tuning the model on specific tasks,By pretraining on large text corpora,C) By fine-tuning the model on specific tasks
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",Which of the following steps is NOT part of the pretraining process for Large Language Models?,Pretraining on large text corpora,Applying the model to new tasks,Incorporating attention mechanisms,Fine-tuning the model on specific tasks,B) Applying the model to new tasks
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the primary training strategy used in self-supervised training algorithms for language models?,Cross-entropy loss minimization,Predicting the next word in a sequence,Reinforcement learning,Supervised classification tasks,B) Predicting the next word in a sequence
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65","In language model pretraining, what is the purpose of generating a probability distribution over the vocabulary?",To classify the text into different languages,To predict the next word in a sequence,To encode the text into a fixed-length vector,To filter out irrelevant words,B) To predict the next word in a sequence
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",Which of the following best describes the 'self-supervised' aspect in language model training?,The model is trained without any human intervention,The model uses the next word as the label for training,The model is trained using large amounts of labeled data,The model is pre-trained on a specific task before fine-tuning,B) The model uses the next word as the label for training
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65","In the context of language model pretraining, what does the term 'transformer model' refer to?",A neural network architecture that focuses on transforming input data,A model that translates text from one language to another,A machine learning technique for transforming categorical data,An algorithm for transforming sequences into fixed-length vectors,A) A neural network architecture that focuses on transforming input data
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the main goal of pretraining a transformer model on a large corpus of text?,To understand the grammatical structure of different languages,To prepare the model for a specific downstream task,To generate a large amount of synthetic text data,To improve the model's ability to perform general language understanding tasks,D) To improve the model's ability to perform general language understanding tasks
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67","In the context of language modeling, what does the cross-entropy loss function primarily measure?",The similarity between two sentences,The accuracy of the model's predictions for the next token,The number of tokens the model can generate,The speed at which the model processes text,B) The accuracy of the model's predictions for the next token
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",What is the main goal of using the cross-entropy loss function in language models?,To minimize the difference between predicted and actual word frequencies,To maximize the speed of model training,To reduce the number of parameters in the model,To increase the diversity of generated text,A) To minimize the difference between predicted and actual word frequencies
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",Teacher forcing in language models is a technique used to:,Train the model using its own predictions as input,Increase the vocabulary size of the model,Train the model using the correct previous tokens as context,Reduce the training time by simplifying the model's architecture,C) Train the model using the correct previous tokens as context
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",What does the cross-entropy loss for a whole sentence indicate?,The average length of sentences the model can generate,The total number of tokens in the sentence,The collective loss across all tokens in predicting the sentence's next token,The number of unique words in the sentence,C) The collective loss across all tokens in predicting the sentence's next token
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67","If a language model assigns a very low probability to the true next word in a sentence, how does this affect the cross-entropy loss?",The loss remains unchanged,The loss decreases,The loss increases,The loss becomes zero,C) The loss increases
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",What is the purpose of 'Teacher forcing' in training transformer language models?,To prevent the model from making predictions,To force the model to use the next correct token as the context for the next step,To increase the computation time of the model,To generate more diverse token sequences,B) To force the model to use the next correct token as the context for the next step
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",What is the primary dataset used for training large language models (LLMs) like those based on the transformer architecture?,The Google Books Corpus,The Common Crawl dataset,Wikipedia,The British National Corpus,B) The Common Crawl dataset
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69","In the context of transformer language models, what does the acronym C4 stand for?",Comprehensive Core Language Model,Colossal Clean Crawled Corpus,Complex Computational Layer Model,Centralized Computational Language Model,B) Colossal Clean Crawled Corpus
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",Which of the following is NOT a characteristic of the Common Crawl dataset?,It contains snapshots of the entire web,It is produced by a non-profit organization,It exclusively contains social media data,It has billions of pages,C) It exclusively contains social media data
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",Which of the following best describes 'Stacked Transformer Blocks' in the context of transformer language models?,A method to reduce the size of the model,A sequence of transformer blocks stacked to increase the model's capacity,Blocks that are only used for image recognition tasks,A single transformer block used to reduce computational complexity,B) A sequence of transformer blocks stacked to increase the model's capacity
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What is the primary source of training data for LLMs mentioned in the lecture?,Common Crawl,Colossal Clean Crawled Corpus (C4),Patent documents,News sites,B) Colossal Clean Crawled Corpus (C4)
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",Which of the following types of content is deliberately filtered out from the Colossal Clean Crawled Corpus (C4)?,Patent text documents,Adult content,News sites,Wikipedia,B) Adult content
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",Which statement is an example of the knowledge that a pretrained language model might learn?,There are canines everywhere!,The author of 'A Room of One's Own' is Virginia Woolf.,"One dog in the front room, and two dogs.",The square root of 4 is 2.,B) The author of 'A Room of One's Own' is Virginia Woolf.
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What attribute of the training data contributes to the language model's ability to do so much?,The size of the Common Crawl,The diversity of text in the C4,The volume of boilerplate content,The amount of toxicity filtered out,B) The diversity of text in the C4
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",Which of these options are not mentioned as a part of the Colossal Clean Crawled Corpus (C4) content?,Wikipedia articles,Patent text documents,Social media posts,News sites,C) Social media posts
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",What is a potential legal issue with pretraining large language models on web-scraped data?,Copyright infringement,Data encryption,Algorithm complexity,Hardware requirements,A) Copyright infringement
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Which doctrine might be in question when using copyrighted web text for pretraining models?,Fair use doctrine,Public domain doctrine,First sale doctrine,Patent exhaustion doctrine,A) Fair use doctrine
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Why might web crawling pose a problem in pretraining large language models?,Websites may not want their sites crawled,Data is often encrypted,Lack of algorithm complexity,Insufficient hardware,A) Websites may not want their sites crawled
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",What privacy issue can arise from scraping text for pretraining models from the web?,Exposure of private IP addresses and phone numbers,Data encryption,Algorithm complexity,Insufficient hardware,A) Exposure of private IP addresses and phone numbers
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",What is a consequence of using copyrighted texts from the web without clear consent for pretraining language models?,Legal disputes over data consent,Increase in data encryption,Reduction in algorithm complexity,Increased hardware requirements,A) Legal disputes over data consent
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",What is the normalization factor in the perplexity formula of a language model?,The logarithm of the length of the test set,The length of the test set,The inverse of the length of the test set,The square root of the length of the test set,B) The length of the test set
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",Which of the following is NOT a way to fine-tune a pre-trained language model?,Parameter-efficient fine-tuning (PEFT),Extended pre-training,Parameter-efficient finetuning,Supervised fine-tuning (SFT),B) Extended pre-training
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",What is the primary difference between parameter-efficient fine-tuning and supervised fine-tuning?,"PEFT trains the entire pre-trained model, while SFT only trains the classification head","PEFT freezes some parameters, while SFT trains the entire model","PEFT only trains a subset of parameters on new data, while SFT creates a dataset of prompts and desired responses",PEFT and SFT are the same thing,"C) PEFT only trains a subset of parameters on new data, while SFT creates a dataset of prompts and desired responses"
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",How is the quality of a language model typically measured?,By the length of the test set,By the inverse length of the test set,By the inverse probability assigned to the test set,By the perplexity of the model on the test set,D) By the perplexity of the model on the test set
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",What is the role of the 'classification head' in a parameter-efficient fine-tuning method?,To generate the entire unseen test set,To predict specific tasks like sentiment classification,To generate the training data for the fine-tuning method,To reduce the size of the pre-trained model,B) To predict specific tasks like sentiment classification
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",What does perplexity measure in the context of language models?,The inverse of the probability assigned to the test set by the language model,The raw probability of the test set by the language model,The length of the test set,The size of the test set,A) The inverse of the probability assigned to the test set by the language model
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",Which of the following is NOT a characteristic of perplexity?,"The higher the probability of the word sequence, the lower the perplexity",Perplexity is sensitive to length/tokenization,"Perplexity range is [0,1]","The lower the perplexity of a model on the data, the better the model","C) Perplexity range is [0,1]"
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77","In the context of language models, what is the purpose of the supervised fine-tuning (SFT) technique?",Training the entire pretrained model on new data,Training a classification head on new data,"Training the model to follow text instructions, such as answering questions or following commands",Freezing some parameters of the model during the training process,"C) Training the model to follow text instructions, such as answering questions or following commands"
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",What is the main advantage of parameter-efficient fine-tuning (PEFT) in large language models?,Retraining all the parameters of the model,Freezing some parameters of the model during the training process,Adding extra neural circuitry after the top layer of the model,Creating a special fine-tuning dataset by hand,B) Freezing some parameters of the model during the training process
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",Which of the following best describes the fine-tuning technique that involves only training the classification head on some new data?,Continued pre-training,Parameter-efficient fine-tuning,Supervised fine-tuning,Classifier fine-tuning,D) Classifier fine-tuning
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",What does a lower perplexity score indicate about a language model?,The model is worse at predicting word sequences.,The model has a higher probability of the word sequence.,The model requires more computational power.,The model is not sensitive to tokenization.,B) The model has a higher probability of the word sequence.
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Why is perplexity considered a useful metric for comparing language models?,It measures the energy usage of the model.,It indicates the model's sensitivity to tokenization.,It is equivalent to maximizing the probability of word sequences.,It provides a direct comparison of model size.,C) It is equivalent to maximizing the probability of word sequences.
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",What aspect of language models does the perplexity metric fail to address?,Tokenization sensitivity,Model size and computational resources,Probability of word sequences,Maximizing model accuracy,B) Model size and computational resources
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79","In the context of language models, what does minimizing perplexity help achieve?",Decreasing the model's energy consumption,Reducing the model's training time,Increasing the fairness of the model,Improving the model's predictive accuracy,D) Improving the model's predictive accuracy
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Which statement best reflects the relationship between perplexity and model evaluation?,A higher perplexity score is preferred for better model performance.,Perplexity scores are irrelevant to the evaluation of language models.,Perplexity is a standalone metric for model evaluation.,Perplexity scores are useful when comparing models with the same tokenizer.,D) Perplexity scores are useful when comparing models with the same tokenizer.
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",Which risk associated with Large Language Models can lead to generating incorrect or nonsensical information?,Overfitting,Hallucination,Underfitting,Regularization,B) Hallucination
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81","In the context of Large Language Models, what is the term used to describe the phenomenon where the model fabricates information or data that is not based on the input provided?",Copyright,Hallucination,Fabrication,Transcription,B) Hallucination
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",Which option best represents the challenge of content generated by Large Language Models in terms of source verification?,The content is always factually accurate.,The content cannot be copyrighted.,The content may include 'hallucinated' information.,The content is always sourced from open databases.,C) The content may include 'hallucinated' information.
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81","When a Large Language Model produces information that seems plausible but is actually false, this issue is known as what?",Copyright infringement,Data leakage,Hallucination,Overfitting,C) Hallucination
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",What is the primary concern when Large Language Models 'hallucinate' during text generation?,The models becoming too complex to understand,The potential spread of misinformation,The violation of copyright laws,The models' inability to learn new languages,B) The potential spread of misinformation
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",Which of the following principles does not directly relate to the protection of individual rights within the context of copyright law?,Fair Use,Public Domain,Licensing Agreements,Trademark Law,D) Trademark Law
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83","In the realm of privacy, which principle is not primarily concerned with personal data protection?",Consent,Anonymity,Transparency,Jurisdiction,D) Jurisdiction
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",Which of the following is least likely to be considered a form of toxicity in an online community?,Spamming,Flame Wars,Cyberbullying,Moderation,D) Moderation
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",What aspect of content creation is least impacted by the principle of fair use in copyright law?,Parody,Commentary,Criticism,Commercial Exploitation,D) Commercial Exploitation
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83","In the context of online interactions, which of these actions is least associated with the prevention of toxicity and abuse?",Flagging inappropriate content,User reporting mechanisms,Content moderation,Algorithmic recommendation,D) Algorithmic recommendation
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",Which of the following is considered a harm of large language models?,They can generate realistic fraudulent content,They require large amounts of electricity to run,They can help improve natural language understanding,They are unable to process human language,A) They can generate realistic fraudulent content
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85","In the context of large language models, what is a potential risk associated with the spread of misinformation?",Increased accuracy of language generation,Greater efficiency in language translation,Propagation of false narratives,Improved accessibility of information,C) Propagation of false narratives
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",How might phishing attacks be influenced by large language models?,Large language models are immune to phishing attacks,Phishing attacks may become more sophisticated with AI assistance,Phishing is entirely unrelated to large language models,Large language models can eliminate phishing attacks,B) Phishing attacks may become more sophisticated with AI assistance
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85","In terms of fraud, what is a potential consequence of the misuse of large language models?",Reduction in overall fraud incidents,Creation of more convincing scam messages,Improved detection of fraudulent activities,Large language models cannot be used for fraud,B) Creation of more convincing scam messages
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",What type of harm is not typically associated with large language models?,Generation of realistic phishing content,Spread of misinformation,Increased energy consumption,Direct physical harm to individuals,D) Direct physical harm to individuals
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87",What is the primary goal of neural network training?,To increase the complexity of the network,To update network weights to minimize loss,To maximize the number of layers in the network,To enhance the speed of the forward computation,B) To update network weights to minimize loss
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87","During a backprop example, what is the purpose of running a forward computation?",To adjust the learning rate η,To find the estimated output 𝑦,To calculate the loss function 𝐿,To determine the blame each weight deserves for the current loss,B) To find the estimated output 𝑦
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87","In neural network training, how is the amount of blame a weight deserves for the current loss calculated?",By comparing it to the learning rate η,Using chain rule to compute partial derivatives,By updating the weight directly,Through random assignment,B) Using chain rule to compute partial derivatives
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87",What is the relationship between the learning rate η and the new weight update in neural network training?,The new weight is updated by adding η to the current weight,The new weight is updated by subtracting η times the blame from the current weight,The learning rate η is irrelevant to weight updates,The new weight is updated by multiplying the current weight by η,B) The new weight is updated by subtracting η times the blame from the current weight
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87","In the context of neural network training, what is the primary use of the loss function 𝐿?",To increase the network's accuracy on training data,To measure the difference between the true output and the network's estimate,To determine the number of layers in the network,To calculate the network's processing speed,B) To measure the difference between the true output and the network's estimate
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","Given the initial weights and input, what is the value of y using the forward pass equation?",8,12,10,14,B) 12
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","If the true value (ytrue) is 10, what is the loss computed using the given forward pass result?",0,2,4,8,C) 4
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91",Which of the following best describes the purpose of creating a computation graph during the backward pass?,To visualize the forward pass computation,To identify the input variables,To find the derivatives of model parameters,To calculate the final prediction accuracy,C) To find the derivatives of model parameters
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","For the given model parameters and input, what would be the updated value of y if the weight w1 is increased by 1?",13,14,11,10,B) 14
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","If the loss function is (ytrue-y)^2, what would be the impact on the loss if the value of y is closer to ytrue?",The loss increases,The loss remains constant,The loss decreases,The loss becomes negative,C) The loss decreases
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93",Which node in the computation graph is necessary to compute the loss function L?,h1,h2,w1,b,A) h1
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93",Which of the following best describes the purpose of backward pass in a computation graph?,To initialize the weights of the model,To compute the loss gradients for weights,To update the loss function,To forward propagate the input data,B) To compute the loss gradients for weights
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93","In the given computation graph, which intermediate node is responsible for the addition operation in the loss L?",h1,h2,w1,b,D) b
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93","If the goal is to minimize the loss function L, which gradient-related operation is directly associated with the weights w1 and w2 in the computation graph?",Gradient descent,Loss computation,Weight initialization,Input normalization,A) Gradient descent
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93","Given the computation graph, what is the relationship between h1 and the input x1?",h1 is the output of w1 multiplied by x1,h1 is the sum of w1 and x1,h1 is the sum of w1 and b,h1 is the weighted input of x1 without any transformation,A) h1 is the output of w1 multiplied by x1
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95","In the backpropagation process, what does the expression 'L=(ytrue−y)2' represent?",The cost function to be minimized,The activation function of the output layer,The weight update rule,The gradient of the loss function with respect to output,A) The cost function to be minimized
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95",What is the role of the term '2(ytrue-y)' in the backpropagation algorithm?,It represents the derivative of the cost function with respect to the weights,It is used to reset the weights to zero,It is a scaling factor for the input data,It calculates the margin between classes,A) It represents the derivative of the cost function with respect to the weights
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95","During backpropagation, the expression 'h1 = w1x1' is used to compute which of the following?",The bias of the first hidden layer,The output of the first hidden layer before activation,The gradients of the weights in the first hidden layer,The activation of the input layer,B) The output of the first hidden layer before activation
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95","In backpropagation, the gradient of the loss function with respect to the weights 'w2' is influenced by which of the following terms?",h1 + h2 + b,w1x1,2(ytrue - y),w1x1 + w2x2,D) w1x1 + w2x2
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95",The backpropagation formula '(ytrue−y)2' is minimized by adjusting which of the following during training?,The learning rate,The number of epochs,The activation functions,The network weights,D) The network weights
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97","During backpropagation, which variable is computed directly using the derivative of the loss function with respect to the output error?",loss gradient with respect to weights,loss gradient with respect to biases,loss gradient with respect to the output,loss gradient with respect to the input,C) loss gradient with respect to the output
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97","In the provided backpropagation example, how is the loss gradient with respect to 'w1' calculated?",It is the derivative of the loss function with respect to 'w1',It is the product of the derivative of the loss function with respect to 'y' and 'x1',It is the product of the derivative of the loss function with respect to 'y' and 'w1',"It is the product of the derivative of the loss function with respect to 'y', 'x1', and 'w1'","D) It is the product of the derivative of the loss function with respect to 'y', 'x1', and 'w1'"
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97","Given the values in the lecture content, what is the final value of 'x1' when calculating the loss gradient with respect to 'w1'?",2,4,16,8,C) 16
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97","If 'w1' and 'w2' are the weights of the first and second neurons respectively, and their values are given as 'w1=2' and 'w2=-1', what is the value of 'w1' after backpropagation assuming the learning rate is 1?",0,1,2,4,B) 1
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97","Based on the given data, if 'x2' equals '-3', what is the value of the loss gradient with respect to 'w2' after backpropagation?",4,-12,-4,12,D) 12
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","In backpropagation, what is the formula for computing the loss gradient with respect to w2?",w2 * (ytrue - y) * 2,(ytrue - y) * 2 * w1,w2 * x2 * (ytrue - y) * 2,(ytrue - y) * 2 * x1,C) w2 * x2 * (ytrue - y) * 2
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","When updating the weights with a learning rate of 0.01, what is the new value of w1?",1.92,1.84,1.76,1.68,B) 1.84
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99",What is the loss gradient with respect to the bias (b) in the given backpropagation process?,4,3,2,1,A) 4
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","In the provided backpropagation process, what is the derivative of the loss with respect to y (dL/dy)?",2(y - ytrue) * -1,2(ytrue - y),(ytrue - y) * -2,2(y - ytrue),B) 2(ytrue - y)
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","In the given backpropagation context, what is the correct expression for the derivative of the loss with respect to x1 (dL/dx1)?",4,16,4x1,16x1,B) 16
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101",What is the updated value of w1 after backpropagation?,1.84,2.00,2.16,3.00,A) 1.84
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101",What is the updated value of w2 after backpropagation?,-0.88,-1.00,-1.12,-2.00,A) -0.88
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101",What is the updated value of b after backpropagation?,0.96,1.00,1.04,2.00,A) 0.96
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","After backpropagation, what should be the result of the forward pass if w1, w2, and b have been updated correctly?",9.00,10.00,10.96,11.84,C) 10.96
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101",Which of the following options correctly represents the derivative of L with respect to w1?,16x1,4x2,-3y,12ytrue,A) 16x1
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103",What is the purpose of having multiple attention heads in a transformer model?,"Each head can focus on different aspects of the context, allowing for more diverse attention patterns.",It helps to reduce the computational complexity of the model.,Multiple heads ensure that the model is more robust against overfitting.,Having multiple heads enables the model to process the input data in parallel.,"A) Each head can focus on different aspects of the context, allowing for more diverse attention patterns."
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","In the context of a multi-head attention layer, what do the weight layers WQ_i represent?",They represent the projections of input data into query embeddings for each attention head.,They are used to compute the softmax scores for each attention head.,They represent the value embeddings for each attention head.,They are the weight matrices for key embeddings for each attention head.,A) They represent the projections of input data into query embeddings for each attention head.
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103",What does the equation q_ci = x_iWQ_c represent in the multi-head attention mechanism?,It computes the score between context and query vectors.,It projects the input into the query embeddings for each attention head.,It represents the softmax normalization of the attention scores.,It calculates the concatenated output of all attention heads.,B) It projects the input into the query embeddings for each attention head.
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103",What is the purpose of the WO layer in the multi-head attention mechanism?,It projects the concatenated output of all attention heads to the desired output dimensionality.,It is used to compute the softmax normalization of the attention scores.,It projects the input data into key embeddings for each attention head.,It is responsible for parallelizing the attention computations.,A) It projects the concatenated output of all attention heads to the desired output dimensionality.
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","In the multi-head attention mechanism, which of the following equations represents the computation of the final output for a single input x_i?",ai = (head_1⊗head_2⊗…⊗head_A)WO(9.19),ai = WO(9.20)(xi,ai = x_iWO,ai = ai_1 + ai_2 + … + ai_A,A) ai = (head_1⊗head_2⊗…⊗head_A)WO(9.19)
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105","In the context of multi-head attention, what dimensionality do the key and query embeddings share?",64,dv,d,dk,A) 64
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the purpose of the mask function in the self-attention computation?,To emphasize the importance of future context,To eliminate knowledge of words that follow in the sequence,To increase the dimensionality of the output,To reduce the computational load of matrix multiplication,B) To eliminate knowledge of words that follow in the sequence
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105","In a multi-head attention mechanism, what is the shape of the output vector ai for each input i?",1 x hdv,1 x dv,1 x d,1 x dk,A) 1 x hdv
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the impact of the multi-head attention mechanism on the length of the input computation?,It reduces the computation to linear complexity,It does not affect the computational complexity,It increases the computation to cubic complexity,It reduces the computation to square complexity,C) It increases the computation to cubic complexity
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",How are the outputs from each head in a multi-head attention layer processed to produce the final output?,They are averaged,They are concatenated and then projected down to the original input dimension,They are summed,They are individually projected and then concatenated,B) They are concatenated and then projected down to the original input dimension
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What is the purpose of introducing a mask function in the calculation of QK| during self-attention computation?,To eliminate any knowledge of words that follow in the sequence and prevent the model from 'cheating' by knowing future words,To increase the dimensionality of the matrices involved in the computation,To reduce the complexity of the softmax function,To speed up the computation by removing unnecessary elements from the matrix,A) To eliminate any knowledge of words that follow in the sequence and prevent the model from 'cheating' by knowing future words
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What does the resulting masked QK| matrix look like after applying the mask function?,A matrix with zeroed elements in the upper-triangular portion,A matrix with increased values in the diagonal,A matrix with random values in the lower-triangular portion,A matrix with decreased values in the diagonal,A) A matrix with zeroed elements in the upper-triangular portion
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",How does the dimensionality of the key embedding WKio and query embedding WQio matrices compare to the value embedding WVio matrix in multi-head attention?,"WKio and WQio have the same dimensionality, but WVio has a different dimensionality","WKio and WQio have different dimensionalities, but WVio has the same dimensionality","WKio, WQio, and WVio all have the same dimensionality","WKio, WQio, and WVio all have different dimensionalities","C) WKio, WQio, and WVio all have the same dimensionality"
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What is the shape of the output matrix after concatenating the Ahead output matrices in multi-head attention?,N⇥dv,N⇥dk,N⇥hdv,N⇥d,C) N⇥hdv
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What is the final linear projection matrix WO used for in multi-head attention?,To reduce the dimensionality of the output matrix,To reshape the output matrix to the original output dimension for each token,To increase the complexity of the computation,To eliminate unnecessary elements from the output matrix,B) To reshape the output matrix to the original output dimension for each token
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",What is the consequence of the self-attention computation's problem in the context of language modeling?,Multiplication of query values by key values from the entire sequence,Guessing the next word is less accurate,Computations become linear in the length of the input,The upper-triangle portion of the matrix remains non-zero,A) Multiplication of query values by key values from the entire sequence
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",How is the problem of self-attention computation addressed to avoid knowledge of future words in the sequence?,Adding a mask matrix to the attention matrix,Removing all values from the upper-triangle of the matrix,Multiplying the attention matrix by zero,Using a softmax function with a higher temperature,A) Adding a mask matrix to the attention matrix
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",What are the dimensions of the key and query embeddings in the original Transformer paper?,64,8,512,1024,A) 64
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",What is the shape of the output matrix of the multi-head attention layer with A heads?,N⇥NQK|,N⇥dv,N⇥hdv,A⇥dv,C) N⇥hdv
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",What is the purpose of the final linear projection WO in the Transformer model?,To reshape the output to the original dimension per token,To reduce the dimensionality of the output,To increase the dimensionality of the output,To multiply the output by a fixed scalar value,A) To reshape the output to the original dimension per token
