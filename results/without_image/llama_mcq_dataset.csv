pdf_name,chunk_number,total_chunks,pages,question,option_A,option_B,option_C,option_D,correct_answer
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",What is the primary function of a simple n-gram language model?,Assigns probabilities to sequences of words,Generates text by sampling possible next words,Translates text from one language to another,Classifies text into predefined categories,A) Assigns probabilities to sequences of words
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",How do large language models generate text?,By predicting the entire sequence of words,By sampling possible next words,By analyzing the context and meaning of the text,By using a predefined set of rules,B) By sampling possible next words
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",What is the primary goal of training a large language model?,To assign probabilities to sequences of words,To generate text by sampling possible next words,To learn a lot of useful language knowledge,To classify text into predefined categories,C) To learn a lot of useful language knowledge
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",What is a key difference between a simple n-gram language model and a large language model?,A simple n-gram model is trained on less text,A large language model is trained on counts computed from less text,A simple n-gram model generates text by sampling possible next words,A large language model assigns probabilities to sequences of words,A) A simple n-gram model is trained on less text
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",What is a consequence of training a large language model on a lot of text?,The model becomes less accurate,The model becomes more specialized,The model learns a lot of useful language knowledge,The model becomes more computationally expensive,C) The model learns a lot of useful language knowledge
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which type of large language model can generate text from a given prompt but cannot condition on future words?,Encoder-Decoder,Encoder,Decoder,Encoder-Decoder with Bidirectional Context,C) Decoder
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",What is another name for the type of large language model that generates text from a given prompt but cannot condition on future words?,Causal LLMs,Autoregressive LLMs,Left-to-Right LLMs,Generative LLMs,A) Causal LLMs
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which type of large language model architecture typically requires pretraining to build strong representations?,Decoder-Only,Encoder-Decoder,Encoder,Encoder with Bidirectional Context,C) Encoder
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which of the following is NOT a characteristic of decoder-only models?,Can generate text from a given prompt,Can condition on future words,Typically used for left-to-right prediction,Can be used for bidirectional context,B) Can condition on future words
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which type of large language model architecture is commonly used for tasks that require bidirectional context?,Decoder-Only,Encoder-Decoder,Encoder,Encoder with Bidirectional Context,D) Encoder with Bidirectional Context
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",What type of pretraining is typically used for BERT family models?,Predicting words from surrounding words on both sides,Predicting words from surrounding words on one side only,Mapping from one sequence to another,Generating words from a fixed starting point,A) Predicting words from surrounding words on both sides
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which type of architecture is typically used for machine translation and speech recognition tasks?,Decoder-only model,Encoder-only model,Encoder-Decoder model,Masked Language Model,C) Encoder-Decoder model
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",What is a key limitation of decoder-only models?,They can condition on future words,They can generate words from a fixed starting point,They can't condition on future words,They are only used for classification tasks,C) They can't condition on future words
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which type of pretraining is typically used for Masked Language Models?,Predicting words from surrounding words on one side only,Predicting words from surrounding words on both sides,Mapping from one sequence to another,Generating words from a fixed starting point,B) Predicting words from surrounding words on both sides
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",What is a key advantage of Encoder-Decoders compared to Decoder-only models?,They can only generate words from a fixed starting point,They can condition on future words,They can generate words from a fixed starting point and condition on future words,They can only predict words from surrounding words on one side,B) They can condition on future words
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What type of tasks can be turned into tasks of predicting words?,Only machine translation tasks,"Many tasks, including machine translation",No tasks can be turned into tasks of predicting words,Only speech recognition tasks,"B) Many tasks, including machine translation"
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What is the term used to describe decoder-only models that predict words left to right?,Autoregressive models,Bidirectional models,Left-to-right models,Causal models,D) Causal models
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What is a characteristic of decoders that makes them 'nice to generate from'?,They can condition on future words,They can generate from a fixed context,They can’t condition on future words,They are only useful for machine translation,C) They can’t condition on future words
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",How does the type of pretraining differ based on the neural architecture?,It doesn’t differ at all,"It differs based on the specific architecture, but not the type of architecture",It differs based on the type of architecture and the natural use cases,"It differs based on the specific architecture, but not the natural use cases",C) It differs based on the type of architecture and the natural use cases
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What is the name given to decoder-only models in today's lecture?,Encoder-Decoder models,Causal Language Models,Left-to-Right Language Models,Autoregressive Language Models,D) Autoregressive Language Models
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",What is the primary limitation of decoders in language models?,"They can only generate text from a given prefix, but cannot condition on future words.","They can condition on future words, but cannot generate text from a given prefix.","They can only predict a single word at a time, and cannot generate longer responses.","They are limited to generating text in a specific language, and cannot adapt to other languages.","A) They can only generate text from a given prefix, but cannot condition on future words."
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",How can we cast sentiment analysis as a word prediction task?,By giving the language model the sentence and asking it to predict the sentiment as a numerical value.,"By giving the language model the sentence and a token like 'positive' or 'negative', and asking it to predict which word comes next.",By giving the language model the sentence and asking it to predict the sentiment as a categorical label.,"By giving the language model the sentence and a token like 'positive' or 'negative', and asking it to predict the probability of each sentiment.","B) By giving the language model the sentence and a token like 'positive' or 'negative', and asking it to predict which word comes next."
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",What is the role of the language modeling head in the architecture shown in Figure 10.1?,It is responsible for generating the next token in the sequence.,It is responsible for conditioning on future words and generating the next token.,It is responsible for computing the probability distribution over all possible next tokens.,It is responsible for adding the generated token to the context as a prefix for generating the next token.,C) It is responsible for computing the probability distribution over all possible next tokens.
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",How can we cast question answering as a word prediction task?,"By giving the language model a question and a token like 'A:', and asking it to predict which word comes next.",By giving the language model a question and asking it to predict the answer as a numerical value.,"By giving the language model a question and a token like 'A:', and asking it to predict the probability of each possible answer.",By giving the language model a question and asking it to predict the answer as a categorical label.,"A) By giving the language model a question and a token like 'A:', and asking it to predict which word comes next."
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",What is the purpose of the token 'tl;dr' in the context of text summarization?,It is a placeholder for the summary text.,It is a token that suggests the language model should generate a shorter summary of the text.,It is a token that indicates the end of the summary text.,It is a token that suggests the language model should generate a longer summary of the text.,B) It is a token that suggests the language model should generate a shorter summary of the text.
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",A language model is given the sentence 'I like Jackie Chan' and the word 'The sentiment of the sentence' as a prefix. What word would it predict as the next word in the sentence?,is positive,is negative,is funny,is interesting,A) is positive
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",A language model is given the question 'Who wrote the book 'The Origin of Species'' and the token 'A:' as a prefix. What word would it predict as the next word in the answer?,Charles,Darwin,Galileo,Einstein,A) Charles
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",A language model is given a text and the token 'tl;dr' as a prefix. What does the token 'tl;dr' stand for?,Too Long; Don't Read,Too Short; Don't Read,Too Long; Read More,Too Short; Read Less,A) Too Long; Don't Read
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",A language model is given a sentence and asked to compute the probability distribution over possible next words. What does it do to determine the next word?,It looks at the sentence and tries to find a word that sounds similar,It looks at the sentence and tries to find a word that is grammatically correct,It computes the probability distribution over possible next words given the prefix,It looks at the sentence and tries to find a word that is related to the topic,C) It computes the probability distribution over possible next words given the prefix
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",A language model is given a question and asked to generate a response. What is one way to cast this task as language modeling?,By giving a large language model a text and asking it to predict the next word,By giving a large language model a question and a token like 'A:' and asking it to predict the next word,By giving a large language model a question and asking it to generate a response from scratch,By giving a large language model a text and asking it to summarize the text,B) By giving a large language model a question and a token like 'A:' and asking it to predict the next word
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",What is the primary function of the system described in the lecture?,To compute the probability that a word w follows a prefix string of words s,To translate text from one language to another,To generate new text based on a given prefix,To classify text as spam or not spam,A) To compute the probability that a word w follows a prefix string of words s
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",What type of neural network architecture is used to build modern Large Language Models?,Recurrent Neural Network (RNN),Convolutional Neural Network (CNN),Stacks of neural networks called transformers,Long Short-Term Memory (LSTM),C) Stacks of neural networks called transformers
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",What is the name of the architecture used to build Large Language Models?,Reformer,XLNet,Transformers,BERT,C) Transformers
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",What is the primary task that Large Language Models are designed to perform?,Language translation,Text classification,Language modeling,Sentiment analysis,C) Language modeling
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",What type of string is used as input to the system described in the lecture?,A sentence,A paragraph,A prefix string of words,A single word,C) A prefix string of words
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What type of network architecture is the Transformer based on?,A specific kind of recurrent neural network,"A fancier feedforward network, but based on attention",A convolutional neural network,A complex recurrent or convolutional neural network,"B) A fancier feedforward network, but based on attention"
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What is the key feature that the Transformer model dispenses with compared to traditional sequence transduction models?,Recurrent connections,Convolutions,Encoder and decoder,Attention mechanisms,B) Convolutions
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What is the name of the paper that introduced the Transformer model?,Attention Is All You Need,TransformersIntroduction to Transformers,The Dominant Sequence Transduction Models,Google Brain's Latest Breakthrough,A) Attention Is All You Need
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What is the name of the conference where the Transformer model was first presented?,31st Conference on Neural Information Processing Systems (NIPS 2017),32nd Conference on Neural Information Processing Systems (NIPS 2018),33rd Conference on Neural Information Processing Systems (NIPS 2019),34th Conference on Neural Information Processing Systems (NIPS 2020),A) 31st Conference on Neural Information Processing Systems (NIPS 2017)
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What year did GPUs begin to be used for neural network training?,2004-6,2008,2012,2015,A) 2004-6
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",In which year were GPUs first utilized in the field of natural language processing?,2004,2008,2012,2003,A) 2004
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",What was re-discovered in 2013 that was initially introduced in the past?,Transformers,Static Word Embeddings,Multi-Task Learning,Attention,B) Static Word Embeddings
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19","In the context of the Transformer model, what is the purpose of the Language Modeling Head?",To generate embeddings for input tokens,To generate logits for the next token,To perform input encoding,To perform multi-task learning,B) To generate logits for the next token
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",In which year was the concept of Attention introduced in the field of natural language processing?,2012,2015,2017,2013,B) 2015
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",What was the primary innovation of the Transformer model in comparison to previous neural language models?,The use of static word embeddings,The use of GPUs for parallelization,The introduction of self-attention mechanisms,The use of multi-task learning,C) The introduction of self-attention mechanisms
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",What is the primary issue with using static embeddings like word2vec?,They are too complex and computationally expensive to calculate.,They are static and do not reflect how a word's meaning changes in context.,They are only suitable for short texts and not for long documents.,They are only applicable to specific domains like sentiment analysis.,B) They are static and do not reflect how a word's meaning changes in context.
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21","According to the lecture, what is the intuition behind contextual embeddings?",A word's meaning should be the same across all contexts.,A word's meaning should be different in different contexts.,Contextual embeddings are only useful for very short texts.,Contextual embeddings are not necessary for machine translation.,B) A word's meaning should be different in different contexts.
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21","How do we compute contextual embeddings, according to the lecture?",Using Recurrent Neural Networks (RNNs).,Using Long Short-Term Memory (LSTM) networks.,Using Attention.,Using Convolutional Neural Networks (CNNs).,C) Using Attention.
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",What is the meaning represented in the static embedding for the word 'it' in the sentence 'The chicken didn't cross the road because it was too tired'?,The chicken's tiredness.,The chicken's ability to cross the road.,The general concept of 'it' as a pronoun.,The chicken's motivation for crossing the road.,C) The general concept of 'it' as a pronoun.
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",What is the primary benefit of using contextual embeddings?,Improved computational efficiency.,Better generalization to out-of-vocabulary words.,Enhanced ability to capture word sense disambiguation.,Simplified model architecture.,C) Enhanced ability to capture word sense disambiguation.
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",What is the main idea behind contextual embeddings?,Each word has a fixed vector representation regardless of context,Each word has a different vector that expresses different meanings depending on surrounding words,Contextual embeddings are only useful for short sentences,Contextual embeddings are only useful for long documents,B) Each word has a different vector that expresses different meanings depending on surrounding words
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",What is the function of attention in building contextual embeddings?,To select the most important neighboring word and ignore others,To selectively integrate information from all neighboring words,To ignore all neighboring words and use only the word itself,To use a fixed weight for all neighboring words,B) To selectively integrate information from all neighboring words
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",How does the contextual embedding of 'it' change in the following sentence: The chicken didn't cross the road because it was too wide?,The vector representation of 'it' remains the same as in the previous sentence,The vector representation of 'it' changes to reflect its new meaning as the street,The vector representation of 'it' changes to reflect its new meaning as the chicken,The vector representation of 'it' becomes undefined,B) The vector representation of 'it' changes to reflect its new meaning as the street
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",What is the implication of the attention mechanism in the given example sentence?,The chicken attends to the road more than the surrounding words,The chicken attends to both the road and the surrounding words,The chicken attends to the surrounding words more than the road,The chicken does not attend to the road at all,B) The chicken attends to both the road and the surrounding words
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",Why is the representation of meaning of a word different in different contexts?,Because the word itself changes meaning,Because the surrounding words change the meaning of the word,Because the sentence structure changes the meaning of the word,Because the author's intent changes the meaning of the word,B) Because the surrounding words change the meaning of the word
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",What is the primary function of attention in the context of contextual embedding?,To selectively integrate information from all neighboring words equally,To selectively integrate information from all neighboring words,To ignore the neighboring words and rely solely on its own information,To perform a simple average of the neighboring words' information,B) To selectively integrate information from all neighboring words
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25","According to the lecture, what can be inferred about the chick's behavior in the example sentence?",The chick was too hungry to cross the road,The chick was too tired to cross the road,The chick was too excited to cross the road,The chick was too scared to cross the road,B) The chick was too tired to cross the road
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",What is the term used to describe the process of computing the embedding for a token by selectively attending to and integrating information from surrounding tokens?,Weighted sum,Self-attention,Contextual embedding,Token integration,B) Self-attention
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",How many neighboring words does a word attend to more than others according to the intuition of attention?,All of them equally,Some of them,None of them,More than one,B) Some of them
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",What is the term used to describe the mechanism for helping compute the embedding for a token by selectively attending to and integrating information from surrounding tokens?,Self-attention distribution,Attention mechanism,Weighted sum of vectors,Contextual embedding method,B) Attention mechanism
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",What is the core intuition of attention in the context of self-attention for language?,Comparing an item of interest to a collection of other items in a way that reveals their relevance in the current context,Computing a weighted sum of vectors using a dot product,Normalizing scores using a softmax function,"Projecting input vectors into a representation of their role as a key, query, or value",A) Comparing an item of interest to a collection of other items in a way that reveals their relevance in the current context
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",How do we compare words to other words in self-attention for language?,Using a dot product of the word embeddings,Using a cosine similarity measure,Using a Euclidean distance metric,Using a word embedding space,A) Using a dot product of the word embeddings
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",What is the role of the softmax function in the attention process?,To compute the weighted sum of the input vectors,To normalize the scores to provide a probability distribution,"To project the input vectors into a representation of their role as a key, query, or value",To compute the dot product of the word embeddings,B) To normalize the scores to provide a probability distribution
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",What are the three different roles that each input embedding plays during the attention process?,"Current focus of attention, preceding input, and value","Query, key, and value","Input vector, output vector, and attention output vector","Word embedding, context vector, and relevance vector","B) Query, key, and value"
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",What is the dimensionality of the key and query vectors in the original transformer work?,1⇥d,1⇥dk,1⇥dv,512,B) 1⇥dk
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29",What is the formula for calculating the score between a current focus of attention and an element in the preceding context in the self-attention mechanism?,"score(xi,xj) = xi · xj","score(xi,xj) = qi · kj","score(xi,xj) = xi + xj","score(xi,xj) = xi - xj","B) score(xi,xj) = qi · kj"
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29",What are the three different roles that each input embedding plays during the course of the attention process?,"query, key, and value","query, key, and context","query, value, and attention","key, value, and attention","A) query, key, and value"
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29",What is the purpose of the softmax calculation in the self-attention mechanism?,To normalize the scores to provide a probability distribution,To add weights to the input tokens,To multiply the input tokens together,To subtract the input tokens from each other,A) To normalize the scores to provide a probability distribution
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29",What is the formula for calculating the output value ai in the self-attention mechanism?,ai = Xj∑iaijxj,ai = Xj∑iaijvj,ai = Xj∑iaijki,ai = Xj∑iaijqi,B) ai = Xj∑iaijvj
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29","What are the dimensions of the weight matrices WQ, WK, and WV in the self-attention mechanism?","WQ: 1⇥d, WK: 1⇥d, WV: 1⇥d","WQ: d⇥dk, WK: d⇥dk, WV: d⇥dv","WQ: 1⇥dk, WK: 1⇥dv, WV: 1⇥d","WQ: d⇥dv, WK: d⇥dk, WV: d⇥d","B) WQ: d⇥dk, WK: d⇥dk, WV: d⇥dv"
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","In a self-attention mechanism, which role is played by a vector xi when it is being compared to the current element?",query,key,value,both query and key,B) key
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31",What happens to a value of a preceding element in a self-attention mechanism?,it is discarded,it is weighted and summed,it is compared to the current element,it is used as a key,B) it is weighted and summed
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","In a self-attention distribution, which columns correspond to input tokens?",columns corresponding to query and key,columns corresponding to values,"columns corresponding to query, key, and value",columns corresponding to input embeddings,"C) columns corresponding to query, key, and value"
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31",What happens to a preceding input in a self-attention mechanism?,it is used as a query,it is used as a key,it is weighted and summed,it is discarded,B) it is used as a key
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31",Which of the following is NOT a role played by a vector xi in a self-attention mechanism?,query,key,value,embedding,D) embedding
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33","What is the purpose of the weight matrices WQ, WK, and WV in the attention head?","To project each input vector xi into a representation of its role as a key, query, or value",To compute the similarity of the current element xi with some prior element xj,To avoid numerical issues and loss of gradients during training,To normalize the scores into a probability distribution,"A) To project each input vector xi into a representation of its role as a key, query, or value"
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",What is the role of the query vector qi in the attention process?,As a preceding input that is being compared to the current element to determine a similarity weight,As the current element being compared to the preceding inputs,As a value of a preceding element that gets weighted and summed up to compute the output for the current element,As a key that determines the similarity of the current element with some prior element,B) As the current element being compared to the preceding inputs
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",What is the effect of scaling the dot product by a factor related to the size of the embeddings?,To increase the value of the dot product,To avoid numerical issues and loss of gradients during training,To reduce the dimensionality of the query and key vectors,To normalize the scores into a probability distribution,B) To avoid numerical issues and loss of gradients during training
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",What is the final output calculation for ai in the self-attention process?,"ai = softmax(score(xi, xj))8ji",ai = Xjiaijvj,ai = qi · kj,ai = vi · xj,B) ai = Xjiaijvj
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",What is the formula for computing the similarity of the current element xi with some prior element xj?,"score(xi, xj) = qi · kj / dk","score(xi, xj) = qi · kj","score(xi, xj) = vi · xj","score(xi, xj) = xj · xi","A) score(xi, xj) = qi · kj / dk"
notes/LLM_cs124_week7_2025.pdf,17,"33, 34, 35","What is the purpose of introducing weight matrices WQ, WK, and WV in the attention head?","To project each input vector xi into a representation of its role as a query, key, or value",To scale the dot product by a factor related to the size of the embeddings,To compute the similarity of the current element xi with some prior element xj,"To sum up the values of the prior elements, each weighted by the similarity of its key to the query from the current element","A) To project each input vector xi into a representation of its role as a query, key, or value"
notes/LLM_cs124_week7_2025.pdf,17,"33, 34, 35",Why is it necessary to scale the dot product by a factor related to the size of the embeddings?,To avoid numerical issues and loss of gradients during training,To improve the accuracy of the attention mechanism,To reduce the computational complexity of the attention head,To increase the dimensionality of the query and key vectors,A) To avoid numerical issues and loss of gradients during training
notes/LLM_cs124_week7_2025.pdf,17,"33, 34, 35",What is the output calculation for ai based on after applying the softmax calculation?,Based on a weighted sum over the value vectors v,"Based on a sum of the prior vectors, normalized by the softmax calculation",Based on a dot product between the current element's query vector and the preceding element's key vector,"Based on a sum of the prior elements, each weighted by the similarity of its key to the query from the current element",A) Based on a weighted sum over the value vectors v
notes/LLM_cs124_week7_2025.pdf,17,"33, 34, 35",What is the role of the value vector vj in the attention head?,To represent the current element being compared to the preceding inputs,To represent the preceding input being compared to the current element to determine a similarity weight,To represent the value of a preceding element that gets weighted and summed up to compute the output for the current element,To represent the query vector qi,C) To represent the value of a preceding element that gets weighted and summed up to compute the output for the current element
notes/LLM_cs124_week7_2025.pdf,17,"33, 34, 35","What is the final output of the attention head, ai, calculated as?",headiWO,Xjiaijvj,"softmax(score(xi,xj))8ji",qi·kjpdk,A) headiWO
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37","What is the purpose of using separate weight matrices WQ, WK, and WV in the attention head?","To capture the three different roles of an input embedding as a query, key, and value",To reduce the dimensionality of the input embeddings,To apply a single weight matrix to all input embeddings,To use a different attention mechanism for each input embedding,"A) To capture the three different roles of an input embedding as a query, key, and value"
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the effect of scaling the dot product by dividing by the square root of the dimensionality of the query and key vectors (dk)?,It increases the numerical stability of the dot product calculation,It reduces the dimensionality of the input embeddings,It applies a different attention mechanism for each input embedding,It ignores the context words in the attention calculation,A) It increases the numerical stability of the dot product calculation
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the purpose of using the softmax function to compute the weighted sum of the prior elements?,To normalize the weights of the prior elements,To reduce the dimensionality of the input embeddings,To apply a different attention mechanism for each input embedding,To ignore the context words in the attention calculation,A) To normalize the weights of the prior elements
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the role of the matrix WO in the attention head?,To reshape the output of the attention head to the desired dimensionality,To apply a different attention mechanism for each input embedding,To reduce the dimensionality of the input embeddings,To ignore the context words in the attention calculation,A) To reshape the output of the attention head to the desired dimensionality
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the benefit of using multiple attention heads in the transformer model?,To reduce the computational complexity of the attention calculation,To allow each head to model different aspects of the relationships among inputs,To ignore the context words in the attention calculation,To apply a single attention mechanism for all input embeddings,B) To allow each head to model different aspects of the relationships among inputs
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",What is a primary outcome of using the Attention method to enrich token representations?,The embedding for each word remains constant across different contexts.,The embedding for each word is the same as its individual embedding.,The embedding for each word will be different in different contexts.,The enriched representation is not passed up layer by layer.,C) The embedding for each word will be different in different contexts.
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",Which architectural component is primarily associated with the Attention method?,Transformer Block,Attention Layer,Transformer Encoder,Transformer Decoder,A) Transformer Block
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",How are enriched token representations typically passed through the model?,Down layer by layer,Up layer by layer,Randomly throughout the model,Only in the final layer,B) Up layer by layer
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",What is a key benefit of using Attention in the Transformer architecture?,It reduces the number of model parameters.,It improves the model's ability to handle out-of-vocabulary words.,It allows the model to capture contextual information.,It speeds up model training time.,C) It allows the model to capture contextual information.
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",What is a primary characteristic of the enriched token representations produced by Attention?,They are less informative than individual word embeddings.,They are more informative than individual word embeddings.,They remain constant across different contexts.,They are only used in the final model output.,B) They are more informative than individual word embeddings.
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","In a Transformer block, what operation is applied to the input tokens after they are passed through the MultiHeadAttention mechanism?",Layer Norm,Feedforward,Dropout,Residual Connection,A) Layer Norm
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","In the context of a Transformer block, what is the purpose of the residual stream?",To add an additional layer of complexity to the model,To scale the input tokens,To allow each token to be modified independently,To pass the output of each layer to the next layer,D) To pass the output of each layer to the next layer
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","In a Transformer block, what is the order of operations for the input tokens after they pass through the MultiHeadAttention mechanism?",Layer Norm -> Feedforward -> Residual Connection,Feedforward -> Layer Norm -> Residual Connection,Residual Connection -> Layer Norm -> Feedforward,Layer Norm -> Residual Connection -> Feedforward,D) Layer Norm -> Residual Connection -> Feedforward
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","In a Transformer block, what is the purpose of the Feedforward mechanism?",To apply a residual connection to the input tokens,To normalize the input tokens,To apply a non-linear transformation to the input tokens,To apply a dropout to the input tokens,C) To apply a non-linear transformation to the input tokens
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","In a Transformer block, what is the order of operations for the input tokens after they pass through the Input Encoding?",MultiHeadAttention -> Feedforward -> Residual Connection,Layer Norm -> MultiHeadAttention -> Feedforward,Feedforward -> MultiHeadAttention -> Residual Connection,Residual Connection -> MultiHeadAttention -> Feedforward,B) Layer Norm -> MultiHeadAttention -> Feedforward
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the primary purpose of the residual stream in a transformer block?,To allow the transformer block to process multiple input vectors in parallel,To enable the transformer block to handle variable-length input sequences,To progressively add the output of each component to the input of the next component,To facilitate the use of attention and feedforward layers in a single block,C) To progressively add the output of each component to the input of the next component
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the typical dimensionality of the hidden layer in a feedforward network used in a transformer block?,Less than the model dimensionality d,Equal to the model dimensionality d,Greater than the model dimensionality d,Equal to the dimensionality of the input vector xi,C) Greater than the model dimensionality d
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",How many times is the vector xi normalized in the layer norm computation?,Once,Twice,Three times,Four times,B) Twice
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the name of the computation that is performed before the attention and feedforward layers in a transformer block?,Layer norm,Feedforward layer,Attention layer,Residual stream,A) Layer norm
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the typical relationship between the weights of the two layers in a feedforward network used in a transformer block?,The weights are the same for each token position i,The weights are different for each token position i,The weights are the same for each layer but different for each token position i,The weights are different for each layer and each token position i,A) The weights are the same for each token position i
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What is the purpose of layer norm in a transformer block?,To apply a z-score transformation to the entire transformer layer,To keep the values of a hidden layer in a range that facilitates gradient-based training,To normalize the entire input sequence,To apply a batch normalization technique,B) To keep the values of a hidden layer in a range that facilitates gradient-based training
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What is the formula for calculating the mean (μ) in layer normalization?,μ = √(1/d) * ∑(xi^2),μ = 1/d * ∑(xi),μ = 1/d * ∑(xi^2),μ = √(1/d) * ∑(xi),B) μ = 1/d * ∑(xi)
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What is the role of multi-head attention in a transformer block?,It only takes the current token's embedding as input,It looks at all neighboring tokens in the context,It only applies a linear transformation to the input,It does not affect the input sequence,B) It looks at all neighboring tokens in the context
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What is the formula for the standard implementation of layer normalization?,LayerNorm(x) = g(x - μ) / s + b,LayerNorm(x) = g(x) + b,LayerNorm(x) = (x - μ) / s,LayerNorm(x) = x + b,A) LayerNorm(x) = g(x - μ) / s + b
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What is the effect of layer normalization on the output vector?,It sets the mean to a fixed value,It sets the standard deviation to a fixed value,It sets the mean to zero and the standard deviation to one,It does not change the mean or standard deviation,C) It sets the mean to zero and the standard deviation to one
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47","What is the formula to calculate the mean, µ, in layer normalization?",µ = 1/d ∑x_i,µ = 1/d ∑(x_i - 1),µ = 1/d ∑(x_i - 1/d),µ = 1/d ∑(x_i - 1/d^2),A) µ = 1/d ∑x_i
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",What is the purpose of the learnable parameters g and b in the standard implementation of layer normalization?,To adjust the scale of the input vector,To shift the mean of the input vector,To introduce gain and offset values,To normalize the input vector to have a standard deviation of zero,C) To introduce gain and offset values
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",Which component of the transformer block takes information from other tokens and adds it to the current token's embedding stream?,Layer Norm,MultiHeadAttention,Feedforward,Residual Stream,B) MultiHeadAttention
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",What is the result of applying layer normalization to a vector?,A vector with zero mean and a standard deviation greater than one,A vector with a mean greater than zero and a standard deviation of one,A vector with zero mean and a standard deviation of one,A vector with a mean of one and a standard deviation greater than zero,C) A vector with zero mean and a standard deviation of one
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",What is the purpose of the residual stream in the transformer block?,To introduce learnable parameters,To add information from other tokens to the current token's embedding stream,To normalize the input vector,To shift the mean of the input vector,B) To add information from other tokens to the current token's embedding stream
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",What is added to the token embedding to create the final embedding for each word in the context?,Token embedding only,Positional embedding only,Token embedding and positional embedding,Neither token nor positional embedding,C) Token embedding and positional embedding
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",What is the shape of the matrix X that contains the token and positional embeddings?,[N × d],[d × N],[1 × d],[d × 1],A) [N × d]
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",What type of embedding is added to the token embedding to create the final embedding for each word in the context?,Semantic embedding,Contextual embedding,Positional embedding,Topic embedding,C) Positional embedding
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",What is the shape of the embedding for each word in the context?,[N × d],[d × N],[1 × d],[d × 1],C) [1 × d]
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",How many distinct embeddings are created for each input token?,1,2,N,d,B) 2
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51",What is the purpose of positional embeddings in the matrix X?,To provide a unique identifier for each word in the vocabulary,To capture the spatial relationship between words in a sentence,To represent the meaning of each word in the vocabulary,To enable the model to attend to specific words in the input,B) To capture the spatial relationship between words in a sentence
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51",What is the shape of the embedding matrix E?,[N × d],[|V| × d],[d × N],[d × |V|],B) [|V| × d]
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","Given the vocabulary indices w = [5, 4000, 10532, 2224], which row from the embedding matrix E corresponds to the word 'Thanks'?",Row 5,Row 4000,Row 10532,Row 2224,A) Row 5
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51",What is the dimensionality of each word embedding in the matrix E?,1,d,|V|,N,B) d
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51",How are token embeddings and positional embeddings combined to create a single embedding for each word in the context?,By concatenating the two embeddings,By adding the two embeddings,By taking the maximum of the two embeddings,By taking the minimum of the two embeddings,B) By adding the two embeddings
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",What is the output of tokenizing the string 'Thanks for all the' using BPE and converting it into vocab indices?,"w = [5, 4000, 10532, 2224]","w = [4000, 10532, 2224, 5]","w = [2224, 5, 4000, 10532]","w = [10532, 2224, 4000, 5]","A) w = [5, 4000, 10532, 2224]"
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",What is the goal when learning a position embedding matrix Epos of shape [1 × N ]?,To learn a position embedding matrix Epos of shape [N × 1],To learn a position embedding matrix Epos of shape [1 × N ],To learn a position embedding matrix Epos of shape [N × N ],To learn a position embedding matrix Epos of shape [1 × 1],B) To learn a position embedding matrix Epos of shape [1 × N ]
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",What is the shape of the position embedding matrix Epos when it is learned?,[N × 1],[1 × N ],[N × N ],[1 × 1],B) [1 × N ]
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",What is the formula for calculating the composite embeddings X in a Transformer block?,X = WordEmbeddings + PositionEmbeddings,X = WordEmbeddings - PositionEmbeddings,X = WordEmbeddings * PositionEmbeddings,X = WordEmbeddings + PositionEmbeddings,D) X = WordEmbeddings + PositionEmbeddings
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",How are position embeddings learned in a Transformer block?,They are pre-trained and then fixed during training.,They are randomly initialized and learned along with other parameters during training.,They are learned only from the word embeddings.,They are not learned at all.,B) They are randomly initialized and learned along with other parameters during training.
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",What is the primary function of the language modeling head in a transformer-based architecture?,To map a [1 x d] vector to a [1 x |V|] probability distribution over the vocabulary,To map a [1 x |V|] vector of logits to a [1 x d] vector,To perform a position embedding,To perform a word embedding,A) To map a [1 x d] vector to a [1 x |V|] probability distribution over the vocabulary
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",What is the purpose of the 'unembedding matrix' in the language modeling head?,To map from a [1 x |V|] vector of logits to a [1 x d] vector,To map from a [1 x d] vector to a [1 x |V|] vector of logits,To perform a softmax operation,To perform a transformer block operation,B) To map from a [1 x d] vector to a [1 x |V|] vector of logits
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",How many steps are involved in mapping a [1 x d] vector to a [1 x |V|] probability distribution over the vocabulary in the language modeling head?,1 step,2 steps,3 steps,4 steps,B) 2 steps
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",What is the operation applied to a [1 x |V|] vector of logits to produce a [1 x |V|] vector of probabilities in the language modeling head?,Unembedding matrix,Softmax,Transformer block,Position embedding,B) Softmax
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",What is the output format of each transformer block in the architecture?,[1 x |V|] vector,[1 x d] vector,[1 x 2d] vector,[1 x 3d] vector,B) [1 x d] vector
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What is the primary function of the unembedding matrix in the language modeling head?,To map from a [1 x d] vector to a [1 x |V|] vector of logits,To map from a [1 x |V|] vector of logits to a [1 x |V|] vector of probabilities,To convert the embedding matrix to a unembedding matrix,To normalize the input vector to have a unit length,A) To map from a [1 x d] vector to a [1 x |V|] vector of logits
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What is the shape of the unembedding matrix ET?,[|V| x d],[d x |V|],[|V| x |V|],[d x d],B) [d x |V|]
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",In which step of the language modeling head are logits converted to probabilities?,Step 1: Using the unembedding matrix,Step 2: Using softmax,Both steps 1 and 2,Neither step 1 nor 2,B) Step 2: Using softmax
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",How many columns does the unembedding matrix ET have?,d,|V|,d + |V|,1,B) |V|
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What is the input and output shape of the language modeling head?,[1 x d] -> [1 x |V|],[1 x |V|] -> [1 x d],[1 x d] -> [1 x d],[1 x |V|] -> [1 x |V|],A) [1 x d] -> [1 x |V|]
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the purpose of the linear layer in the language modeling head?,To project from the output embedding to a probability distribution over words,To map from a one-hot vector to an embedding,To perform the reverse mapping from an embedding to a vector over the vocabulary,To compute the probability of a word given counts of its occurrence with the prior words,C) To perform the reverse mapping from an embedding to a vector over the vocabulary
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",Why is the transpose of the embedding matrix called the 'unembedding layer'?,Because it performs the same mapping as the embedding matrix,Because it is used to map from a one-hot vector to an embedding,Because it performs the reverse mapping from an embedding to a vector over the vocabulary,Because it is used to compute the probability of a word given counts of its occurrence with the prior words,C) Because it performs the reverse mapping from an embedding to a vector over the vocabulary
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the shape of the logit vector u?,[|V| × d],[d × |V|],[1 × |V|],[1 × d],C) [1 × |V|]
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the purpose of the softmax layer in the language modeling head?,To project from the output embedding to a probability distribution over words,To map from a one-hot vector to an embedding,To turn the logits into probabilities over the vocabulary,To perform the reverse mapping from an embedding to a vector over the vocabulary,C) To turn the logits into probabilities over the vocabulary
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59","What is weight tying, and how is it used in the language modeling head?",Weight tying is the practice of sharing weights between two different matrices in the model,Weight tying is the practice of using different weights for two different matrices in the model,Weight tying is not used in the language modeling head,Weight tying is the practice of using the same weights for the embedding matrix and the unembedding layer,A) Weight tying is the practice of sharing weights between two different matrices in the model
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the primary function of the unembedding layer in the language modeling head?,To project from the output embedding to the logit vector,To map from a one-hot vector over the vocabulary to an embedding,To perform a reverse mapping from an embedding to a vector over the vocabulary,To normalize the output of the softmax layer,C) To perform a reverse mapping from an embedding to a vector over the vocabulary
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",How is the matrix of the linear layer in the language modeling head typically tied?,To the embedding matrix E,To the transpose of the embedding matrix E,To a separate matrix learned during training,To a matrix derived from the input tokens,B) To the transpose of the embedding matrix E
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the purpose of the softmax layer in the language modeling head?,To normalize the output of the feedforward layer,To project from the output embedding to the logit vector,To turn the logits into probabilities over the vocabulary,To perform a reverse mapping from an embedding to a vector over the vocabulary,C) To turn the logits into probabilities over the vocabulary
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the shape of the logit vector u in the language modeling head?,[1 x d],[1 x |V|],[|V| x d],[d x |V|],B) [1 x |V|]
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",Why is the unembedding layer called such in the language modeling head?,Because it performs a linear transformation,Because it maps from a one-hot vector to an embedding,Because it performs a reverse mapping from an embedding to a vector over the vocabulary,Because it is tied to the embedding matrix E,C) Because it performs a reverse mapping from an embedding to a vector over the vocabulary
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",What is the primary idea that underlies the amazing performance of language models?,Fine-tuning a pre-trained model on specific tasks,Using Position embeddings and the Language Model Head,First pretrain a transformer model on enormous amounts of text,Applying a pre-trained model to new tasks without modification,C) First pretrain a transformer model on enormous amounts of text
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",What is the recommended approach to developing language models?,Pretraining and fine-tuning on specific tasks simultaneously,Only fine-tuning a pre-trained model on specific tasks,Only pretraining a model on enormous amounts of text,Pretraining a model on specific tasks and then applying it to new tasks,C) Only pretraining a model on enormous amounts of text
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",How are large language models typically trained?,By pretraining on specific tasks and then fine-tuning on new tasks,By pretraining on enormous amounts of text and then applying it to new tasks,By fine-tuning pre-trained models on specific tasks,By pretraining on specific tasks and then pretraining on new tasks,B) By pretraining on enormous amounts of text and then applying it to new tasks
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",What is the role of Position embeddings and the Language Model Head in language models?,They are essential components in the pretraining process,They are used to fine-tune pre-trained models on specific tasks,They are used to apply pre-trained models to new tasks,They are not directly related to the pretraining process,D) They are not directly related to the pretraining process
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",What is the recommended approach to applying pre-trained language models to new tasks?,Pretraining the model on new tasks,Fine-tuning the pre-trained model on new tasks,Applying the pre-trained model to new tasks without modification,Re-training the pre-trained model on new tasks,C) Applying the pre-trained model to new tasks without modification
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the primary goal of the self-supervised training algorithm used in pretraining language models?,To predict the previous word in the sequence,To predict the next word in the sequence,To classify the sentiment of the text,To identify the speaker in a conversation,B) To predict the next word in the sequence
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the output of the language model when given a sequence of text?,A single word prediction,A probability distribution over the vocabulary,A sentiment analysis of the text,A topic model of the text,B) A probability distribution over the vocabulary
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the loss function being minimized during the self-supervised training of the language model?,The difference between the predicted and actual next word,The error in the prediction of the next word,The probability of the correct word in the output distribution,The entropy of the output distribution,B) The error in the prediction of the next word
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the role of the language modeling head in the transformer architecture?,To generate new text based on the input sequence,To predict the next word in the sequence,To classify the input sequence into a category,To identify the speaker in a conversation,B) To predict the next word in the sequence
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the term for the type of training algorithm used in pretraining language models?,Supervised training,Self-supervised training,Reinforced training,Unsupervised training,B) Self-supervised training
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71","What is the primary source of data used to train LLMs, aside from webCommon Crawl?",Colossal Clean Crawled Corpus (C4) and filtered patent text documents,"Wikipedia and news sites, with no patent text documents",Colossal Clean Crawled Corpus (C4) and filtered Wikipedia articles,Filtered adult content and toxicity reports,A) Colossal Clean Crawled Corpus (C4) and filtered patent text documents
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What is the approximate number of tokens in the Colossal Clean Crawled Corpus (C4)?,10 billion,50 billion,100 billion,156 billion,D) 156 billion
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What type of text is filtered out from the Colossal Clean Crawled Corpus (C4)?,"Patent text documents, Wikipedia, and news sites","Boilerplate, adult content, and toxicity","Wikipedia, news sites, and patent text documents",Patent text documents and boilerplate,"B) Boilerplate, adult content, and toxicity"
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What is the primary benefit of pretraining language models on large amounts of text?,Improved ability to recognize and generate grammatically correct sentences,Enhanced ability to understand and generate creative writing,Increased ability to process and understand vast amounts of knowledge,Faster processing speeds and reduced memory usage,C) Increased ability to process and understand vast amounts of knowledge
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What is an example of a type of text that is present in the Colossal Clean Crawled Corpus (C4)?,Novels and poetry,Wikipedia articles and news sites,Patent text documents and academic papers,All of the above,D) All of the above
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",What is the primary concern with using web scraping for pretraining language models?,Copyright infringement and data consent issues,Limited data availability and poor data quality,High computational costs and resource requirements,Insufficient model evaluation and testing,A) Copyright infringement and data consent issues
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",What is an open legal question in the context of web scraping for pretraining language models?,Whether fair use doctrine in the US allows for web scraping,Whether website owners can indicate they don't want their site crawled,Whether websites contain private IP addresses and phone numbers,Whether large language models are subject to copyright laws,A) Whether fair use doctrine in the US allows for web scraping
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",What is a potential privacy concern with using web scraping for pretraining language models?,Exposure of sensitive user information,Disclosure of private IP addresses and phone numbers,Unauthorized access to secure websites,Invasion of website owners' privacy,B) Disclosure of private IP addresses and phone numbers
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",What is the primary reason for pretraining large language models on a large amount of text?,To improve model interpretability and transparency,To reduce computational costs and resource requirements,To enable the models to perform a wide range of tasks,To increase model evaluation and testing efficiency,C) To enable the models to perform a wide range of tasks
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73","What is a potential issue with using web scraping for pretraining language models, according to the text?",Data quality and consistency,Data quantity and availability,Copyright and data consent issues,Model performance and accuracy,C) Copyright and data consent issues
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",What is the name of the method that involves freezing some model parameters and training only a subset of parameters on new data?,Fine-Tuning,Parameter-Efficient Fine-Tuning (PEFT),Supervised Fine-Tuning (SFT),Continued Pre-Training,B) Parameter-Efficient Fine-Tuning (PEFT)
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75","What is the goal of the third kind of fine-tuning, which involves using a language model as a classifier or labeler for a specific task?",To predict the next word in a sequence,To classify text as positive or negative,To use the model as a kind of classifier or labeler for a specific task,To retrain the entire model from scratch,C) To use the model as a kind of classifier or labeler for a specific task
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",What is the name of the loss function used in supervised fine-tuning to train the language model to produce the desired response from a command in the prompt?,Cross-Entropy Loss,Mean Squared Error,Binary Cross-Entropy Loss,Negative Log-Likelihood,A) Cross-Entropy Loss
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",What is the formula for computing the perplexity of a model q on an unseen test set?,Pq(w1:n) = 1/n * ∑(1/Pq(wi)),Pq(w1:n) = 1/n * ∏(Pq(wi)),Pq(w1:n) = 1/n * ∑(Pq(wi)),Pq(w1:n) = 1/n * ∏(1/Pq(wi)),A) Pq(w1:n) = 1/n * ∑(1/Pq(wi))
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",Why is retraining all the parameters of a large language model slow and expensive?,Because it requires a lot of computational power,Because it requires a lot of data to train,Because it requires a lot of time and computational resources,Because the model is too complex,C) Because it requires a lot of time and computational resources
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",Why is perplexity used as a metric to evaluate language models instead of the raw probability of the test set?,Perplexity is more sensitive to the length of the test set than raw probability.,Raw probability is not normalized by the test set length.,Perplexity is more interpretable than raw probability.,Perplexity is less affected by tokenization than raw probability.,B) Raw probability is not normalized by the test set length.
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",What is the effect of increasing the length of the test set on the probability of the test set assigned by a language model?,The probability increases as the length of the test set increases.,The probability decreases as the length of the test set increases.,The probability remains the same regardless of the test set length.,The probability is not affected by the test set length.,B) The probability decreases as the length of the test set increases.
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",What is the relationship between minimizing perplexity and maximizing probability for a language model?,Minimizing perplexity is equivalent to minimizing probability.,Minimizing perplexity is equivalent to maximizing probability.,Minimizing perplexity is unrelated to maximizing probability.,Minimizing perplexity is equivalent to minimizing the difference between probability and 1.,B) Minimizing perplexity is equivalent to maximizing probability.
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",Why is it best to use perplexity when comparing language models that use the same tokenizer?,Perplexity is less sensitive to tokenization than raw probability.,Perplexity is more interpretable than raw probability.,Perplexity is more affected by tokenization than raw probability.,Perplexity is not affected by tokenization.,A) Perplexity is less sensitive to tokenization than raw probability.
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",What is the relationship between the probability range and the perplexity range for a language model?,"Probability range is [1,∞] and perplexity range is [0,1].","Probability range is [0,1] and perplexity range is [1,∞].","Probability range is [1,∞] and perplexity range is [0,1].","Probability range is [0,1] and perplexity range is [0,1].","B) Probability range is [0,1] and perplexity range is [1,∞]."
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",What does a lower perplexity of a model on the data indicate?,A worse model,A better model,A model with the same performance,A model that is more complex,B) A better model
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",What is a disadvantage of using perplexity to compare models?,It is insensitive to length/tokenization,It is sensitive to length/tokenization,It does not measure energy usage,It does not measure fairness,B) It is sensitive to length/tokenization
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",What is a benefit of using large models?,They take up less memory to store,They use less energy,They are faster to train,They can handle more complex tasks,D) They can handle more complex tasks
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",What is one of the factors that benchmarks for fairness measure?,Decreased performance for language from or about some groups,Increased performance for language from or about some groups,Energy usage,Size,A) Decreased performance for language from or about some groups
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",What is equivalent to minimizing perplexity?,Maximizing probability of the word sequence,Minimizing probability of the word sequence,Increasing perplexity,Decreasing perplexity,A) Maximizing probability of the word sequence
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81","A Large Language Model (LLM) generates a response that is not supported by the input context, but is plausible. What is this phenomenon known as?",Overfitting,Underfitting,Hallucination,Extrapolation,C) Hallucination
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81","A company uses a Large Language Model to generate content for its website. If the model's responses are not properly attributed, what is the potential issue?",Plagiarism,Copyright infringement,Trademark violation,Patent infringement,B) Copyright infringement
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81","A user inputs a question to a Large Language Model, and the model responds with an answer that is not supported by the input context. The model's response is also not explicitly mentioned in any publicly available text. What is the potential consequence?",The model's accuracy is decreased,The model's response is considered a hallucination,The model's performance is improved,The model's output is not relevant to the input,B) The model's response is considered a hallucination
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",A researcher uses a Large Language Model to analyze a dataset. The model generates insights that are not supported by the data. What is the potential issue with the model's output?,The model's results are biased,The model's results are unreliable,The model's results are incomplete,The model's results are accurate,B) The model's results are unreliable
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81","A company uses a Large Language Model to generate product descriptions. If the model's responses are not properly reviewed, what is the potential consequence?",The company's product descriptions are more engaging,The company's product descriptions are less accurate,The company's product descriptions are more consistent,The company's product descriptions are more informative,B) The company's product descriptions are less accurate
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",What is the primary concern addressed by the 'Toxicity and Abuse' section in the lecture content?,Intellectual property rights and copyright laws,The prevention of online harassment and abuse,Data protection and privacy regulations,Academic integrity and plagiarism policies,B) The prevention of online harassment and abuse
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",Which section of the lecture content deals with the legal and ethical implications of online interactions?,Toxicity and Abuse,Privacy,Copyright,All of the above,D) All of the above
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",What is the main focus of the 'Privacy' section in the lecture content?,Protecting users from online harassment and abuse,Ensuring the accuracy of online information,Preventing copyright infringement,Regulating the collection and use of personal data,D) Regulating the collection and use of personal data
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",Which two sections of the lecture content are primarily concerned with legal and regulatory issues?,Copyright and Toxicity and Abuse,Privacy and Copyright,Toxicity and Abuse and Privacy,Copyright and Privacy,C) Toxicity and Abuse and Privacy
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",What is the primary purpose of the 'Copyright' section in the lecture content?,To educate users about online safety and security,To explain the concept of intellectual property rights,To provide guidance on data protection and privacy,To outline the legal consequences of online abuse,B) To explain the concept of intellectual property rights
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87",What is the primary goal of the backward computation in neural network training?,To increase the loss between the true output and the estimated output,To update network weights to minimize the loss,To run the forward computation again to find a new estimate,To assess how much blame each weight deserves for the current loss,B) To update network weights to minimize the loss
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87",How is the weight update in neural network training typically computed?,wnew = w + η∇L,wnew = w − η∇L,wnew = w + η(L - ytrue),wnew = w − η(L - ytrue),B) wnew = w − η∇L
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87",What is the purpose of assessing how much blame each weight deserves for the current loss?,To determine the optimal learning rate,To identify the most influential weights in the network,To update the weights to increase the loss,To use the chain rule to compute partial derivatives,D) To use the chain rule to compute partial derivatives
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87",What is the role of the learning rate η in the weight update formula?,To increase the loss between the true output and the estimated output,To update network weights to minimize the loss,To adjust the rate at which the weights are updated,To run the forward computation again to find a new estimate,C) To adjust the rate at which the weights are updated
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87",In which step of the neural network training process is the loss L computed?,Step 1: Run forward computation to find our estimate y,Step 2: Compute loss L between ytrue and the estimated y,Step 3: Run backward computation; update weights to decrease L,Step 4: Update w: wnew = w − η∇L,B) Step 2: Compute loss L between ytrue and the estimated y
notes/LLM_cs124_week7_2025.pdf,44,"87, 88, 89",What is the formula to update a weight w in a neural network to decrease the loss L?,wnew = w - η * ∂L/∂w,wnew = w + η * ∂L/∂w,wnew = w - η * ∂L/∂y,wnew = w + η * ∂L/∂y,A) wnew = w - η * ∂L/∂w
notes/LLM_cs124_week7_2025.pdf,44,"87, 88, 89","What is the value of the estimated output y for the given input (x1, x2) = (4, -3) and initial weights w1 = 2, w2 = -1, b = 1?",11,12,13,14,B) 12
notes/LLM_cs124_week7_2025.pdf,44,"87, 88, 89",What is the value of the squared loss L for the estimated output y = 12 and true output ytrue = 10?,1,4,9,16,C) 9
notes/LLM_cs124_week7_2025.pdf,44,"87, 88, 89",What is the partial derivative of the loss L with respect to the weight w1 for the given example?,-2 * (ytrue - y) * x1,2 * (ytrue - y) * x1,-2 * (ytrue - y) * w1,2 * (ytrue - y) * w1,A) -2 * (ytrue - y) * x1
notes/LLM_cs124_week7_2025.pdf,44,"87, 88, 89",What is the value of the learning rate η used in the update rule for the weight w?,0.01,0.1,1,10,A) 0.01
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91",What is the initial value of the bias (b) in the forward pass computation?,0,1,2,−1,B) 1
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","What is the value of y computed in the forward pass with inputs (x1, x2) = (4, −3)?",10,12,−1,4,B) 12
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91",What is the value of L computed in the loss computation with ytrue = 10 and y = 12?,0,4,−4,16,B) 4
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91",What is the initial value of the weight w2 in the forward pass computation?,1,−1,2,0,B) −1
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91",What is the correct order of computation in the backward pass?,"Update weights, then compute loss","Create computation graph, then update weights","Compute loss, then create computation graph","Create computation graph, then compute loss","D) Create computation graph, then compute loss"
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93",What are the key nodes in the computation graph that we need gradients to update?,The input nodes x1 and x2,"The parameters w1, w2, and b",The loss node L and the intermediate nodes h1 and h2,The output node y and the true label ytrue,"B) The parameters w1, w2, and b"
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93",Why did we create intermediate nodes h1 and h2 in the computation graph?,To make the computation graph more complex,To simplify the computation of the loss gradients,To include the input nodes x1 and x2,To exclude the parameters w1 and w2,B) To simplify the computation of the loss gradients
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93",What is the purpose of the backward pass in the computation graph?,To create the computation graph,To compute the loss gradients for the weights,To update the parameters of the model,To normalize the input data,B) To compute the loss gradients for the weights
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93",What is the formula for computing the loss L?,L = (ytrue - y)^2,L = ytrue + y,L = (ytrue - y)^2 + h1 + h2 + b,L = ytrue * y,A) L = (ytrue - y)^2
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93",Why did we not create nodes for the input data x1 and x2?,Because they are not part of the model parameters,Because they are not necessary for the computation of the loss gradients,Because they are already included in the intermediate nodes h1 and h2,Because they are not part of the computation graph,B) Because they are not necessary for the computation of the loss gradients
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97",What is the formula to compute the loss gradient for the weights w1 and w2?,2(ytrue - y) * -1,2(ytrue - y) * 1,2(ytrue - y) * -1 * 4,2(y - ytrue) * 1,A) 2(ytrue - y) * -1
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97","Given y = 12, ytrue = 10, x1 = 4, and x2 = -3, what is the value of the loss L?",4,12,16,20,C) 16
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97","In the backpropagation process, what is the formula to compute the value of h2?",w2x2,w1x1 + w2x2,h1 + h2 + b,w1x1 + h2,A) w2x2
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97","Given the equation h1 = w1x1, what is the value of w1 if h1 = 12 and x1 = 4?",2,4,6,8,A) 2
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97",What is the formula to compute the value of y?,h1 + h2 + b,h1 + w2x2,w1x1 + h2,h1 + w2x2 + b,A) h1 + h2 + b
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","Given the equation for the loss L = (ytrue - y)^2, what is the value of the partial derivative of L with respect to y?",2(ytrue - y),2(ytrue + y),2(ytrue - y)^2,-2(ytrue - y),A) 2(ytrue - y)
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99",What is the value of x2 that results in the least change in the loss function during backpropagation?,-3,4,1,12,A) -3
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99",What is the value of the learning rate η that results in the smallest update to the weight w1?,0.01,0.1,1,10,B) 0.1
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","What is the new value of the weight w1 after one iteration of backpropagation and weight update, given η = 0.01?",1.84,0.84,0.96,2.84,A) 1.84
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99",What is the partial derivative of the loss L with respect to the bias b?,-4,4,12,-12,B) 4
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","What is the value of the new weight for w1 after one iteration of backpropagation, given η = 0.01?",2 - 0.01 * 16,2 + 0.01 * 16,2 - 0.01 * 12,2 + 0.01 * 12,A) 2 - 0.01 * 16
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","What is the value of the new weight for w2 after one iteration of backpropagation, given η = 0.01?",-1 - 0.01 * -12,-1 + 0.01 * -12,-1 - 0.01 * 12,-1 + 0.01 * 12,A) -1 - 0.01 * -12
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","What is the new value of b after one iteration of backpropagation, given η = 0.01?",1 - 0.01 * 4,1 + 0.01 * 4,1 - 0.01 * -4,1 + 0.01 * -4,A) 1 - 0.01 * 4
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","After one iteration of backpropagation, what is the expected result of the forward pass, given ytrue = 10?",y = w1x1 + w2x2 + b,y = w1x1 + w2x2 - b,y = w1x1 - w2x2 + b,y = w1x1 - w2x2 - b,A) y = w1x1 + w2x2 + b
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101",What is the relationship between the value of the new weights and the convergence of the neural network during backpropagation?,Increasing the value of the new weights leads to faster convergence.,Decreasing the value of the new weights leads to faster convergence.,The value of the new weights has no effect on convergence.,The new weights should be initialized to their final values to converge faster.,C) The value of the new weights has no effect on convergence.
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","In a multi-head attention layer, what is the dimensionality of the key and query embeddings for each head?","dk = 64, dv = 64","dk = 512, dv = 512","dk = 64, dv = 512","dk = 512, dv = 64","C) dk = 64, dv = 512"
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103",What is the shape of the weight layer WK_i for each head i in a multi-head attention layer?,[d x dk],[dk x d],[d x dv],[dv x dk],B) [dk x d]
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","In the multi-head attention computation, what is the output shape of each head?",[1 x d],[1 x dk],[1 x dv],[1 x hd],C) [1 x dv]
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103",What is the purpose of using multiple attention heads in a multi-head attention layer?,To reduce the dimensionality of the input,To increase the dimensionality of the output,To attend to different linguistic relationships or patterns in the context,To simplify the attention computation,C) To attend to different linguistic relationships or patterns in the context
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103",What is the final output shape of the multi-head attention layer after concatenating the outputs from each head?,[1 x d],[1 x hd],[1 x hdv],[1 x dk],C) [1 x hdv]
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105","How many sets of key, query, and value matrices are used in a multi-head self-attention layer?",One set,Two sets,"Multiple sets, equal to the number of attention heads",The same number of sets as the model dimension,"C) Multiple sets, equal to the number of attention heads"
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the purpose of the mask function in the self-attention computation?,To scale the scores of the query-key comparisons,To take the softmax of the query-key scores,To eliminate any knowledge of words that follow the query,To compute the dot products between each pair of tokens,C) To eliminate any knowledge of words that follow the query
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the shape of the QK| matrix resulting from the multiplication of Q and K?,N x N,N x d,d x N,d x d,A) N x N
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",How can the self-attention computation be parallelized using a single matrix X?,By computing each query-key comparison separately,"By multiplying X by the key, query, and value matrices",By using a separate attention head for each token,By using a different algorithm for self-attention,"B) By multiplying X by the key, query, and value matrices"
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the output shape of each head in a multi-head attention layer?,1 x d,1 x hdv,N x dv,N x d,B) 1 x hdv
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What is the shape of the matrix Q after multiplying X by the query matrix WQ?,N × d,N × dk,N × dv,d × N,B) N × dk
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What is the purpose of the mask function in the self-attention computation?,To allow the model to learn the order of the input sequence,To eliminate any knowledge of words that follow in the sequence,To introduce noise into the computation,To speed up the computation,B) To eliminate any knowledge of words that follow in the sequence
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What is the shape of the output of each of the A attention heads in multi-head attention?,N × d,N × dk,N × dv,N × hdv,D) N × hdv
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",Why does the self-attention computation have a problem in the setting of language modeling?,Because it is too slow,Because it is too complex,Because it allows the model to guess the next word if it already knows it,Because it does not use the entire input sequence,C) Because it allows the model to guess the next word if it already knows it
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What is the shape of the matrix QK| after multiplying Q and K?,N × d,N × dk,N × dv,N × N,D) N × N
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What is the purpose of the mask function in the self-attention computation?,To eliminate any knowledge of words that precede the query,To eliminate any knowledge of words that follow the query,To improve the computational efficiency of the self-attention computation,To reduce the dimensionality of the input embeddings,B) To eliminate any knowledge of words that follow the query
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What is the effect of the softmax function on the masked QK| matrix?,It sets all elements to a constant value,It sets the upper-triangular portion of the matrix to zero,It sets the lower-triangular portion of the matrix to zero,It leaves the matrix unchanged,B) It sets the upper-triangular portion of the matrix to zero
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What is the computational complexity of the self-attention computation?,Linear in the length of the input,Quadratic in the length of the input,Exponential in the length of the input,Constant,B) Quadratic in the length of the input
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What is the purpose of the final linear projection WO in the multi-head attention layer?,To reduce the dimensionality of the output embeddings,To reshape the output embeddings to the original output dimension,To apply a non-linear transformation to the output embeddings,To apply a linear transformation to the output embeddings,B) To reshape the output embeddings to the original output dimension
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What is the shape of the output of each attention head in the multi-head attention layer?,N x dk,N x dv,N x hdv,N x d,C) N x hdv
notes/LLM_cs124_week7_2025.pdf,56,"111, 112, 113",What is the shape of the matrix X in the context of a transformer block?,Matrix X is of shape [N⇥d] where N is the context length and d is the dimensionality of the embedding.,Matrix X is of shape [d⇥N] where N is the context length and d is the dimensionality of the embedding.,Matrix X is of shape [d⇥d] where d is the dimensionality of the embedding.,Matrix X is of shape [N⇥|V|] where N is the context length and |V| is the size of the vocabulary.,A) Matrix X is of shape [N⇥d] where N is the context length and d is the dimensionality of the embedding.
notes/LLM_cs124_week7_2025.pdf,56,"111, 112, 113",What is the purpose of the input positional embedding in a transformer?,To provide an initial representation for the input token.,To incorporate context and play a different role depending on the kind of language model.,To provide a unique representation for each position in the input sequence.,To normalize the input embeddings.,C) To provide a unique representation for each position in the input sequence.
notes/LLM_cs124_week7_2025.pdf,56,"111, 112, 113",How are token embeddings selected from the embedding matrix E?,By indexing the corresponding rows from E using vocabulary indices.,By multiplying the embedding matrix E by a one-hot vector.,By normalizing the embedding matrix E.,By applying a feed-forward neural network to the embedding matrix E.,A) By indexing the corresponding rows from E using vocabulary indices.
notes/LLM_cs124_week7_2025.pdf,56,"111, 112, 113",What is the shape of a one-hot vector for a word in the vocabulary?,Shape [d⇥d] where d is the dimensionality of the embedding.,Shape [1⇥|V|] where |V| is the size of the vocabulary.,Shape [|V|⇥d] where |V| is the size of the vocabulary and d is the dimensionality of the embedding.,Shape [d⇥|V|] where |V| is the size of the vocabulary and d is the dimensionality of the embedding.,B) Shape [1⇥|V|] where |V| is the size of the vocabulary.
notes/LLM_cs124_week7_2025.pdf,56,"111, 112, 113",What is the result of multiplying a one-hot vector by the embedding matrix E?,A vector of all zeros.,A vector of all ones.,The embedding vector for the word corresponding to the non-zero element in the one-hot vector.,A vector of random numbers.,C) The embedding vector for the word corresponding to the non-zero element in the one-hot vector.
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",What is the shape of the input matrix X in the transformer block?,N x d,d x N,N x |V|,|V| x d,A) N x d
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",How are token embeddings selected from the embedding matrix E?,Using one-hot vectors of shape [1 x d],Using indexing to select rows from E,Using a linear transformation matrix,Using a non-linear activation function,B) Using indexing to select rows from E
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",What is the purpose of the input positional embedding in the transformer?,To provide a fixed representation for each token,To capture the position of each token in the sequence,To introduce a bias term for each token,To normalize the token embeddings,B) To capture the position of each token in the sequence
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",What is the shape of the output H in the transformer block?,N x d,d x N,N x |V|,|V| x d,A) N x d
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",How many embedding vectors are computed in parallel for each token in the transformer?,One,Two,N,h,B) Two
