pdf_name,chunk_number,total_chunks,pages,question,option_A,option_B,option_C,option_D,correct_answer
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",How do large language models generate text?,By sampling possible next words and assigning probabilities to sequences of words,By learning to guess the next word and predicting the entire sequence at once,By generating a random sequence of words,By using a combination of machine learning and natural language processing,A) By sampling possible next words and assigning probabilities to sequences of words
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",What is one key difference between simple n-gram language models and large language models?,Large language models are trained on smaller amounts of text,Large language models are trained on counts computed from lots of text,Large language models are not trained by learning to guess the next word,Large language models are not used for natural language processing,B) Large language models are trained on counts computed from lots of text
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",What is one useful aspect of language knowledge learned by large language models?,They are able to generate random sequences of words,They are able to learn a lot of useful language knowledge since training on a lot of text,They are not able to learn any language knowledge,They are only able to learn language knowledge from small amounts of text,B) They are able to learn a lot of useful language knowledge since training on a lot of text
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",How are large language models similar to simple n-gram language models?,Large language models are not similar to simple n-gram language models,Large language models are similar but assign probabilities to sequences of words differently,Large language models are similar and both assign probabilities to sequences of words,Large language models are similar but are trained on smaller amounts of text,C) Large language models are similar and both assign probabilities to sequences of words
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",What type of pretraining is typically used for decoder-only models?,Masked language modeling,Next sentence prediction,Causal language modeling,Perplexity minimization,C) Causal language modeling
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which of the following architectures is an example of an encoder-decoder model?,BERT,GPT,Flan-T5,HuBERT,C) Flan-T5
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",What is the primary limitation of decoder-only models?,They can only generate text one word at a time,They can only condition on future words,They are slow and computationally expensive,They are not suitable for question-answering tasks,B) They can only condition on future words
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which of the following is a characteristic of encoder models?,They can only generate text one word at a time,They can condition on future words,They get bidirectional context,They are only suitable for language translation tasks,C) They get bidirectional context
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",What is the term used to describe decoder-only models that predict words left to right?,Causal LLMs,Autoregressive LLMs,Left-to-right LLMs,Bidirectional LLMs,C) Left-to-right LLMs
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",What type of pretraining is typically used for Encoder models like BERT?,Left-to-right pretraining,Masked Language Model pretraining,Right-to-left pretraining,Bidirectional pretraining,B) Masked Language Model pretraining
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",What is the primary limitation of Decoder-only models like the ones used in GPT?,They can condition on future words,They can generate text but not condition on future words,They can only be used for machine translation,They can only be used for speech recognition,B) They can generate text but not condition on future words
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which type of architecture is typically used for machine translation and speech recognition tasks?,Decoder-only models,Encoder-only models,Encoder-Decoder models,Masked Language Model,C) Encoder-Decoder models
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",How do Encoder-Decoder models differ from Decoder-only models?,Encoder-Decoder models can generate text but not condition on future words,Encoder-Decoder models can condition on future words,Encoder-Decoder models are only used for machine translation,Encoder-Decoder models are only used for speech recognition,B) Encoder-Decoder models can condition on future words
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",What type of pretraining is typically used for Decoder models like GPT?,Masked Language Model pretraining,Left-to-right pretraining,Right-to-left pretraining,Bidirectional pretraining,A) Masked Language Model pretraining
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What type of tasks can be turned into tasks of predicting words?,Only machine translation and speech recognition,Many tasks can be turned into tasks of predicting words!,Only encoder-decoder models,Only decoder-only models,B) Many tasks can be turned into tasks of predicting words!
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What is another term for decoder-only models?,Masked LLMs,Causal LLMs,Encoder-Decoder models,Unidirectional LLMs,B) Causal LLMs
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What is a characteristic of decoders that makes them 'nice to generate from'?,They can condition on future words,They can generate from any word,They can’t condition on future words,They are slow to train,C) They can’t condition on future words
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What is a characteristic of encoders that makes them 'get bidirectional context'?,They can generate from any word,They can condition on future words,They get bidirectional context – can condition on future!,They are slow to train,C) They get bidirectional context – can condition on future!
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What type of pretraining is influenced by the neural architecture?,Only decoder-only models,Only encoder-decoder models,"The neural architecture influences the type of pretraining, and natural use cases",None of the above,"C) The neural architecture influences the type of pretraining, and natural use cases"
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",What type of language models can condition on future words?,Causal LLMs,Autoregressive LLMs,Decoder-only LLMs,Encoder-only LLMs,A) Causal LLMs
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",Which of the following tasks can be cast as word prediction?,Named Entity Recognition,Sentiment Analysis,Part-of-Speech Tagging,Dependency Parsing,B) Sentiment Analysis
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11","In the context of question answering, how can a language model be used to compute the probability distribution over possible next words?",By giving the model a question and a token like 'A:',By giving the model a question and a token like 'Q:',By giving the model a question and a token like 'tl;dr',By giving the model a question and a token like 'S:',A) By giving the model a question and a token like 'A:'
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",What is the purpose of the 'Language Modeling Head' in a transformer-based large language model?,To generate the next token in the sequence,To compute the probability distribution over all words in the vocabulary,To condition on future words,To perform named entity recognition,B) To compute the probability distribution over all words in the vocabulary
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",How can the task of text summarization be cast as language modeling?,By giving a large language model a text and following the text by a token like 'tl;dr',By giving a large language model a text and following the text by a token like 'Q:',By giving a large language model a text and following the text by a token like 'A:',By giving a large language model a text and following the text by a token like 'S:',A) By giving a large language model a text and following the text by a token like 'tl;dr'
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",Which task can be cast as word prediction by giving a language model a question and a token like 'A'?,Sentiment analysis,Question answering,Text summarization,Language modeling,B) Question answering
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",What token is used to suggest that an answer should come next in the task of question answering?,Q,A,tl;dr,P,B) A
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",Which of the following tasks can be cast as language modeling by giving a large language model a text and following the text by a token like 'tl;dr'?,Sentiment analysis,Question answering,Text summarization,Language modeling,C) Text summarization
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",How does a language model compute the probability distribution over possible next words given a prefix?,By using a stack of neural networks called transformers,By taking a prefix string of words s and computing the probability that word w follows s,By using a pre-trained language model,By using a rule-based system,B) By taking a prefix string of words s and computing the probability that word w follows s
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13",What is the function of the token 'tl;dr' in text summarization?,To suggest that an answer should come next,To suggest that a summary should be generated,To suggest that a longer text should be generated,To suggest that a shorter text should be generated,D) To suggest that a shorter text should be generated
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",What is the primary function of the system being described in the context of language modeling?,To generate a complete sentence from a given prefix,To compute the probability that a word follows a given prefix,To translate text from one language to another,To summarize a given piece of text,B) To compute the probability that a word follows a given prefix
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",What is the primary component of modern Large Language Models?,Stacks of neural networks called neural networks,Stacks of neural networks called transformers,Single neural network called a transformer,Single neural network called a language model,B) Stacks of neural networks called transformers
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",What type of networks are used to build Large Language Models?,Convolutional Neural Networks,Recurrent Neural Networks,Stacks of neural networks called transformers,Generative Adversarial Networks,C) Stacks of neural networks called transformers
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",What is the specific type of neural network being referred to in the context of Large Language Models?,Neural networks,Transformers,Recurrent Neural Networks,Generative Neural Networks,B) Transformers
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What was the primary innovation in the Transformer architecture?,The use of recurrent neural networks (RNNs) for sequence transduction,The replacement of RNNs with self-attention and the introduction of scaled dot-product attention,The use of convolutional neural networks (CNNs) for sequence transduction,The use of ensemble methods for improving model performance,B) The replacement of RNNs with self-attention and the introduction of scaled dot-product attention
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What was the key benefit of the Transformer architecture in terms of training time?,It required significantly more time to train compared to other models,It was more parallelizable and required significantly less time to train,It was only suitable for small datasets and required less training time,It was only suitable for large datasets and required more training time,B) It was more parallelizable and required significantly less time to train
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What was the BLEU score achieved by the Transformer model on the WMT 2014 English-to-German translation task?,25.4,28.4,30.8,35.6,B) 28.4
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Who proposed the scaled dot-product attention mechanism?,Ashish Vaswani,Noam Shazeer,Jakob Uszkoreit,Niki Parmar,B) Noam Shazeer
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What was the approximate year when GPUs began to be used in neural networks?,1990,2003,2004-6,2008,C) 2004-6
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",What year did GPUs begin to be used?,2003,2004-6,2008,2012,B) 2004-6
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",What was re-discovered in 2013?,Transformers,Attention,Static Word Embeddings,Contextual Word Embeddings,C) Static Word Embeddings
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",In what year did Pretraining and Contextual Word Embeddings emerge?,2015,2017,2018,2019,C) 2018
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",What is the primary function of the Language Modeling Head in a Transformer model?,Input Encoding,Output Decoding,Language Modeling,Token Classification,C) Language Modeling
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",What is the purpose of the Input Encoding in a Transformer model?,To generate logits,To process the input tokens,To perform multi-task learning,To apply prompting,B) To process the input tokens
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",What year did Attention emerge in NLP models?,2012,2013,2015,2017,C) 2015
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",What is the term for the re-use of a model for a different task or application?,Multi-Task Learning,Prompting,Pretraining,Fine-Tuning,A) Multi-Task Learning
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",What year did ChatGPT emerge?,2015,2017,2018,2022,D) 2022
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",What is the purpose of the stacked Transformer Blocks in a Transformer model?,To process the input tokens sequentially,To generate logits for each token,To apply attention mechanism to each token,To capture long-range dependencies in the input sequence,D) To capture long-range dependencies in the input sequence
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",In what year did GPUs take off?,2008,2012,2013,2015,B) 2012
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21","What is the main issue with using static embeddings, such as word2vec, for representing word meanings?",They are too complex and difficult to compute,They do not reflect how a word's meaning changes in context,They are not effective for capturing nuances of language,They are not scalable for large datasets,B) They do not reflect how a word's meaning changes in context
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",Which of the following is an intuition behind the need for contextual embeddings?,A word's meaning should be the same across all contexts,A word's meaning should be different in different contexts,A word's meaning should only depend on its part of speech,A word's meaning should only depend on its definition,B) A word's meaning should be different in different contexts
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",How do contextual embeddings differ from static embeddings?,"Contextual embeddings are computed using a single vector, while static embeddings use multiple vectors",Contextual embeddings express different meanings for a word depending on the surrounding words,"Contextual embeddings are only used for short texts, while static embeddings are used for long texts","Contextual embeddings are only used for specific domains, while static embeddings are used for general language understanding",B) Contextual embeddings express different meanings for a word depending on the surrounding words
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",What is a key challenge in computing contextual embeddings?,Computing contextual embeddings is a trivial task,Computing contextual embeddings requires a lot of computational resources,Computing contextual embeddings requires the use of attention mechanisms,Computing contextual embeddings does not require any special techniques,C) Computing contextual embeddings requires the use of attention mechanisms
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",What should be the properties of 'it' in the sentence 'The chicken didn't cross the road because it...?,A fixed vector that expresses a single meaning,A vector that changes depending on the surrounding words,A word that can only refer to the chicken,A word that can only refer to the street,B) A vector that changes depending on the surrounding words
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",How does the word 'it' integrate information from neighboring words in the sentence 'The chicken didn't cross the road because it...?,By attending to all neighboring words equally,By selectively integrating information from all neighboring words,By ignoring the neighboring words,By attending to only the previous word,B) By selectively integrating information from all neighboring words
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",What is the primary purpose of attention in the context of contextual embedding?,To uniformly integrate information from all neighboring words,To selectively integrate information from all neighboring words,To ignore information from neighboring words,To focus solely on the current word,B) To selectively integrate information from all neighboring words
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25","According to the definition, what is attention doing when computing the embedding for a token?",A weighted average of vectors,A weighted sum of vectors,A concatenation of vectors,A subtraction of vectors,B) A weighted sum of vectors
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",What is the result of a word 'attending to' some neighboring words more than others?,A uniform contextual embedding,A contextual embedding that reflects the relative importance of neighboring words,A contextual embedding that ignores neighboring words,A contextual embedding that focuses solely on the current word,B) A contextual embedding that reflects the relative importance of neighboring words
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",What is the relationship between a word's embedding and the information it 'attends to' from neighboring words?,The embedding is independent of the information attended to,The embedding is a function of the information attended to,The embedding is a weighted average of the information attended to,The embedding is a weighted sum of the information attended to,B) The embedding is a function of the information attended to
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",What is the mechanism described by the phrase'selectively integrating information'?,Self-attention,Attention,Selective weighting,Contextual embedding,B) Attention
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",What is the core intuition of attention in self-attention for language?,Comparing an item of interest to a collection of other items in a way that reveals their relevance in the current context,Computing a weighted sum of vectors,Using a dot product to compare words to other words,Normalizing scores with a softmax,A) Comparing an item of interest to a collection of other items in a way that reveals their relevance in the current context
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",What is the role of the softmax in the attention process?,To compute the similarity between two vectors,To create a probability distribution from a set of scores,"To project each input vector into a representation of its role as a key, query, or value",To compute the output for the current focus of attention,B) To create a probability distribution from a set of scores
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27","What is the purpose of the weight matrices WQ, WK, and WV in transformers?","To project each input vector into a representation of its role as a key, query, or value",To compute the similarity between two vectors,To create a probability distribution from a set of scores,To normalize the scores with a softmax,"A) To project each input vector into a representation of its role as a key, query, or value"
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",What is the result of the dot product between a query vector qi and a key vector kj?,A scalar value ranging from -1 to 1,A vector of weights that indicates the proportional relevance of each input to the input element i,A score that indicates the similarity between two vectors,A weighted sum of the value vectors v,C) A score that indicates the similarity between two vectors
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",What is the output calculation for ai in the attention process?,ai = Xj∑iaijxj,ai = Xj∑iaijvj,"ai = softmax(score(xi,xj))",ai = qi·kj,B) ai = Xj∑iaijvj
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29",What is the formula for computing the score between a current focus of attention and an element in the preceding context in a transformer model?,"score(xi,xj)=xi·xj","score(xi,xj)=qi·kj","score(xi,xj)=xi·kj","score(xi,xj)=qi·xj","B) score(xi,xj)=qi·kj"
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29",What are the three different roles that each input embedding plays during the course of the attention process in a transformer model?,"query, key, and value","query, key, and input","query, key, and output","query, value, and key","A) query, key, and value"
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29",What is the formula for computing the output value ai in a transformer model?,ai=Xjiaijxj,ai=Xjiaijvj,ai=Xjiaijxi,ai=Xjiaijxj,B) ai=Xjiaijvj
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29",What is the purpose of the softmax calculation in a transformer model?,to compute the output value ai,to normalize the scores to provide a probability distribution,to compute the score between a current focus of attention and an element in the preceding context,"to project each input vector xi into a representation of its role as a key, query, or value",B) to normalize the scores to provide a probability distribution
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29",What is the dimensionality of the key and query vectors in a transformer model?,1⇥d,1⇥dk,1⇥dv,1⇥dk and 1⇥dv,B) 1⇥dk
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31",What role does the vector xi play when it is being compared to the current element in the self-attention mechanism?,query,key,value,none of the above,A) query
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","In the self-attention mechanism, what happens to the value of a preceding element?",it is ignored,it is used as a query,it is weighted and summed,it is used as a key,C) it is weighted and summed
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","In the self-attention mechanism, what is the purpose of the self-attention distribution?",to compute the similarity between the current element and the preceding inputs,to compute the similarity between the preceding inputs,to compute the weighted sum of the preceding elements,to ignore the preceding inputs,A) to compute the similarity between the current element and the preceding inputs
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","In the self-attention mechanism, which two roles do the vector xi play?",query and value,key and value,query and key,key and none,B) key and value
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31",What is the purpose of the weighted sum of the value of a preceding element in the self-attention mechanism?,to compute the similarity between the current element and the preceding inputs,to ignore the current element,to compute the weighted sum of the preceding elements,to use the current element as a query,C) to compute the weighted sum of the preceding elements
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","In the self-attention mechanism, which element is being compared to the preceding inputs to determine a similarity?",the current element,the preceding element,the current value,none of the above,A) the current element
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",Which role is represented by the projection of an input vector xi into a representation of its role as key?,The current element being compared to the preceding inputs,A preceding input that is being compared to the current element to determine a similarity weight,A value of a preceding element that gets weighted and summed up to compute the output for the current element,None of the above,B) A preceding input that is being compared to the current element to determine a similarity weight
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",What is the purpose of scaling the dot product in the attention calculation by a factor related to the size of the embeddings?,To increase the magnitude of the dot product,To avoid numerical issues and loss of gradients during training,To reduce the computational complexity of the attention calculation,To improve the accuracy of the attention model,B) To avoid numerical issues and loss of gradients during training
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33","In the self-attention calculation, what is the output calculation for aij based on?",A weighted sum over the key vectors,A weighted sum over the value vectors,A weighted sum over the query vectors,A weighted sum over the key and query vectors,B) A weighted sum over the value vectors
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",How many different roles do input embedding play during the attention process?,2,3,4,5,B) 3
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",What is the formula for computing the score of the similarity between xi and xj?,qi · kjpdk,qi · kjk,qi · vkjpdk,qi · vkjk,A) qi · kjpdk
notes/LLM_cs124_week7_2025.pdf,17,"33, 34, 35",What are the three different roles that each input embedding plays during the attention process?,"Query, Key, and Weight","Query, Key, and Value","Input, Output, and Attention","Embedding, Representation, and Attention","B) Query, Key, and Value"
notes/LLM_cs124_week7_2025.pdf,17,"33, 34, 35",What is the purpose of scaling the dot product by a factor related to the size of the embeddings?,To increase the value of the dot product,To avoid numerical issues and loss of gradients during training,To reduce the dimensionality of the query and key vectors,To improve the accuracy of the attention mechanism,B) To avoid numerical issues and loss of gradients during training
notes/LLM_cs124_week7_2025.pdf,17,"33, 34, 35",What is the formula for computing the similarity of the current element xi with some prior element xj?,"score(xi,xj) = qi · kj","score(xi,xj) = qi · kj / √(dk)","score(xi,xj) = qi · kj + √(dk)","score(xi,xj) = qi · kj - √(dk)","B) score(xi,xj) = qi · kj / √(dk)"
notes/LLM_cs124_week7_2025.pdf,17,"33, 34, 35",What is the formula for computing the output of the attention head for a single input vector xi?,headi = Xj ∑ aij vj,headi = Xj ∑ aij vj / √(dk),headi = Xj ∑ aij vj + √(dk),headi = Xj ∑ aij vj - √(dk),A) headi = Xj ∑ aij vj
notes/LLM_cs124_week7_2025.pdf,17,"33, 34, 35","What is the final output of the attention head, headi, used for?",To compute the similarity of the current element xi with some prior element xj,"To compute the weighted sum of the prior elements, each weighted by the similarity of its key to the query from the current element",To compute the output of the attention head for a single input vector xi,To compute the final output of the attention mechanism,"B) To compute the weighted sum of the prior elements, each weighted by the similarity of its key to the query from the current element"
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the purpose of the softmax function in the self-attention mechanism?,To compute the weighted sum of the prior elements,To normalize the scores into a probability distribution,To scale the dot product by a factor related to the size of the embeddings,To reshape the output of the head,B) To normalize the scores into a probability distribution
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37","What is the role of the weight matrices WQ, WK, and WV in the self-attention mechanism?","To project each input vector xi into a representation of its role as a key, query, or value",To scale the dot product by a factor related to the size of the embeddings,To compute the weighted sum of the prior elements,To normalize the scores into a probability distribution,"A) To project each input vector xi into a representation of its role as a key, query, or value"
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the purpose of the dimension dk in the self-attention mechanism?,To scale the dot product by a factor related to the size of the embeddings,"To project each input vector xi into a representation of its role as a key, query, or value",To compute the weighted sum of the prior elements,To reshape the output of the head,A) To scale the dot product by a factor related to the size of the embeddings
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the function of the matrix WO in the self-attention mechanism?,To scale the dot product by a factor related to the size of the embeddings,"To project each input vector xi into a representation of its role as a key, query, or value",To reshape the output of the head,To normalize the scores into a probability distribution,C) To reshape the output of the head
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",How many attention heads are used in the multi-head attention mechanism?,A single attention head,Multiple attention heads,All attention heads are used simultaneously,The number of attention heads is fixed at 64,B) Multiple attention heads
notes/LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the purpose of using multiple attention heads in the multi-head attention mechanism?,To model different aspects of the relationships among inputs,To scale the dot product by a factor related to the size of the embeddings,To compute the weighted sum of the prior elements,To reshape the output of the head,A) To model different aspects of the relationships among inputs
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",What is the primary purpose of Attention in the context of token representation?,To incorporate numerical information into the embedding,To enrich the representation of a token by incorporating contextual information,To reduce the dimensionality of the embedding,To increase the noise in the embedding,B) To enrich the representation of a token by incorporating contextual information
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",How do the embeddings for a word change in different contexts according to the Attention method?,They remain the same,They become identical,They will be different,They will be identical and then the same,C) They will be different
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",What can be passed up layer by layer in the enriched representation of a token?,Only the embedding,Only the context,The enriched representation,Neither the embedding nor the context,C) The enriched representation
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","In the Transformer architecture, what is the primary function of the residual stream?",To normalize the input tokens,To modify each token and then combine the results,To attend to the input tokens and generate output tokens,To apply feedforward transformations to the input tokens,B) To modify each token and then combine the results
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",What is the purpose of the Layer Norm operation in the Transformer block?,To normalize the output of the multi-head attention mechanism,To normalize the input tokens before they are fed into the multi-head attention mechanism,To apply a feedforward transformation to the input tokens,To modify each token and then combine the results,A) To normalize the output of the multi-head attention mechanism
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","In the Transformer block, what is the sequence of operations applied to the input tokens?",Multi-head attention -> Layer Norm -> Feedforward -> Residual stream,Layer Norm -> Multi-head attention -> Feedforward -> Residual stream,Residual stream -> Layer Norm -> Multi-head attention -> Feedforward,Feedforward -> Layer Norm -> Multi-head attention -> Residual stream,A) Multi-head attention -> Layer Norm -> Feedforward -> Residual stream
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",What is the effect of the residual stream on the input tokens in the Transformer block?,It adds the output of the multi-head attention mechanism to the input tokens,It subtracts the output of the multi-head attention mechanism from the input tokens,It replaces the input tokens with the output of the multi-head attention mechanism,It applies a feedforward transformation to the input tokens,A) It adds the output of the multi-head attention mechanism to the input tokens
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the purpose of the residual stream in the transformer architecture?,To introduce non-linearity to the feedforward layer,To allow components to read their input from the stream and add their output back in,To reduce the dimensionality of the input vector,To enable the transformer to handle multiple input sequences,B) To allow components to read their input from the stream and add their output back in
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the typical dimensionality of the hidden layer in the feedforward network?,Smaller than the model dimensionality,Larger than the model dimensionality,Equal to the model dimensionality,Depends on the specific model implementation,B) Larger than the model dimensionality
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",How many times is the vector xi normalized in the layer norm process?,Once,Twice,Three times,Four times,B) Twice
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the common metaphor used to describe the residual connections in the transformer architecture?,Residual stream,Residual connections,Layer norm,Feedforward layer,B) Residual connections
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the typical number of weight matrices in the feedforward layer?,One,Two,Three,Four,B) Two
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What is the primary purpose of layer normalization in a transformer block?,To scale the values of a hidden layer to a fixed range,To improve training performance by keeping values in a range that facilitates gradient-based training,To reduce the dimensionality of the input vector,To introduce non-linearity in the transformer block,B) To improve training performance by keeping values in a range that facilitates gradient-based training
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What is the formula for calculating the mean (μ) of a vector x in layer normalization?,μ = 1/d ∑ xi,μ = √(1/d ∑ xi^2),μ = 1/d ∑ (xi - mean),μ = 1/d ∑ (xi - 1),A) μ = 1/d ∑ xi
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45","In the standard implementation of layer normalization, what are the two learnable parameters introduced?",Gain and offset values,Scale and shift values,Bias and variance values,Mean and standard deviation values,A) Gain and offset values
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What is the equation for calculating the normalized vector x in layer normalization?,ˆx = (x - μ) / s,ˆx = (x - μ) + s,ˆx = x / (μ + s),ˆx = x + (μ - s),A) ˆx = (x - μ) / s
notes/LLM_cs124_week7_2025.pdf,22,"43, 44, 45",Which component of a transformer block takes input information from other tokens (residual streams)?,LayerNorm,MultiHeadAttention,FeedForward,Residual connection,B) MultiHeadAttention
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",What is the purpose of layer normalization in a transformer block?,To improve the stability of the transformer block during training,To reduce the effect of vanishing gradients in the transformer block,To normalize the values of a hidden layer in a range that facilitates gradient-based training,To increase the dimensionality of the input vector,C) To normalize the values of a hidden layer in a range that facilitates gradient-based training
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47","What is the formula to calculate the standard deviation, s, in layer normalization?",s = √(d * (xi^2)),s = √(d * (xi - μ)^2),s = √(d * (xi^2) / (d - 1)),s = √(d * (xi^2) / d),B) s = √(d * (xi - μ)^2)
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",What is the effect of layer normalization on the input vector x?,It increases the dimensionality of the input vector,It decreases the dimensionality of the input vector,It normalizes the vector components to have zero mean and a standard deviation of one,It scales the vector components by a gain value,C) It normalizes the vector components to have zero mean and a standard deviation of one
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",What is the role of the learnable parameters g and b in layer normalization?,They are used to scale the input vector x,They are used to shift the input vector x,They are used to add gain and offset values to the normalized vector,They are used to subtract the mean from the input vector x,C) They are used to add gain and offset values to the normalized vector
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",Which component of the transformer block takes as input information from other tokens?,Layer Norm,MultiHeadAttention,Feedforward,All of the above,B) MultiHeadAttention
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",What is the effect of attention heads on the high-dimensional embedding space?,It reduces the dimensionality of the embedding space,It increases the dimensionality of the embedding space,It contains information about the current token and about neighboring tokens in different subspaces of the vector space,It scales the embedding space by a gain value,C) It contains information about the current token and about neighboring tokens in different subspaces of the vector space
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",How many residual streams are involved in the transformer block?,1,2,3,N,B) 2
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",What type of embedding is added to the token embedding to create the final embedding in the Transformer model?,Semantic embedding,Token embedding,Positional embedding,Contextual embedding,C) Positional embedding
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49","In the Transformer model, what is the shape of the matrix X that contains the token and position embeddings?",[N × d],[d × N],[N × N],[d × d],A) [N × d]
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",What is the purpose of the positional embedding in the Transformer model?,To capture the semantic meaning of words,To capture the contextual relationships between words,To preserve the order of words in the input sequence,To reduce the dimensionality of the input data,C) To preserve the order of words in the input sequence
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",How many distinct embeddings are added together to create the final embedding for each word in the context?,One,Two,Three,Four,B) Two
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51",What is the purpose of adding two distinct embeddings for each input in the matrix X?,To create a single embedding for each word in the context,To add positional information to the word embeddings,To reduce the dimensionality of the word embeddings,To increase the number of rows in the embedding matrix E,B) To add positional information to the word embeddings
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51",What is the shape of the embedding matrix E?,[N × d],[|V | ×  d ],[d × N],[N × |V |],B) [|V | ×  d ]
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51",How are the word embeddings created from the embedding matrix E?,By averaging the rows of E,By selecting the corresponding rows from E,By concatenating the rows of E,By multiplying the rows of E,B) By selecting the corresponding rows from E
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","What is the output of tokenizing the string ""Thanks for all the"" using BPE and converting it into vocabulary indices?","[1, 2, 3, 4]","[5, 4000, 10532, 2224]","[10, 20, 30, 40]","[50, 60, 70, 80]","B) [5, 4000, 10532, 2224]"
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",What is the result of tokenizing the string 'Thanks for all the' using BPE and converting it into vocab indices?,"w = [5, 4000, 10532, 2224]","w = [3, 4000, 10532, 2224]","w = [5, 4000, 2224, 10532]","w = [4000, 10532, 2224, 5]","A) w = [5, 4000, 10532, 2224]"
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",How are position embeddings learned in a Transformer model?,Position embeddings are pre-trained and fixed during training.,Position embeddings are learned along with other parameters during training.,Position embeddings are only used for word embeddings.,Position embeddings are not used in Transformer models.,B) Position embeddings are learned along with other parameters during training.
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",What is the goal of learning a position embedding matrix Epos in a Transformer model?,To learn a position embedding matrix Epos of shape [N × 1].,To learn a position embedding matrix Epos of shape [1 × N ].,To learn a position embedding matrix Epos of shape [N × N ].,To learn a position embedding matrix Epos of shape [1 × 1].,B) To learn a position embedding matrix Epos of shape [1 × N ].
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",How are word and position embeddings combined in a Transformer model?,Word and position embeddings are multiplied together.,Word and position embeddings are subtracted from each other.,Word and position embeddings are summed together.,Word and position embeddings are concatenated together.,C) Word and position embeddings are summed together.
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",What is the primary function of the language modeling head in a transformer-based architecture?,To perform a self-attention mechanism within the transformer block,To map a [1 x d] vector to a [1 x |V|] probability distribution over the vocabulary,To compute the position embeddings of input tokens,To concatenate word and position embeddings,B) To map a [1 x d] vector to a [1 x |V|] probability distribution over the vocabulary
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",How does the language modeling head transform a [1 x d] vector from the transformer block into a probability distribution over the vocabulary?,Using only the softmax function,By applying the unembedding matrix followed by a softmax operation,Through a convolutional neural network,Through a recurrent neural network,B) By applying the unembedding matrix followed by a softmax operation
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",What is the purpose of the 'unembedding matrix' in the language modeling head?,To map a [1 x d] vector to a [1 x |V|] vector of logits,To compute the position embeddings of input tokens,To concatenate word and position embeddings,To perform a self-attention mechanism within the transformer block,A) To map a [1 x d] vector to a [1 x |V|] vector of logits
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",What type of transformation does the language modeling head apply to the output of the transformer block?,Linear transformation,Non-linear transformation,Concatenation of vectors,Element-wise multiplication,A) Linear transformation
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",In what order are the two steps performed in the language modeling head to produce a probability distribution over the vocabulary?,Softmax followed by unembedding matrix,Unembedding matrix followed by softmax,Concatenation of vectors followed by softmax,Element-wise multiplication followed by unembedding matrix,B) Unembedding matrix followed by softmax
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What is the primary purpose of the unembedding matrix in the language modeling head?,To map from a [1 x d] vector to a [1 x |V|] vector of logits,To map from a [1 x |V|] vector of logits to a [1 x |V|] vector of probabilities,To generate a random probability distribution over the vocabulary,To perform a dimensionality reduction of the input vector,A) To map from a [1 x d] vector to a [1 x |V|] vector of logits
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What is the shape of the unembedding matrix ET in the language modeling head?,[|V| × d],[d × |V|],[|V| × |V|],[d × d],B) [d × |V|]
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What is the relationship between the unembedding matrix and the embedding matrix E in the language modeling head?,They are the same matrix,The unembedding matrix is the transpose of the embedding matrix E,"The embedding matrix E is used for unembedding, not the other way around",The unembedding matrix is a separate matrix from the embedding matrix E,B) The unembedding matrix is the transpose of the embedding matrix E
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57","In the language modeling head, what is the function of the softmax operation?",To map from a [1 x |V|] vector of logits to a [1 x d] vector,To map from a [1 x d] vector to a [1 x |V|] vector of logits,To map from a [1 x |V|] vector of logits to a [1 x |V|] vector of probabilities,To perform a dimensionality reduction of the input vector,C) To map from a [1 x |V|] vector of logits to a [1 x |V|] vector of probabilities
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What is the shape of the output vector of the language modeling head after applying softmax?,[1 x d],[1 x |V|],[d x |V|],[|V| x d],B) [1 x |V|]
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the purpose of the unembedding layer in the language modeling head?,To project from the output embedding to a vector of logit scores for each word in the vocabulary,To map from a one-hot vector to an embedding,To perform a weighted sum of the input tokens,To apply a ReLU activation function to the output embedding,A) To project from the output embedding to a vector of logit scores for each word in the vocabulary
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",Why is the unembedding layer sometimes called the unembedding layer?,Because it is performing a reverse mapping,Because it is tied to the embedding matrix,Because it is used to compute the logit scores,Because it is used to apply a softmax activation function,A) Because it is performing a reverse mapping
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the shape of the logit vector produced by the unembedding layer?,[1 × d],[d × 1],[1 × |V|],[|V| × d],C) [1 × |V|]
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the purpose of the softmax layer in the language modeling head?,To compute the logit scores for each word in the vocabulary,To apply a weighted sum to the input tokens,To turn the logit scores into probabilities over the vocabulary,To apply a ReLU activation function to the output embedding,C) To turn the logit scores into probabilities over the vocabulary
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59","What is weight tying, and how is it used in the unembedding layer?",Weight tying is the practice of using the same weights for two different matrices in the model,Weight tying is the practice of using different weights for two different matrices in the model,Weight tying is not used in the unembedding layer,Weight tying is used to optimize the embedding matrix,A) Weight tying is the practice of using the same weights for two different matrices in the model
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the primary purpose of the language modeling head in a transformer model?,To predict the sentiment of a given text,To generate a probability distribution over words in the vocabulary,To classify a given text as spam or not spam,To translate a given text from one language to another,B) To generate a probability distribution over words in the vocabulary
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the role of the unembedding layer in the language modeling head?,To project the output embedding from the final transformer layer to the logit vector,To perform the reverse mapping of an embedding to a vector over the vocabulary,To normalize the output of the final transformer layer,To perform attention over the input sequence,B) To perform the reverse mapping of an embedding to a vector over the vocabulary
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the effect of using weight tying in the language modeling head?,The embedding matrix and the logit vector are optimized separately,The embedding matrix and the logit vector are optimized together,The embedding matrix is fixed and the logit vector is optimized,The logit vector is fixed and the embedding matrix is optimized,B) The embedding matrix and the logit vector are optimized together
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the output shape of the softmax layer in the language modeling head?,1 x d,1 x |V|,d x |V|,|V| x d,B) 1 x |V|
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the purpose of the softmax layer in the language modeling head?,To normalize the output of the final transformer layer,To project the output embedding from the final transformer layer to the logit vector,To turn the logits into probabilities over the vocabulary,To perform attention over the input sequence,C) To turn the logits into probabilities over the vocabulary
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",How does the language modeling head take the output of the final transformer layer and use it to predict the upcoming word?,It uses a linear layer to project the output embedding to the logit vector,It uses a softmax layer to turn the logits into probabilities over the vocabulary,It uses a feedforward layer to normalize the output of the final transformer layer,It uses an attention layer to perform attention over the input sequence,A) It uses a linear layer to project the output embedding to the logit vector
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",What is the fundamental approach used to develop large language models?,First fine-tune a pre-trained model on specific tasks and then apply it to new tasks,First pretrain a transformer model on enormous amounts of text and then apply it to new tasks,First use a pre-trained model as a starting point and then add custom layers for new tasks,First collect data from specific tasks and then train a model from scratch,B) First pretrain a transformer model on enormous amounts of text and then apply it to new tasks
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",What is the primary function of the pretraining stage in developing large language models?,To fine-tune the model for specific tasks,To apply the model to new tasks directly,To pretrain a transformer model on enormous amounts of text,To collect data from specific tasks,C) To pretrain a transformer model on enormous amounts of text
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",What is the key idea that underlies the amazing performance of language models?,Using a pre-trained model as a starting point for new tasks,Collecting data from specific tasks and training a model from scratch,First pretrain a transformer model on enormous amounts of text and then apply it to new tasks,Fine-tuning a pre-trained model on specific tasks and then applying it to new tasks,C) First pretrain a transformer model on enormous amounts of text and then apply it to new tasks
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the primary goal of the self-supervised training algorithm for pretraining a transformer model?,To predict the next word in a sequence of text,To classify the sentiment of a given text,To generate a coherent piece of text from a prompt,To translate a sentence from one language to another,A) To predict the next word in a sequence of text
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is unique about the training label used in the self-supervised training algorithm?,It is a human-annotated label,It is a random word from the vocabulary,It is the next word in the sequence,It is a word that is not in the vocabulary,C) It is the next word in the sequence
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the output of the pretraining process for a language model?,A probability distribution over the vocabulary of possible words,A single word that is most likely to come next,A sentence that is most likely to be completed,A translation of a sentence from one language to another,A) A probability distribution over the vocabulary of possible words
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the purpose of the language modeling head in the transformer architecture?,To generate a coherent piece of text from a prompt,To predict the next word in a sequence of text,To classify the sentiment of a given text,To translate a sentence from one language to another,B) To predict the next word in a sequence of text
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the loss function used to train the pretraining model?,Cross-entropy loss,Mean squared error,How high is this probability,Perplexity,C) How high is this probability
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",What type of loss function is used in language modeling to measure the difference between the model's predicted probability distribution and the true word?,Mean Squared Error,Cross-Entropy Loss,Mean Absolute Error,Binary Cross-Entropy Loss,B) Cross-Entropy Loss
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67","What is the effect of a high loss value in language modeling, and how does the model respond to it?","The model assigns a lower probability to the true word, and the loss increases further.","The model assigns a higher probability to the true word, and the loss decreases.",The model moves its weights to give a higher probability to the true word.,The model ignores the true word and continues with its previous prediction.,C) The model moves its weights to give a higher probability to the true word.
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",What is the purpose of teacher forcing in language modeling?,To allow the model to predict the next token based on its previous predictions.,To ignore the model's predictions and use the correct word at each token position.,To assign a high probability to the true word and a low probability to other words.,To use a different loss function for language modeling.,B) To ignore the model's predictions and use the correct word at each token position.
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",What is the formula for the Cross-Entropy Loss (CE Loss) for a single word?,-log p(wi),-log p(wi) + log(1-p(wi)),-log p(wi) - log(1-p(wi)),-log p(wi) + log(p(wi)),A) -log p(wi)
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",What is the formula for the Cross-Entropy Loss (CE Loss) for a whole sentence?,-log p(w1) - log p(w2) -... - log p(wn),-log p(w1) + log p(w2) +... + log p(wn),∑#$! -log p(wi),-log p(w1) - log p(w2) +... - log p(wn),C) ∑#$! -log p(wi)
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",What is the process that model sees correct tokens and computes loss for the next token at each token position?,Training a transformer language model,Teacher forcing,StackedTransformerBlocks,LanguageModelingHead,B) Teacher forcing
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69","What is the dataset used to train LLMs, which contains 156 billion tokens of English?",Common Crawl,Colossal Clean Crawled Corpus (C4),Webpage snapshots,Patent text documents,B) Colossal Clean Crawled Corpus (C4)
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",What is removed from the Colossal Clean Crawled Corpus (C4) dataset?,Patent text documents,Wikipedia articles,"Boilerplate, adult content, toxicity",News sites,"C) Boilerplate, adult content, toxicity"
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",What is the process of taking the correct word and adding it to the context at the next token position?,Teacher forcing,Training a transformer language model,StackedTransformerBlocks,Ignoring the model's prediction,A) Teacher forcing
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",What is the name of the non-profit that produces snapshots of the entire web?,Common Crawl,Colossal Clean Crawled Corpus (C4),Webpage snapshots,LanguageModelingHead,A) Common Crawl
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",What is the type of content mostly present in the Colossal Clean Crawled Corpus (C4) dataset?,Patent text documents,Wikipedia articles,News sites,"Boilerplate, adult content, toxicity",A) Patent text documents
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What is a primary source of data used for pretraining LLMs?,Common Crawl web snapshots,Wikipedia and news sites only,Patent text documents and filtered web data,User-generated content on social media,A) Common Crawl web snapshots
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What is a notable characteristic of the Colossal Clean Crawled Corpus (C4)?,It contains mostly news articles,It is filtered to remove boilerplate content,It has 156 billion tokens of English,It is only used for fine-tuning LLMs,C) It has 156 billion tokens of English
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What is a consequence of pretraining LLMs on a large corpus of text?,The model is limited to a specific domain,The model has limited knowledge,The model gains the ability to perform many tasks,The model is less accurate than other models,C) The model gains the ability to perform many tasks
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What is removed from the Colossal Clean Crawled Corpus (C4) during filtering?,Patent text documents,Wikipedia articles,Boilerplate content and adult content,News articles,C) Boilerplate content and adult content
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",What is a major issue with using web-scraped text for pretraining large language models?,Lack of domain-specific knowledge,Copyright infringement and unclear fair use doctrine,Insufficient data for model training,Inadequate computational resources,B) Copyright infringement and unclear fair use doctrine
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Why is data consent a concern when using web-scraped text for pretraining large language models?,Website owners may not want their site crawled due to security concerns,Websites can contain private IP addresses and phone numbers,Data scraping may violate website terms of service,Data scraping may compromise website performance,B) Websites can contain private IP addresses and phone numbers
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",What is a potential consequence of using copyrighted text for pretraining large language models?,Loss of model accuracy,Copyright infringement lawsuits,Model training time reduction,Improved model interpretability,B) Copyright infringement lawsuits
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",What is a benefit of pretraining large language models on lots of text?,Improved model sparsity,Increased model robustness,Ability to perform many tasks,Reduced model training time,C) Ability to perform many tasks
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75","What is the name of the method where a subset of parameters of a large language model are updated when fine-tuning, and the rest are left unchanged from their pre-trained values?",Continued Pre-training,Parameter-Efficient Fine-tuning (PEFT),Supervised Fine-tuning (SFT),Retraining all the parameters of the model,B) Parameter-Efficient Fine-tuning (PEFT)
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",What is the name of the loss function used to train a language model to produce the desired response from a command in the prompt during supervised fine-tuning?,Cross-Entropy Loss,Mean Squared Error,Perplexity,Maximum Likelihood Estimation,A) Cross-Entropy Loss
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",What is the purpose of the classification head added after the top layer of a language model during fine-tuning for a specific task?,To predict the next word in the sequence,To classify the input text into a specific category,To generate a response to a prompt,To update the model's parameters,B) To classify the input text into a specific category
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75","What is the inverse probability that a model assigns to the test set, normalized by the test set length, also known as the measure of how well the model predicts unseen text?",Perplexity,Cross-Entropy,Accuracy,Precision,A) Perplexity
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",Why is retraining all the parameters of a large language model slow and expensive?,Because it requires a lot of computational resources,Because the language model is huge,Because it requires a lot of labeled data,Because it is a complex process,B) Because the language model is huge
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",Why is perplexity a suitable metric for evaluating language models?,It measures the model's ability to assign higher probabilities to unseen data,It is a direct measure of the model's accuracy on a test set,It is insensitive to the length of the test set,It is equivalent to maximizing the probability of the test set,A) It measures the model's ability to assign higher probabilities to unseen data
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",What is the effect of increasing the length of a test set on the probability of a language model?,The probability increases,The probability decreases,The probability remains the same,The probability is unaffected,B) The probability decreases
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",What is the relationship between minimizing perplexity and maximizing probability?,Minimizing perplexity is equivalent to minimizing probability,Minimizing perplexity is equivalent to maximizing probability,Minimizing perplexity is equivalent to minimizing the length of the test set,Minimizing perplexity is equivalent to maximizing the length of the test set,B) Minimizing perplexity is equivalent to maximizing probability
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",Why is perplexity sensitive to tokenization?,Because it measures the model's ability to assign higher probabilities to unseen data,Because it is a direct measure of the model's accuracy on a test set,Because it is sensitive to the length of the test set,Because it is sensitive to the tokenizer used,D) Because it is sensitive to the tokenizer used
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",What is the range of values for probability and perplexity?,"Probability: [0,1], Perplexity: [0,∞]","Probability: [0,1], Perplexity: [1,∞]","Probability: [1,∞], Perplexity: [0,1]","Probability: [1,∞], Perplexity: [1,∞]","B) Probability: [0,1], Perplexity: [1,∞]"
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",What is true about perplexity in the context of language models?,"The higher the perplexity of a model on the data, the better the model.",Minimizing perplexity is the same as maximizing probability.,Perplexity is not sensitive to length/tokenization.,Perplexity is always higher for smaller models.,B) Minimizing perplexity is the same as maximizing probability.
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Which of the following factors is NOT directly related to the evaluation of language models?,Energy usage,Size,Fairness,User experience,D) User experience
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",What is a potential drawback of using perplexity as a metric for comparing language models?,It is not sensitive to length/tokenization.,It is not a good indicator of model performance.,It is not a widely used metric.,It can be misleading when comparing models with different tokenizers.,D) It can be misleading when comparing models with different tokenizers.
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",What is a potential harm of large language models?,They are too small to be effective.,They are not energy-efficient.,They can perpetuate biases and stereotypes.,They are too slow to be useful.,C) They can perpetuate biases and stereotypes.
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",Which of the following is a characteristic of hallucination in Large Language Models?,The model generates information that is not present in the training data but is plausible,The model generates information that is not present in the training data but is impossible,The model generates information that is present in the training data but is irrelevant,The model generates information that is present in the training data but is outdated,A) The model generates information that is not present in the training data but is plausible
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",What is the primary concern regarding copyright in the context of Large Language Models?,The models may infringe on existing copyrights by generating similar content,The models may not be able to generate content that is copyrightable,The models may require copyright licenses to operate,The models may generate content that is in the public domain,A) The models may infringe on existing copyrights by generating similar content
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",What is a common concern related to the online publication of copyrighted material?,It is not a concern as it is allowed under fair use,It may infringe on the rights of the copyright holder,It is only a concern for physical copies,It is not a concern as it is a public domain work,B) It may infringe on the rights of the copyright holder
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",How might the university address potential online toxicity and abuse?,By ignoring it and allowing users to regulate themselves,By implementing a robust reporting system and taking disciplinary action,By removing the online platform altogether,By requiring users to agree to a strict code of conduct,B) By implementing a robust reporting system and taking disciplinary action
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",What is a key aspect of maintaining user privacy online?,Collecting and storing user data for targeted advertising,Ensuring that user data is anonymized and secure,Sharing user data with third-party vendors,Requiring users to provide personal information upfront,B) Ensuring that user data is anonymized and secure
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",Which of the following is NOT a type of harm associated with Large Language Models?,Misinformation,Phishing,Abuse,Bias in decision-making processes,D) Bias in decision-making processes
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",What is one way in which Large Language Models can be used to commit fraud?,Generating fake customer reviews,Creating phishing emails,Both A and B,Neither A nor B,C) Both A and B
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",Which of the following is an example of misinformation that can be spread through Large Language Models?,Spreading fake news about a company's financial status,Sharing conspiracy theories,Posting fake product reviews,All of the above,D) All of the above
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",How can Large Language Models be used in phishing attacks?,Generating fake login pages,Creating convincing social engineering messages,Both A and B,Neither A nor B,C) Both A and B
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",What is a potential consequence of a Large Language Model being used to spread misinformation?,Loss of public trust in the model's creators,Financial losses for individuals or companies,Both A and B,Neither A nor B,C) Both A and B
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87",What is the primary goal of the backward computation in neural network training?,To compute the loss between the true and estimated outputs,To update the network weights to decrease the loss,To run the forward computation for the next training tuple,To assess how much blame each weight deserves for the current loss,B) To update the network weights to decrease the loss
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87",What is the purpose of computing the partial derivatives using the chain rule in the backward computation?,To update the network weights directly,To compute the loss between the true and estimated outputs,To assess how much blame each weight deserves for the current loss,To determine how much each weight is responsible for the current loss,D) To determine how much each weight is responsible for the current loss
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87",What is the formula for updating a weight w in the network?,wnew = w - η!,wnew = w + η!,wnew = w - η!,wnew = w - η,C) wnew = w - η!
notes/LLM_cs124_week7_2025.pdf,43,"85, 86, 87",What is the value of η used for updating the weights in the network?,0.01,0.1,1,0.0001,A) 0.01
notes/LLM_cs124_week7_2025.pdf,44,"87, 88, 89","What is the initial loss L for the example (x1, x2, ytrue) = (4, -3, 10) in the simple 1-layer network with weights w1 = 2, w2 = -1, and b = 1?",L = (10 - 12)^2,L = (10 - 11)^2,L = (10 - 13)^2,L = (10 - 14)^2,A) L = (10 - 12)^2
notes/LLM_cs124_week7_2025.pdf,44,"87, 88, 89","What is the partial derivative of the loss L with respect to w1 for the example (x1, x2, ytrue) = (4, -3, 10) in the simple 1-layer network with weights w1 = 2, w2 = -1, and b = 1?",!L/!w1 = 2 * (10 - 12),!L/!w1 = 2 * (10 - 11),!L/!w1 = 2 * (10 - 13),!L/!w1 = 2 * (10 - 14),A) !L/!w1 = 2 * (10 - 12)
notes/LLM_cs124_week7_2025.pdf,44,"87, 88, 89","How do you update the weight w1 to decrease the loss L for the example (x1, x2, ytrue) = (4, -3, 10) in the simple 1-layer network with weights w1 = 2, w2 = -1, and b = 1?",w1_new = w1 - 0.01 * 2 * (10 - 12),w1_new = w1 - 0.01 * 2 * (10 - 11),w1_new = w1 - 0.01 * 2 * (10 - 13),w1_new = w1 - 0.01 * 2 * (10 - 14),A) w1_new = w1 - 0.01 * 2 * (10 - 12)
notes/LLM_cs124_week7_2025.pdf,44,"87, 88, 89","What will be the new estimate y after updating the weight w1 for the example (x1, x2, ytrue) = (4, -3, 10) in the simple 1-layer network with initial weights w1 = 2, w2 = -1, and b = 1?",y_new = 1.98 * 4 - 1 * -3 + 1,y_new = 1.99 * 4 - 1 * -3 + 1,y_new = 2 * 4 - 1 * -3 + 1,y_new = 1.99 * 4 - 1 * -3 + 1.01,A) y_new = 1.98 * 4 - 1 * -3 + 1
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","What is the value of y computed during the forward pass with inputs (x1, x2) = (4, -3) and initial weights w1 = 2, w2 = -1, b = 1?",8,12,16,20,B) 12
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91",What is the value of L computed during the loss calculation with y_true = 10 and y = 12?,0,4,16,36,B) 4
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91",Which of the following is a key node in the computation graph that we need gradients to update?,Intermediate variables,Parameters of the model,Model architecture,Training data,B) Parameters of the model
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93",Why are h1 and h2 created as intermediate nodes in the computation graph?,To simplify the backpropagation process,To reduce the number of nodes in the graph,Because x1 and x2 are not needed for weight updates,To increase the accuracy of the model,C) Because x1 and x2 are not needed for weight updates
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93",What are the key nodes in the computation graph that we need gradients to update?,"w1, w2, b, and L","w1, w2, b, and y","w1, w2, b, and x1","h1, h2, and L","A) w1, w2, b, and L"
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93","What is the purpose of creating nodes for w1, w2, and b in the computation graph?",To calculate the loss function,To update the weights during backpropagation,To create intermediate variables for weight updates,To reduce the number of nodes in the graph,C) To create intermediate variables for weight updates
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93",What is the correct expression for h2 in the computation graph?,h2 = w1x2,h2 = w2x2,h2 = w1x1 + w2x2,h2 = w2x1 + b,B) h2 = w2x2
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95",What is the equation for the output of the hidden layer (h1) in the context of backpropagation?,h1 = w1x1 + h2 + b,h1 = w1x1 + h2 - b,h1 = w1x1 + h2,h1 = w1x1 - h2,A) h1 = w1x1 + h2 + b
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95",What is the equation for the loss (L) in the context of backpropagation?,L = (ytrue - y)^2 * y,L = (ytrue - y)^2,L = (ytrue + y)^2,L = (ytrue - y)^2 * -1,B) L = (ytrue - y)^2
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95",What is the equation for the gradient of the loss with respect to the bias term (b) in the context of backpropagation?,2(ytrue - y) * -1,2(ytrue - y) * 1,2(ytrue + y) * -1,2(ytrue + y) * 1,A) 2(ytrue - y) * -1
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95",What is the equation for the gradient of the output of the hidden layer (h1) with respect to the input (x1) in the context of backpropagation?,2(ytrue - y) * -1 * x1,2(ytrue - y) * 1 * x1,2(ytrue + y) * -1 * x1,2(ytrue + y) * 1 * x1,A) 2(ytrue - y) * -1 * x1
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95",What is the equation for the output of the output layer (y) in the context of backpropagation?,y = h1 + h2 + b,y = h1 + h2 - b,y = h1 + h2,y = h1 - h2,A) y = h1 + h2 + b
notes/LLM_cs124_week7_2025.pdf,47,"93, 94, 95",What is the equation for the output of the hidden layer (h2) in the context of backpropagation?,h2 = w2x2 + h1 + b,h2 = w2x2 + h1 - b,h2 = w2x2 + h1,h2 = w2x2 - h1,A) h2 = w2x2 + h1 + b
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97",What is the formula to compute the loss gradient for the weight w1?,2(ytrue-y) * -1,2(ytrue-y) * 1,2(ytrue-y) * -1 * w1,2(ytrue-y) * w1,A) 2(ytrue-y) * -1
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97",What is the value of y in the given example?,12,10,4,-3,A) 12
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97",What is the value of w2 in the given example?,2,-1,1,4,B) -1
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97",What is the formula to compute the value of h1?,w1x1 + b,w2x2 + b,h1 + h2 + b,w1x1 + w2x2 + b,A) w1x1 + b
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97",What is the value of x1 in the given example?,4,-3,1,2,A) 4
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97",What is the value of L in the given example?,(ytrue-y)^2,2(ytrue-y)^2,2(ytrue-y),(ytrue-y),B) 2(ytrue-y)^2
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97",What is the formula to compute the value of ytrue?,h1 + h2 + b,h1 + h2,h1 + h2 + b + L,h1 + h2 + L,A) h1 + h2 + b
notes/LLM_cs124_week7_2025.pdf,48,"95, 96, 97",What is the value of b in the given example?,1,2,-1,4,A) 1
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","If y_true = 10 and y = 12, what is the value of L in the equation L = (y_true - y)^2?",4,16,36,64,B) 16
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99",What is the value of ∂L/∂w1?,4,16,-16,24,B) 16
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","If η = 0.01, what is the new value of w1?",1.84,2,0.84,1.96,A) 1.84
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99",What is the value of ∂L/∂b?,4,-4,-12,16,A) 4
notes/LLM_cs124_week7_2025.pdf,49,"97, 98, 99","If w2 = -1 and η = 0.01, what is the new value of w2?",-0.88,-1.01,-1.12,-0.98,A) -0.88
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","Given the equation for computing new weights, what is the formula for updating the bias term b?",b = b + η(),b = b - η(),b = b + η()(*,b = b - η(),B) b = b - η()
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","In the given backpropagation example, what is the value of the learning rate η?",0.1,0.01,0.001,0.0001,B) 0.01
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","After updating the weights w1, w2, and bias b, the result of the forward pass should be closer to which value of y?",ytrue = 5,ytrue = 10,ytrue = 15,ytrue = 20,B) ytrue = 10
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101",What is the value of the partial derivative of the loss function with respect to w2?,4,-3,-12,16,C) -12
notes/LLM_cs124_week7_2025.pdf,50,"99, 100, 101","Given the initial values of w1, w2, and b, what is the new value of w1 after updating it using backpropagation?",2 - 0.01 * 4,2 - 0.01 * 16,2 + 0.01 * 16,2 + 0.01 * 4,B) 2 - 0.01 * 16
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the purpose of using multiple attention heads in a self-attention layer?,To reduce the dimensionality of the input and output embeddings,To enable each head to attend to the context for different purposes,To increase the computational complexity of the self-attention step,To eliminate the need for masking in the self-attention computation,B) To enable each head to attend to the context for different purposes
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105","What is the shape of the matrices Q, K, and V produced by multiplying X by the key, query, and value matrices WQ, WK, and WV?","Q: [N x dk], K: [N x dk], V: [N x dv]","Q: [d x N], K: [d x N], V: [d x N]","Q: [N x d], K: [N x d], V: [N x d]","Q: [N x dv], K: [N x dv], V: [N x dv]","A) Q: [N x dk], K: [N x dk], V: [N x dv]"
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the purpose of masking the upper-triangular portion of the QK| matrix in the self-attention computation?,To enable the model to attend to words that follow in the sequence,To eliminate any knowledge of words that follow in the sequence,To reduce the dimensionality of the input and output embeddings,To increase the computational complexity of the self-attention step,B) To eliminate any knowledge of words that follow in the sequence
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the shape of the output matrix a produced by the self-attention step for a single head?,[N x d],[N x dv],[d x N],[N x hdv],B) [N x dv]
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",How can the self-attention step for an entire sequence of N tokens for one head be parallelized using a single matrix X?,By computing a single output at a single time step i in a single residual stream,"By multiplying X by the key, query, and value matrices WQ, WK, and WV to produce matrices Q, K, and V",By using a mask function to eliminate any knowledge of words that follow in the sequence,By using multiple attention heads to attend to the context for different purposes,"B) By multiplying X by the key, query, and value matrices WQ, WK, and WV to produce matrices Q, K, and V"
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107","What is the shape of the matrices Q, K, and V produced by multiplying X by WQ, WK, and WV respectively?","Q of shape [N × d], K of shape [N × d], V of shape [N × d]","Q of shape [N × dk], K of shape [N × dk], V of shape [N × dv]","Q of shape [N × d], K of shape [N × dv], V of shape [N × dk]","Q of shape [N × dv], K of shape [N × dk], V of shape [N × d]","B) Q of shape [N × dk], K of shape [N × dk], V of shape [N × dv]"
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What is the effect of the mask function in the self-attention computation?,It eliminates any knowledge of words that precede the query,It eliminates any knowledge of words that follow the query,"It scales the scores, takes the softmax, and then multiplies the result by V",It concatenates the matrices of shape N × dv,B) It eliminates any knowledge of words that follow the query
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What is the output shape of the multi-head attention layer with A heads?,A matrix of shape N × d,A matrix of shape N × dv,A matrix of shape N × hdv,A matrix of shape N × Adv,C) A matrix of shape N × hdv
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What is the purpose of the final linear projection WO in the multi-head attention layer?,"To scale the scores, take the softmax, and then multiply the result by V",To reshape the output to the original output dimension for each token,To concatenate the matrices of shape N × dv,To eliminate any knowledge of words that follow the query,B) To reshape the output to the original output dimension for each token
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What is the purpose of the mask function in the self-attention computation?,To allow the model to consider the future words in the sequence,To eliminate any knowledge of words that follow in the sequence,To introduce a penalty for incorrect predictions,To add noise to the input tokens,B) To eliminate any knowledge of words that follow in the sequence
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What is the shape of the weight layer WQ in multi-head attention?,d x dk,dk x d,d x dk,dk x d,A) d x dk
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",How many matrices of shape N x dv are produced by the output of each of the A heads in multi-head attention?,A,N,A-1,1,A) A
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What is the dimensionality of the concatenated matrix output in multi-head attention?,N x hdv,N x d,N x dk,N x dv,A) N x hdv
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What is the shape of the final linear projection WO in multi-head attention?,Adv x d,d x Adv,dv x d,d x dv,A) Adv x d
notes/LLM_cs124_week7_2025.pdf,56,"111, 112, 113",What is the purpose of the embedding matrix E in the transformer architecture?,To store the input token embeddings and positional embeddings,To store the set of initial embeddings for each token in the vocabulary,To compute the self-attention output A of shape [N⇥d],To apply the same FFN to each of the N embedding vectors in the window,B) To store the set of initial embeddings for each token in the vocabulary
notes/LLM_cs124_week7_2025.pdf,56,"111, 112, 113",How are token embeddings selected from the embedding matrix E?,By multiplying the embedding matrix E by a one-hot vector of shape [1⇥|V|],By using indexing to select the corresponding rows from E,By applying the softmax function to the product of Qi and Ki,By using the LayerNorm function to normalize the input X,B) By using indexing to select the corresponding rows from E
notes/LLM_cs124_week7_2025.pdf,56,"111, 112, 113",What is the shape of the input matrix X in the transformer architecture?,[N⇥d],[|V|⇥d],[1⇥d],[N⇥|V|],A) [N⇥d]
notes/LLM_cs124_week7_2025.pdf,56,"111, 112, 113",What is the role of the positional embedding in the transformer architecture?,To represent the initial embedding for each token in the vocabulary,To incorporate context and play a different role depending on the kind of language model,To normalize the input X and apply the same FFN to each of the N embedding vectors,"To separately compute two embeddings: an input token embedding, and an input positional embedding","D) To separately compute two embeddings: an input token embedding, and an input positional embedding"
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",What is the shape of the matrix X that represents the input to a transformer block?,N x d,|V| x d,N x |V|,d x N,A) N x d
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",How are token embeddings selected from the embedding matrix E?,Using one-hot vectors to select the relevant row vector,Using a linear combination of all row vectors,Using a random selection of row vectors,Using a fixed embedding for all tokens,A) Using one-hot vectors to select the relevant row vector
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",What is the role of the embedding matrix E in the transformer architecture?,To store the output of the transformer block,To store the input token embeddings,To store the positional embeddings,To store the vocabulary of words,B) To store the input token embeddings
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",How many dimensions does a token embedding have?,d,|V|,N,d + |V| + N,A) d
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",What is the shape of the output H of a transformer block?,N x d,|V| x d,N x |V|,d x N,A) N x d
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",What is the purpose of the LayerNorm operation in a transformer block?,To normalize the input X,To normalize the output H,To normalize the input token embeddings,To normalize the positional embeddings,B) To normalize the output H
