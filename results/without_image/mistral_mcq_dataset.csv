pdf_name,chunk_number,pages,question,option_A,option_B,option_C,option_D,correct_answer
LLM_cs124_week7_2025.pdf,1,"1, 2, 3",What is the main goal of large language models?,To assign probabilities to sequences of words,To generate text by sampling possible next words,To be trained only on specific words,To learn a little language knowledge,A) To assign probabilities to sequences of words
LLM_cs124_week7_2025.pdf,1,"1, 2, 3",How does a large language model generate text?,By assigning probabilities to sequences of words and sampling the next word,By being trained on specific text only,By learning to predict the next character instead of the next word,By being a neural network layer,A) By assigning probabilities to sequences of words and sampling the next word
LLM_cs124_week7_2025.pdf,1,"1, 2, 3",What does a large language model learn during training?,To predict the next character in a sequence,To assign probabilities to sequences of words,To learn a lot of language knowledge from text,To generate text by itself,C) To learn a lot of language knowledge from text
LLM_cs124_week7_2025.pdf,1,"1, 2, 3",What is the difference between a simple n-gram language model and a large language model?,"A large language model assigns probabilities to sequences of words and generates text by sampling possible next words, while a simple n-gram model only assigns probabilities to sequences of words","A large language model learns a lot of language knowledge from text, while a simple n-gram model is trained on counts computed from lots of text","A large language model generates text by sampling possible next characters, while a simple n-gram model generates text by sampling possible next words","A large language model is a neural network layer, while a simple n-gram model is an optimization algorithm","A) A large language model assigns probabilities to sequences of words and generates text by sampling possible next words, while a simple n-gram model only assigns probabilities to sequences of words"
LLM_cs124_week7_2025.pdf,2,"3, 4, 5",What are decoder-only models also called?,Right-to-left LLMs,Encoder-decoder models,Autoregressive LLMs,Causal LLMs,C) Autoregressive LLMs
LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which type of language model gets bidirectional context?,Decoder-only models,Encoder-only models,Encoder-decoder models,Both A and B,B) Encoder-only models
LLM_cs124_week7_2025.pdf,2,"3, 4, 5",How does a decoder-only model generate text?,Predicting words in both directions,Predicting words based on bidirectional context,Predicting words left to right,Predicting words based on future words,C) Predicting words left to right
LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which type of language model can condition on future words?,Decoder-only models,Encoder-only models,Encoder-decoder models,Both A and B,B) Encoder-only models
LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which model type is used for generating text from left to right?,Decoder-only model,Encoder-only model,Encoder-decoder model,Transformer model,A) Decoder-only model
LLM_cs124_week7_2025.pdf,3,"5, 6, 7",What is the primary difference between decoder-only and encoder-only models?,"Decoder-only models generate text left to right, encoder-only models generate text right to left","Decoder-only models get bidirectional context, encoder-only models don't","Decoder-only models are used for machine translation, encoder-only models are used for speech recognition","Decoder-only models are trained on supervised data, encoder-only models are pretrained","A) Decoder-only models generate text left to right, encoder-only models generate text right to left"
LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which model type is used for machine translation?,Decoder-only model,Encoder-only model,Encoder-decoder model,Transformer model,C) Encoder-decoder model
LLM_cs124_week7_2025.pdf,3,"5, 6, 7",What is the primary use case for encoder-decoder models?,Generating text left to right,Generating text right to left,"Mapping from one sequence to another, such as machine translation",Training on supervised data for classification tasks,"C) Mapping from one sequence to another, such as machine translation"
LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What are decoder-only models used for in language processing?,Generating responses based on given context,Creating bidirectional context for encoding,Predicting words in a sequence without conditioning on future words,Training encoders to build strong representations,A) Generating responses based on given context
LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Which type of language model can't condition on future words?,Decoder-only models,Encoder-only models,Bidirectional models,Transformer models,C) Bidirectional models
LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What is the main difference between encoder-only and decoder-only models?,Encoder-only models can't condition on future words,Decoder-only models can't build strong representations,Encoder-only models get bidirectional context,Decoder-only models generate responses based on given context,B) Decoder-only models can't build strong representations
LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Which type of language model is used for generating responses based on given context?,Encoder-only models,Decoder-only models,Bidirectional models,Transformer models,B) Decoder-only models
LLM_cs124_week7_2025.pdf,5,"9, 10, 11",Which type of language model generates words left-to-right?,Decoder-only model,Encoder-only model,Transformer model,Recurrent neural network model,A) Decoder-only model
LLM_cs124_week7_2025.pdf,5,"9, 10, 11",What is the process called when a language model generates a word based on the context of previous words?,Prefix text completion,Question answering,Sentiment analysis,Word prediction,D) Word prediction
LLM_cs124_week7_2025.pdf,5,"9, 10, 11",Which token is used to indicate that a summary should come next in text summarization?,tl;dr,Q: ,A: ,P(w|,A) tl;dr
LLM_cs124_week7_2025.pdf,5,"9, 10, 11",Which type of language model can condition on future words?,Decoder-only model,Encoder-only model,Transformer model,Recurrent neural network model,B) Encoder-only model
LLM_cs124_week7_2025.pdf,6,"11, 12, 13",Which token is used to indicate a short summary in text summarization?,tl;dr,softmax,encoder,transformer,A) tl;dr
LLM_cs124_week7_2025.pdf,6,"11, 12, 13",Which method is used to determine the sentiment of a sentence?,Conditional generation,Prefix text completion,Language modeling,Transformer blocks,C) Language modeling
LLM_cs124_week7_2025.pdf,6,"11, 12, 13",Which neural network architecture is used in modern language models?,Convolutional Neural Networks (CNNs),Recurrent Neural Networks (RNNs),Transformers,Long Short-Term Memory (LSTM),C) Transformers
LLM_cs124_week7_2025.pdf,7,"13, 14, 15",What are large language models built from?,Linear regression models,Decision trees,Stacks of neural networks called transformers,Logistic regression models,C) Stacks of neural networks called transformers
LLM_cs124_week7_2025.pdf,7,"13, 14, 15",What are transformers used in?,Image recognition,Speech recognition,Text generation and language modeling,Sentiment analysis,C) Text generation and language modeling
LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Who proposed the idea of replacing RNNs with self-attention in the Transformer architecture?,Ashish Vaswani,Noam Shazeer,Jakob Uszkoreit,Niki Parmar,C) Jakob Uszkoreit
LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Which year was the Transformer architecture proposed?,2012,2015,2017,2018,C) 2017
LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Which researchers were involved in the original Transformer model implementation?,Ashish Vaswani and Illia Polosukhin,Noam Shazeer and Niki Parmar,Jakob Uszkoreit and Llion Jones,Lukasz Kaiser and Aidan Gomez,A) Ashish Vaswani and Illia Polosukhin
LLM_cs124_week7_2025.pdf,8,"15, 16, 17",In which conference was the Transformer architecture paper first presented?,31st Conference on Neural Information Processing Systems (NIPS 2014),31st Conference on Neural Information Processing Systems (NIPS 2016),31st Conference on Neural Information Processing Systems (NIPS 2017),31st Conference on Neural Information Processing Systems (NIPS 2018),C) 31st Conference on Neural Information Processing Systems (NIPS 2017)
LLM_cs124_week7_2025.pdf,9,"17, 18, 19",When were Static Word Embeddings first introduced?,1990,2003,2013,2015,A) 1990
LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which technology began to be used around 2004-2006?,Multi-Task Learning,GPUs,Transformers,Attention,B) GPUs
LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which concept was re-discovered in 2013?,Static Word Embeddings,Neural Language Model,Transformers,Contextual Word Embeddings,A) Static Word Embeddings
LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which technology took off around 2012?,GPUs,Multi-Task Learning,Transformers,Attention,B) Multi-Task Learning
LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which concept was introduced in 2015?,Attention,Transformer,Contextual Word Embeddings,Pretraining,C) Contextual Word Embeddings
LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which concept was introduced in 2017?,Transformer,Contextual Word Embeddings,Pretraining,Prompting,A) Transformer
LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which concept was introduced in 2018?,Contextual Word Embeddings,Pretraining,Prompting,ChatGPT,C) Prompting
LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which concept was introduced in 2019?,Prompting,ChatGPT,Transformers,Contextual Word Embeddings,A) Prompting
LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which technology is used in Transformers?,GPUs,Multi-Task Learning,Attention,Contextual Word Embeddings,C) Attention
LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which concept is used in Language Modeling Head?,Attention,Transformer,Contextual Word Embeddings,Pretraining,A) Attention
LLM_cs124_week7_2025.pdf,10,"19, 20, 21",Which representation of word meaning is static?,Contextual embedding,Static embedding (word2vec),Logits from a transformer model,Input tokens in a language modeling head,B) Static embedding (word2vec)
LLM_cs124_week7_2025.pdf,10,"19, 20, 21",What is the main issue with static embeddings?,They are too complex,They don't change based on context,They require too much data,They are computationally expensive,B) They don't change based on context
LLM_cs124_week7_2025.pdf,10,"19, 20, 21",Which method is used to compute contextual embeddings?,Max pooling,Average pooling,Attention mechanism,Dropout,C) Attention mechanism
LLM_cs124_week7_2025.pdf,10,"19, 20, 21",What does the contextual embedding for a word represent?,"A single, unchanging meaning",A vector that changes based on context,The logits from a transformer model,The input tokens in a language modeling head,B) A vector that changes based on context
LLM_cs124_week7_2025.pdf,13,"25, 26, 27",Which equation represents the dot product between a query vector qi and a key vector kj in self-attention?,"score(xi, xj) = qi·xj","score(xi, xj) = qi·kj","aij = softmax(score(xi, xj))",ai = Xj·iaijxj,"B) score(xi, xj) = qi·kj"
LLM_cs124_week7_2025.pdf,13,"25, 26, 27",What is the role of the query vector in self-attention?,It is the current focus of attention being compared to all preceding inputs,It is a preceding input being compared to the current focus of attention,It is used to compute the output for the current focus of attention,It is a normalization of the scores to provide a probability distribution,A) It is the current focus of attention being compared to all preceding inputs
LLM_cs124_week7_2025.pdf,13,"25, 26, 27",Which equation represents the weighted sum of value vectors vj to generate the output ai in self-attention?,"score(xi, xj) = qi·kj","aij = softmax(score(xi, xj))",ai = Xj·iaijvj,qi = xiWQ,C) ai = Xj·iaijvj
LLM_cs124_week7_2025.pdf,13,"25, 26, 27",What is the dimensionality of the key and query vectors in transformers?,"dk = 512, dv = 64","dk = 64, dv = 512","dk = 1⇥d, dv = 1⇥d","dk = dk, dv = dv","A) dk = 512, dv = 64"
LLM_cs124_week7_2025.pdf,14,"27, 28, 29","Given the self-attention model, which equation calculates the score between a current focus of attention and a preceding input?","score(xi, xj) = xi·xj","aij = softmax(score(xi, xj))",ai = Xj·iaijxj,qi·kj,D) qi·kj
LLM_cs124_week7_2025.pdf,14,"27, 28, 29",What are the three different roles each input embedding plays during the attention process in transformers?,"query, value, and context","key, value, and attention","input, output, and hidden","query, key, and value","D) query, key, and value"
LLM_cs124_week7_2025.pdf,14,"27, 28, 29",Which equation generates the output value ai for the current focus of attention in self-attention?,ai = Xj·iaijxj,"aij = softmax(score(xi, xj))",ai = xi·xi + xj·xj,ai = Xj·iaijvj,D) ai = Xj·iaijvj
LLM_cs124_week7_2025.pdf,14,"27, 28, 29",What is the dimensionality of the key and query vectors in transformers?,"dk = 512, dv = 64","dk = 64, dv = 512","dk = d, dv = d","dk = dk, dv = dv","A) dk = 512, dv = 64"
LLM_cs124_week7_2025.pdf,15,"29, 30, 31",Which roles does each vector in an input sequence represent in self-attention?,"query, key, value","key, value, bias","query, bias, value","key, query, value","A) query, key, value"
LLM_cs124_week7_2025.pdf,15,"29, 30, 31","In self-attention, what do the distribution columns correspond to?",Input tokens,Output tokens,Hidden layer weights,Learning rates,A) Input tokens
LLM_cs124_week7_2025.pdf,15,"29, 30, 31","What is the name of the process where an input sequence is transformed into query, key, and value vectors?",Self-attention distribution,Self-attention encoding,Self-attention transformation,Self-attention scaling,C) Self-attention transformation
LLM_cs124_week7_2025.pdf,15,"29, 30, 31",What does the 'The chicken didn’t cross the road because it was too tired' example illustrate in the context of self-attention?,"The transformation of input vectors into query, key, and value vectors",The comparison of query and key vectors to determine similarity,The weighting and summing of value vectors,The distribution of attention across input tokens,B) The comparison of query and key vectors to determine similarity
LLM_cs124_week7_2025.pdf,16,"31, 32, 33","Which matrices are used in an attention head to project each input vector into a representation of its role as query, key, or value?","WQ, WK, WV, WD","WQ, WK, WV, WS","WQ, WK, VW, VS","WQ, WK, WV, WR","C) WQ, WK, VW, VS"
LLM_cs124_week7_2025.pdf,16,"31, 32, 33",What is the role of the query vector in the attention process?,It is the current element being compared to the preceding inputs.,It is the preceding input being compared to the current element.,It is the value of a preceding element that gets weighted and summed.,It is the weighted sum of the prior vector.,A) It is the current element being compared to the preceding inputs.
LLM_cs124_week7_2025.pdf,16,"31, 32, 33",Which equation is used to compute the similarity score between the current element and a prior element in the attention process?,Eq.9.10,Eq.9.11,Eq.9.12,Eq.9.13,B) Eq.9.11
LLM_cs124_week7_2025.pdf,16,"31, 32, 33",What is the output calculation for ai based on a weighted sum over the value vectors?,Eq.9.9,Eq.9.11,Eq.9.12,Eq.9.13,D) Eq.9.13
LLM_cs124_week7_2025.pdf,17,"33, 34, 35",Which role does the query vector play in the attention head?,It represents the current element being compared to preceding inputs,It represents the preceding input being compared to the current element,It is the weighted sum of the prior elements,It is the representation of an input vector into its role as a key,A) It represents the current element being compared to preceding inputs
LLM_cs124_week7_2025.pdf,17,"33, 34, 35",What is the purpose of the key vectors in the attention head?,They represent the current element being compared to preceding inputs,They represent the preceding input being compared to the current element,They are the weighted sum of the prior elements,They are the representations of input vectors into their role as keys,D) They are the representations of input vectors into their role as keys
LLM_cs124_week7_2025.pdf,17,"33, 34, 35",Which equation is used to compute the similarity score between two vectors in the attention head?,qi·kj/dk,qi·kj,"aij=softmax(score(xi,xj))",headi=Xjiaijvj,A) qi·kj/dk
LLM_cs124_week7_2025.pdf,17,"33, 34, 35",What is the final calculation for the attention output vector ai based on the weighted sum of value vectors?,ai=headiWO,ai=Xjiaijvj,ai=Xjiaijvj/dk,ai=headi+iaijvj,B) ai=Xjiaijvj
LLM_cs124_week7_2025.pdf,18,"35, 36, 37","Which matrices are used to project input vectors into their roles as keys, queries, and values in an attention head?","WQ, WK, WV, WO","WQ, WK, WV","WO, WQ, WK, WV","WQ, WK, VW","A) WQ, WK, WV, WO"
LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the shape of the output of the attention head in a single attention head computation?,[1 × d],[1 × dv],[dv × d],[1 × dv],B) [1 × dv]
LLM_cs124_week7_2025.pdf,18,"35, 36, 37",Which matrix is used to reshape the output of the attention head?,WQ,WK,WO,WV,C) WO
LLM_cs124_week7_2025.pdf,18,"35, 36, 37",What is the shape of the key and query vectors in an attention head computation?,[1 × d],[1 × dv],[d × dk],[dk × d],C) [d × dk]
LLM_cs124_week7_2025.pdf,19,"37, 38, 39",What is the function of SummaryAttention in NLP?,It is a method for enriching the representation of a token by incorporating contextual information,It is a neural network layer,It is a loss function,It is a regularization method,A) It is a method for enriching the representation of a token by incorporating contextual information
LLM_cs124_week7_2025.pdf,19,"37, 38, 39",Which component of Transformers is responsible for attention mechanisms?,SummaryAttention,Transformer Block,A neural network layer,A loss function,A) SummaryAttention
LLM_cs124_week7_2025.pdf,19,"37, 38, 39",What does the Transformer Block in NLP do?,It is responsible for attention mechanisms,It is a loss function,It is a neural network layer,It is a regularization method,C) It is a neural network layer
LLM_cs124_week7_2025.pdf,19,"37, 38, 39",How does the Transformer Block modify the representation of a token?,It keeps the representation constant,It passes up the enriched representation layer by layer,It applies a loss function to the representation,It applies a regularization method to the representation,B) It passes up the enriched representation layer by layer
LLM_cs124_week7_2025.pdf,20,"39, 40, 41",Which component of the Transformer model applies Multi-Head Attention?,Input Encoding,Language Modeling Head,Layer NormMultiHeadAttention,Feedforward,C) Layer NormMultiHeadAttention
LLM_cs124_week7_2025.pdf,20,"39, 40, 41",What is the role of Layer Norm in the Transformer model?,Applies Multi-Head Attention,Applies Feedforward,Normalizes the input,Adds residual connections,C) Normalizes the input
LLM_cs124_week7_2025.pdf,20,"39, 40, 41",Which part of the Transformer model applies residual connections?,Input Encoding,Language Modeling Head,Layer Norm,Residual stream,D) Residual stream
LLM_cs124_week7_2025.pdf,20,"39, 40, 41",Which component of the Transformer model is responsible for the final output?,Input Encoding,Language Modeling Head,Layer Norm,Transformer language model,D) Transformer language model
LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the purpose of the layer norm computation in a transformer block?,To add the input of a component to its output,To normalize the vector xi before the attention layer,To fully-connect two layers with weight matrices,To introduce a nonlinearity using ReLU activation,B) To normalize the vector xi before the attention layer
LLM_cs124_week7_2025.pdf,21,"41, 42, 43",Which layer in a transformer block applies the feedforward layer computation?,The attention layer,The layer norm computation,The feedforward layer,The residual stream,C) The feedforward layer
LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the dimensionality of the hidden layer in the feedforward network?,The same as the model dimensionality,Larger than the model dimensionality,Smaller than the model dimensionality,Equal to the model dimensionality,B) Larger than the model dimensionality
LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the role of the residual stream in a transformer block?,It is a metaphor for residual connections,It is a fully-connected 2-layer network,It is a normalization process for the vector xi,It is a stream of inputs and outputs between layers,D) It is a stream of inputs and outputs between layers
LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What is the purpose of layer normalization in a transformer block?,To normalize the entire transformer layer,To normalize the embedding vector of a single token,To introduce learnable gain and offset values,To perform multi-head attention,B) To normalize the embedding vector of a single token
LLM_cs124_week7_2025.pdf,22,"43, 44, 45",Which component of a transformer block takes input from other tokens?,Layer normalization,Multi-Head Attention,Feedforward network,Gain and offset parameters,B) Multi-Head Attention
LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What are the learnable parameters g and b used for in layer normalization?,To calculate the mean and standard deviation,To normalize the vector components,To introduce non-linearity in the feedforward network,To represent gain and offset values,D) To represent gain and offset values
LLM_cs124_week7_2025.pdf,22,"43, 44, 45",What is the result of layer normalization?,A new vector with zero mean and a standard deviation of one,A new vector with a mean of one and a standard deviation of zero,A new vector with all components set to zero,A new vector with all components set to one,A) A new vector with zero mean and a standard deviation of one
LLM_cs124_week7_2025.pdf,24,"47, 48, 49",What are the two distinct embeddings added for each input token in Transformers?,Token embedding and hidden state,Positional embedding and token embedding,Hidden state and output embedding,Token embedding and output embedding,B) Positional embedding and token embedding
LLM_cs124_week7_2025.pdf,24,"47, 48, 49",Which embeddings are added for each input token in Transformers?,Position and hidden state embeddings,Token and positional embeddings,Output and hidden state embeddings,Token and output embeddings,B) Token and positional embeddings
LLM_cs124_week7_2025.pdf,24,"47, 48, 49",What are the shapes of the matrices X and [1× d] in the provided lecture content?,[N × d] and [1× N],[N × d] and [d × 1],[N × 1] and [d × d],[N × d] and [d × N],B) [N × d] and [d × 1]
LLM_cs124_week7_2025.pdf,24,"47, 48, 49",Which embeddings are added to the input matrix X in Transformers?,Token and hidden state embeddings,Positional and output embeddings,Token and positional embeddings,Hidden state and output embeddings,C) Token and positional embeddings
LLM_cs124_week7_2025.pdf,25,"49, 50, 51","Given a string and a tokenized version of it, which step involves selecting the corresponding row vectors from the embedding matrix E?",Tokenizing the string with Byte Pair Encoding (BPE),Converting the string into vocabulary indices,Selecting the corresponding row vectors from the embedding matrix E,Creating the token and positional embeddings,C) Selecting the corresponding row vectors from the embedding matrix E
LLM_cs124_week7_2025.pdf,25,"49, 50, 51",Which matrix contains the embeddings for each word in the context?,Matrix X,Matrix E,Matrix V,Matrix d,B) Matrix E
LLM_cs124_week7_2025.pdf,25,"49, 50, 51",What shape does the embedding matrix E have?,[N × d],[|V| × d],[d × |V|],[d × N],B) [|V| × d]
LLM_cs124_week7_2025.pdf,25,"49, 50, 51",Which of the following is NOT a part of the process of creating token embeddings?,Tokenizing the string with Byte Pair Encoding (BPE),Converting the string into vocabulary indices,Selecting the corresponding row vectors from the embedding matrix E,Creating the positional embeddings,D) Creating the positional embeddings
LLM_cs124_week7_2025.pdf,26,"51, 52, 53",Which step comes before calculating composite embeddings in the processing of a sentence?,Calculating position embeddings,Tokenizing the sentence,Converting tokens to vocab indices,Summing word and position embeddings,B) Tokenizing the sentence
LLM_cs124_week7_2025.pdf,26,"51, 52, 53",What is the goal of learning position embeddings?,To represent the meaning of a token,To represent the position of a token in a sentence,To convert tokens into vocab indices,To initialize embeddings for each integer up to a maximum length,B) To represent the position of a token in a sentence
LLM_cs124_week7_2025.pdf,26,"51, 52, 53",Which shape does the position embedding matrix Epos have?,[N × 1],[1 × N],[N × N],[N × D],B) [1 × N]
LLM_cs124_week7_2025.pdf,26,"51, 52, 53",What is the purpose of the CompositeEmbeddings function?,To tokenize a sentence,To calculate position embeddings,To sum word and position embeddings,To convert tokens into vocab indices,C) To sum word and position embeddings
LLM_cs124_week7_2025.pdf,27,"53, 54, 55",What is the output dimension of a transformer block?,[1 x d],[1 x |V|],[d x |V|],[|V| x 1],A) [1 x d]
LLM_cs124_week7_2025.pdf,27,"53, 54, 55",How does the language modeling head transform a [1 x d] vector into a probability distribution over the vocabulary?,By multiplying it with an unembedding matrix and then applying softmax,By adding it to an embedding matrix and then applying softmax,By multiplying it with a weight matrix and then applying ReLU,By dividing it by a temperature value and then applying sigmoid,A) By multiplying it with an unembedding matrix and then applying softmax
LLM_cs124_week7_2025.pdf,27,"53, 54, 55",What is the role of the unembedding matrix in the language modeling head?,It maps a [1 x d] vector to a [1 x |V|] vector of logits,It maps a [1 x |V|] vector to a [1 x d] vector of logits,It maps a [1 x |V|] vector to a [1 x |V|] vector of probabilities,It maps a [1 x d] vector to a [d x |V|] matrix of logits,A) It maps a [1 x d] vector to a [1 x |V|] vector of logits
LLM_cs124_week7_2025.pdf,27,"53, 54, 55",What transformation is applied to the [1 x |V|] vector of logits to obtain the final probability distribution?,Softmax,ReLU,MaxPooling,AveragePooling,A) Softmax
LLM_cs124_week7_2025.pdf,28,"55, 56, 57","Given a [1 x d] vector, which layer of the language modeling head maps it to a [1 x |V|] vector of logits?",Softmax layer,Unembedding layer,TransformerBlock,Language Model Head,B) Unembedding layer
LLM_cs124_week7_2025.pdf,28,"55, 56, 57",Which matrix maps from a [1 x d] vector to a [1 x |V|] vector of logits in the language modeling head?,Unembedding matrix,Embedding matrix,TransformerBlock,Softmax,A) Unembedding matrix
LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What shape does the unembedding matrix have in the language modeling head?,[|V| × d],[d × |V|],[|V| × |V|],[1 × |V|],B) [d × |V|]
LLM_cs124_week7_2025.pdf,28,"55, 56, 57",Which layer of the language modeling head takes the output of the unembedding layer and outputs a distribution over the vocabulary?,TransformerBlock,Softmax,Unembedding layer,Language Model Head,B) Softmax
LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the role of the unembedding layer in a transformer language model?,It is a linear layer that projects from the output of the final transformer layer to the logit vector,It is a softmax layer that turns logits into probabilities over the vocabulary,It is the transpose of the embedding matrix used to map from an embedding to a vector over the vocabulary,It is the embedding matrix used to map from a one-hot vector over the vocabulary to an embedding,C) It is the transpose of the embedding matrix used to map from an embedding to a vector over the vocabulary
LLM_cs124_week7_2025.pdf,29,"57, 58, 59",Which matrix is used to map from an embedding to a vector over the vocabulary in a transformer language model?,The unembedding layer,The embedding matrix,The logit vector,The softmax layer,B) The embedding matrix
LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the output shape of the unembedding layer in a transformer language model?,1 × d,d × 1 × |V|,1 × |V|,|V| × 1 × d,C) 1 × |V|
LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the purpose of weight tying in a transformer language model?,To allow for different weights to be used in the input and output stages of the transformer,To use the same weights for two different matrices in the model,To apply the softmax function to the logits,To learn the logit vector and the embedding matrix separately,B) To use the same weights for two different matrices in the model
LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the role of the unembedding layer in the language modeling head?,It is a feedforward layer used for normalization.,It is a linear layer used for projecting from the output of the final transformer layer to the logit vector.,It is a softmax layer used to turn logits into probabilities.,It is a transpose of the embedding matrix used to map from an embedding to a vector over the vocabulary.,D) It is a transpose of the embedding matrix used to map from an embedding to a vector over the vocabulary.
LLM_cs124_week7_2025.pdf,30,"59, 60, 61",Which layer in the language modeling head maps from the output embedding for token N from the last transformer layer to a probability distribution over words in the vocabulary?,The input encoding layer,The feedforward layer,The attention layer,The unembedding layer,D) The unembedding layer
LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the output shape of the language modeling head?,1 x |V| logits,1 x d output embedding,1 x |V| token probabilities,d x |V| logits,C) 1 x |V| token probabilities
LLM_cs124_week7_2025.pdf,30,"59, 60, 61",Which layer in the language modeling head turns logits into probabilities?,The input encoding layer,The feedforward layer,The attention layer,The softmax layer,D) The softmax layer
LLM_cs124_week7_2025.pdf,31,"61, 62, 63",Why is pretraining a transformer model on large amounts of text important?,To apply it directly to new tasks without further training,To learn position embeddings for input sequences,To optimize the language model head,To reduce the model's complexity,B) To learn position embeddings for input sequences
LLM_cs124_week7_2025.pdf,31,"61, 62, 63",Which step comes first in the process of using large language models?,Applying the model to new tasks,Pretraining the transformer model on enormous amounts of text,Optimizing the language model head,Reducing the model's complexity,B) Pretraining the transformer model on enormous amounts of text
LLM_cs124_week7_2025.pdf,31,"61, 62, 63",What is the primary goal of pretraining a large language model?,To directly apply the model to new tasks,To learn position embeddings for input sequences,To optimize the language model head,To reduce the model's complexity,B) To learn position embeddings for input sequences
LLM_cs124_week7_2025.pdf,31,"61, 62, 63",Which component of a transformer model is responsible for learning position embeddings?,Position embeddings layer,Language model head,Transformer encoder,Transformer decoder,A) Position embeddings layer
LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the goal of self-supervised training in language modeling?,To predict the previous word in a text corpus,To predict the next word in a text corpus,To train a transformer model on a corpus of images,To apply a transformer model to new tasks after pretraining,B) To predict the next word in a text corpus
LLM_cs124_week7_2025.pdf,32,"63, 64, 65",Which step in the language modeling process involves asking the model to predict the next word?,Corpus selection,Training using gradient descent,Predicting the next word based on the current context,Calculating the probability distribution over the vocabulary,C) Predicting the next word based on the current context
LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the role of gradient descent in language modeling?,It is used to predict the next word in a text corpus,It is used to train the model using the predicted next word as a label,It is used to calculate the probability distribution over the vocabulary,It is used to select the corpus of text for training,B) It is used to train the model using the predicted next word as a label
LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the output of a language model during inference?,A probability distribution over the previous words in the text,A probability distribution over the next words in the text,A probability distribution over the entire text,A single word prediction,B) A probability distribution over the next words in the text
LLM_cs124_week7_2025.pdf,33,"65, 66, 67",Which loss function is used in language modeling with TransformerStack?,Mean squared error,Cross-entropy loss,Hinge loss,MSE with L2 regularization,B) Cross-entropy loss
LLM_cs124_week7_2025.pdf,33,"65, 66, 67",What does teacher forcing involve in language modeling?,Ignoring the model's prediction and using the correct word as input instead,Using the model's prediction as input for the next token,Applying regularization to the loss function,Computing the loss based on the probability of the entire sentence,A) Ignoring the model's prediction and using the correct word as input instead
LLM_cs124_week7_2025.pdf,33,"65, 66, 67",Which loss calculation is used for each token in teacher forcing?,Mean loss of all tokens,Loss for the entire sentence,Loss for the current token,Loss for the previous token,C) Loss for the current token
LLM_cs124_week7_2025.pdf,33,"65, 66, 67",What is the goal of using cross-entropy loss in language modeling?,To minimize the difference between the model's prediction and the true word,To maximize the difference between the model's prediction and the true word,To assign a high probability to the true word,To assign a low probability to the true word,A) To minimize the difference between the model's prediction and the true word
LLM_cs124_week7_2025.pdf,34,"67, 68, 69",Which organization provides the data for training most large language models?,Google,Microsoft,Common Crawl,IBM,C) Common Crawl
LLM_cs124_week7_2025.pdf,34,"67, 68, 69",Which filtering steps are taken for the Colossal Clean Crawled Corpus?,"Removal of patent text documents, Wikipedia, and news sites","Addition of boilerplate, adult content, and toxicity",Removal of filtering steps,Addition of Common Crawl data,"A) Removal of patent text documents, Wikipedia, and news sites"
LLM_cs124_week7_2025.pdf,34,"67, 68, 69",What is the main source of data for training Language Model (LM) heads?,Input tokens x1:x5,Stacked Transformer blocks,Language Modeling Head,Input Encoding,A) Input tokens x1:x5
LLM_cs124_week7_2025.pdf,34,"67, 68, 69",Which loss function is used in teacher forcing during training?,logits,logy,long and thanks for,next token all Loss,D) next token all Loss
LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What is the main source of data for training LLMs?,Common Crawl with billions of pages,Filtered patent text documents,Wikipedia and news sites,Adult content and toxicity,A) Common Crawl with billions of pages
LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What is the size of the Colossal Clean Crawled Corpus (C4)?,156 billion tokens of English,An enormous amount of data,The author of 'A Room of One's Own',The square root of 4,A) 156 billion tokens of English
LLM_cs124_week7_2025.pdf,35,"69, 70, 71",Which types of content are removed from the Colossal Clean Crawled Corpus (C4)?,"Boilerplate, adult content, and toxicity",One dog in the front room and two dogs,The author of 'A Room of One's Own',The doctor's diagnosis,"A) Boilerplate, adult content, and toxicity"
LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What gives language models their ability to do so much?,Pretraining on lots of text with all that knowledge,The size of the model,The use of regularization methods,The optimization algorithm,A) Pretraining on lots of text with all that knowledge
LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Why is it problematic to scrape copyrighted text for pretraining large language models?,It is not a problem as long as the website owner gives permission,It is not a problem as long as the text is considered fair use,It is not a problem as long as the text is publicly available,It is not a problem as the text is not copyrighted,B) It is not a problem as long as the text is considered fair use
LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Which issue arises when pretraining large language models on text from the web?,Data consent is not an issue,Privacy is not an issue,Copyright is not an issue,Large amounts of knowledge can't be contained in the models,A) Data consent is not an issue
LLM_cs124_week7_2025.pdf,36,"71, 72, 73",What is a concern when pretraining large language models on text from the web?,Websites can't contain private IP addresses and phone numbers,Websites can't contain copyrighted text,Websites can't indicate they don't want their site crawled,Websites can't contain large amounts of knowledge,C) Websites can't indicate they don't want their site crawled
LLM_cs124_week7_2025.pdf,37,"73, 74, 75","Which method is used to measure the quality of a language model by calculating the inverse probability of the model assigning to an unseen test set, normalized by the test set length?",Pretraining,Supervised finetuning,Perplexity calculation,Parameter-efficient finetuning,C) Perplexity calculation
LLM_cs124_week7_2025.pdf,37,"73, 74, 75","What is the name of the method used to measure the quality of a language model by calculating the inverse probability of the model assigning to an unseen test set, normalized by the test set length?",Perplexity calculation,Cross-entropy loss,MSE loss,RMSE loss,A) Perplexity calculation
LLM_cs124_week7_2025.pdf,37,"73, 74, 75",Which method is used to train only a subset of the parameters of a language model on new data while leaving the rest in their pretrained values?,Supervised finetuning,Pretraining,Parameter-efficient finetuning,Masked language model training,C) Parameter-efficient finetuning
LLM_cs124_week7_2025.pdf,37,"73, 74, 75",What is the name of the method used to add extra neural circuitry after the top layer of a language model to produce a classification output?,Supervised finetuning,Parameter-efficient finetuning,Classification head training,Masked language model training,C) Classification head training
LLM_cs124_week7_2025.pdf,38,"75, 76, 77","What is the inverse of the probability of a language model on a test set, normalized by the test set length, called?",Perplexity loss,Cross-entropy loss,Probability score,Perplexity metric,D) Perplexity metric
LLM_cs124_week7_2025.pdf,38,"75, 76, 77",Which method of fine-tuning a language model involves training only the classification head on new data?,Supervised finetuning,Parameter-efficient finetuning,Continued pre-training,Freezing some parameters and training others,A) Supervised finetuning
LLM_cs124_week7_2025.pdf,38,"75, 76, 77",What is the perplexity of a language model on a test set calculated as?,The product of the probabilities of each word in the test set,The sum of the probabilities of each word in the test set,"The inverse probability of the test set, normalized by the number of words",The probability of the longest word in the test set,"C) The inverse probability of the test set, normalized by the number of words"
LLM_cs124_week7_2025.pdf,38,"75, 76, 77",Which method of evaluating a language model involves measuring how well it predicts unseen text?,Probability evaluation,Perplexity evaluation,Cross-entropy evaluation,Tokenization evaluation,B) Perplexity evaluation
LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Which metric is used to evaluate the probability of a word sequence in a language model?,Perplexity,Accuracy,Fairness,Size,A) Perplexity
LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Minimizing which value in a language model leads to a better model?,Perplexity,Size,Fairness,Energy usage,A) Perplexity
LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Which factor is not typically considered when evaluating large language models?,Perplexity,Size,Fairness,Energy efficiency,D) Energy efficiency
LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Which metric is used to measure the performance decrease for language from or about certain groups in language models?,Perplexity,Size,Fairness,Energy usage,C) Fairness
LLM_cs124_week7_2025.pdf,40,"79, 80, 81",Which issue is related to large language models being called 'hallucinators'?,Copyright infringement,Inability to handle long context,Generating false information,Lack of interpretability,C) Generating false information
LLM_cs124_week7_2025.pdf,40,"79, 80, 81",Which problem does the term 'hallucination' refer to in the context of large language models?,Generating false information,Lack of interpretability,Inability to handle long context,Copyright infringement,A) Generating false information
LLM_cs124_week7_2025.pdf,42,"83, 84, 85",Which of the following are harms associated with large language models?,Toxicity and abuse,Fraud and misinformation,Phishing attacks,Improving user experience,A) Toxicity and abuse
LLM_cs124_week7_2025.pdf,42,"83, 84, 85",Which of the following is NOT a harm associated with large language models?,Fraud and misinformation,Toxicity and abuse,Phishing attacks,Improving user experience,D) Improving user experience
LLM_cs124_week7_2025.pdf,43,"85, 86, 87",What is the goal of neural network training?,To maximize loss,To minimize loss,To find the forward computation,To compute partial derivatives,B) To minimize loss
LLM_cs124_week7_2025.pdf,43,"85, 86, 87",Which operation is performed during the forward computation in neural network training?,Loss computation,Weight update,Forward computation,Backward computation,C) Forward computation
LLM_cs124_week7_2025.pdf,43,"85, 86, 87",What is the role of the loss in neural network training?,It is the target output,It is the difference between the target output and the estimated output,It is the weight update rule,It is the learning rate,B) It is the difference between the target output and the estimated output
LLM_cs124_week7_2025.pdf,43,"85, 86, 87",What is the purpose of the backward computation in neural network training?,To find the forward computation,To update the weights,To compute the loss,To compute the partial derivatives,D) To compute the partial derivatives
LLM_cs124_week7_2025.pdf,44,"87, 88, 89","Given a simple 1-layer neural network with no activation function and squared loss, what is the initial loss value for the weights w1=2, w2=-1, and b=1, when the true output is ytrue=10 and the input is (x1, x2) = (4, -3)?",0,16,20,24,B) 16
LLM_cs124_week7_2025.pdf,44,"87, 88, 89","During the training process of a neural network, which step computes the loss between the true output and the estimated output?",Forward pass,Backward pass,Weight update,Loss computation,D) Loss computation
LLM_cs124_week7_2025.pdf,44,"87, 88, 89",What is the role of the backward pass in neural network training?,Computes the forward pass,Updates the weights,Computes the loss,Assesses blame for the current loss,D) Assesses blame for the current loss
LLM_cs124_week7_2025.pdf,44,"87, 88, 89",Which of the following is a step in the neural network training process?,Running forward computation to find the estimate,Running backward computation to update weights,Computing the loss between the true output and the estimated output,Multiplying the input with the weights,A) Running forward computation to find the estimate
LLM_cs124_week7_2025.pdf,45,"89, 90, 91",Which variables are the key nodes in the computation graph for the backward pass?,Intermediate variables only,Input features only,Parameters of the model only,Both input features and parameters,C) Parameters of the model only
LLM_cs124_week7_2025.pdf,45,"89, 90, 91",What is the role of the backward pass in a neural network?,To compute the forward pass,To initialize the weights and biases,To compute the gradients of the loss function with respect to the model parameters,To update the weights and biases using the computed gradients,C) To compute the gradients of the loss function with respect to the model parameters
LLM_cs124_week7_2025.pdf,45,"89, 90, 91","Given the input (x1, x2) = (4, -3), what is the value of y in the forward pass?",1,12,10,-1,B) 12
LLM_cs124_week7_2025.pdf,45,"89, 90, 91",What is the purpose of creating a computation graph in the backward pass?,To store the intermediate results for later use,To initialize the weights and biases,To compute the gradients of the loss function with respect to the model parameters,To update the weights and biases using the computed gradients,A) To store the intermediate results for later use
LLM_cs124_week7_2025.pdf,46,"91, 92, 93",Which nodes are created during the backward pass for updating gradients in a neural network?,"Nodes for x1, x2 and the loss function","Nodes for w1, w2, b, h1, h2 and the loss function","Nodes for w1, w2, b and the loss function","Nodes for h1, h2 and the loss function","C) Nodes for w1, w2, b and the loss function"
LLM_cs124_week7_2025.pdf,46,"91, 92, 93",Which intermediate nodes are created during the backward pass for updating gradients in a neural network?,h1 and h2,"w1, w2 and b",x1 and x2,"w1, w2, b, h1 and h2",A) h1 and h2
LLM_cs124_week7_2025.pdf,46,"91, 92, 93",Which nodes are included in the computation graph for updating gradients during the backward pass?,"Nodes for w1, w2, b, h1 and h2","Nodes for w1, w2, b and the loss function","Nodes for x1, x2 and the loss function","Nodes for w1, w2, b, h1, h2 and the loss function","D) Nodes for w1, w2, b, h1, h2 and the loss function"
LLM_cs124_week7_2025.pdf,46,"91, 92, 93",Which nodes are the key nodes for gradient updates in a neural network?,"Nodes for w1, w2, b, h1 and h2","Nodes for x1, x2 and the loss function","Nodes for w1, w2, b","Nodes for h1, h2 and the loss function","C) Nodes for w1, w2, b"
LLM_cs124_week7_2025.pdf,47,"93, 94, 95",What is the formula for computing the gradient of the loss with respect to weight w1 in backpropagation?,h1 + h2 + b,w1x1y,2(ytrue-y) * -1,h1 + h2 + bh1,C) 2(ytrue-y) * -1
LLM_cs124_week7_2025.pdf,47,"93, 94, 95",Which of the following is NOT a term in the formula for computing the gradient of the loss with respect to weight w1 in backpropagation?,h1,h2,bh1,2(ytrue-y),D) 2(ytrue-y)
LLM_cs124_week7_2025.pdf,47,"93, 94, 95",What is the role of the backpropagation algorithm in training a neural network?,It computes the forward pass of the network,It computes the loss and the gradients of the loss with respect to the weights,It updates the weights based on the gradients,It initializes the weights of the network,B) It computes the loss and the gradients of the loss with respect to the weights
LLM_cs124_week7_2025.pdf,47,"93, 94, 95",Which of the following is the formula for computing the gradient of the loss with respect to weight w2 in backpropagation?,h1 + h2 + b,w1x1y,2(ytrue-y) * -1,w2x2,D) w2x2
LLM_cs124_week7_2025.pdf,48,"95, 96, 97","Given the backpropagation algorithm, which variable is updated based on the gradient of the loss function with respect to the weight w1x1?",h1,w1x1,h2,y,B) w1x1
LLM_cs124_week7_2025.pdf,48,"95, 96, 97","In the backpropagation algorithm, which of the following is the gradient of the loss function with respect to the bias term b?",2(ytrue-y),2(y-ytrue),h1+h2,-1,B) 2(y-ytrue)
LLM_cs124_week7_2025.pdf,48,"95, 96, 97","Given the backpropagation algorithm, which variable is updated based on the gradient of the loss function with respect to the weight w2x2?",h1,w2x2,h2,y,B) w2x2
LLM_cs124_week7_2025.pdf,48,"95, 96, 97","In the backpropagation algorithm, which variable is updated based on the gradient of the loss function with respect to the input feature x1?",h1,w1x1,h2,x1,D) x1
LLM_cs124_week7_2025.pdf,49,"97, 98, 99",What is the value of dw1 in the backpropagation algorithm?,1.84,0.96,-0.88,16,A) 1.84
LLM_cs124_week7_2025.pdf,49,"97, 98, 99",What is the value of db in the backpropagation algorithm?,1,0.96,-1,4,C) -1
LLM_cs124_week7_2025.pdf,49,"97, 98, 99",What is the value of dw2 in the backpropagation algorithm?,-12,4,1.84,-0.88,A) -12
LLM_cs124_week7_2025.pdf,49,"97, 98, 99",What is the value of dx1 in the backpropagation algorithm?,4,-3,12,10,B) -3
LLM_cs124_week7_2025.pdf,49,"97, 98, 99",What is the value of dy in the backpropagation algorithm?,10,12,1,4,A) 10
LLM_cs124_week7_2025.pdf,50,"99, 100, 101",What is the value of w1 after one backpropagation iteration?,1.84,0.96,1.84 - 0.88,0.96 + 1.84,C) 1.84 - 0.88
LLM_cs124_week7_2025.pdf,50,"99, 100, 101",What is the value of b after one backpropagation iteration?,1,0.96,1 - 0.01 * 4,1 + 1.84,C) 1 - 0.01 * 4
LLM_cs124_week7_2025.pdf,50,"99, 100, 101","Which weight is updated more in this backpropagation example, w1 or w2?",w1 is updated more,w2 is updated more,They are updated equally,Their updates cancel each other out,A) w1 is updated more
LLM_cs124_week7_2025.pdf,50,"99, 100, 101",What is the value of η in this backpropagation example?,0.01,0.01 * 16,1.84 * 0.01,1 / 0.01,A) 0.01
LLM_cs124_week7_2025.pdf,51,"101, 102, 103",What is the shape of the output from each head in a multi-head attention layer?,1⇥d,1⇥dv,hdv ⇥ 1,dv ⇥ hd,B) 1⇥dv
LLM_cs124_week7_2025.pdf,51,"101, 102, 103","What is the purpose of the key, query, and value weight matrices in each head of a multi-head attention layer?","To project the inputs into separate key, value, and query embeddings for each head",To perform the self-attention computation,To concatenate the outputs from each head,To reshape the output of each head,"A) To project the inputs into separate key, value, and query embeddings for each head"
LLM_cs124_week7_2025.pdf,51,"101, 102, 103","What is the shape of the key, query, and value embeddings in each head of a multi-head attention layer?","[d⇥dk], [d⇥dk], and [d⇥dv]","[1 x d], [1 x d], and [1 x d]","[dk ⇥ d], [dk ⇥ d], and [dv ⇥ d]","[d ⇥ dk], [d ⇥ dk], and [d ⇥ dv]","A) [d⇥dk], [d⇥dk], and [d⇥dv]"
LLM_cs124_week7_2025.pdf,51,"101, 102, 103",What is the shape of the output from a multi-head attention layer?,[1 x d],[1 x hdv],[hdv x d],[1 x dv],B) [1 x hdv]
LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What shape does the output of each head in a multi-head attention layer have?,1⇥dv,hdv x d,1 x d,N x d,A) 1⇥dv
LLM_cs124_week7_2025.pdf,52,"103, 104, 105","Which matrices are used to produce key, query, and value vectors for each head in a multi-head attention layer?","WQ, WK, WV","XWQ, XWK, XWV","dk x d, dk x dk, dv x dv","d x dk, d x dk, d x dv","B) XWQ, XWK, XWV"
LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the shape of the QK| matrix produced by multiplying Q and K matrices in parallelized attention?,N x N,N x dk,dk x dk,N x dv,A) N x N
LLM_cs124_week7_2025.pdf,52,"103, 104, 105",Which matrix is used to reshape the output of a multi-head attention layer?,WO,WO2,WK,WV,B) WO2
LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What shape does the QK| matrix have after masking out future values?,N × N,N × d,N × dk,dk × dk,A) N × N
LLM_cs124_week7_2025.pdf,53,"105, 106, 107",Which matrix is multiplied with the masked QK| matrix to obtain the final output of a single attention head?,V,WQ,WK,WV,B) WQ
LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What is the dimensionality of the key and query embeddings in multi-head attention?,d,dk,dv,hdv,B) dk
LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What is the shape of the output of a single attention head?,N × d,N × dk,N × dv,N × hdv,C) N × dv
LLM_cs124_week7_2025.pdf,54,"107, 108, 109",What is the purpose of masking in attention computation?,To eliminate knowledge of words that follow in the sequence,To make attention computation quadratic in the length of the input,To concatenate the output of each attention head,To reshape the output of the final linear projection,A) To eliminate knowledge of words that follow in the sequence
LLM_cs124_week7_2025.pdf,54,"107, 108, 109",What shape does the mask matrix Min have?,N×N,dk×dk,d×d,N×hdv,B) dk×dk
LLM_cs124_week7_2025.pdf,54,"107, 108, 109",What is the shape of the output of a single attention head?,N×dk,N×dv,N×hdv,N×d,B) N×dv
LLM_cs124_week7_2025.pdf,54,"107, 108, 109",What is the final step in processing the output of multi-head attention?,Concatenating the output of each attention head,Multiplying the concatenated output by a final linear projection,Setting the upper-triangular portion of the matrix to zero,Softmax activation of the QK| matrix,B) Multiplying the concatenated output by a final linear projection
LLM_cs124_week7_2025.pdf,55,"109, 110, 111",Why is the upper-triangular portion of the QK| matrix set to 0 in masking out the future?,To reduce the computational complexity,To eliminate knowledge of words that follow in the sequence,To increase the model dimension,To make use of words in the future for tasks that need it,B) To eliminate knowledge of words that follow in the sequence
LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What shape does each output of the A heads in multi-head attention have?,N x dk,N x dv,N x dkd,N x hdv,D) N x hdv
LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What is the dimensionality of the key and query embeddings in multi-head attention?,dk x dk and dv x dv,dk x dv and dv x dk,dk x d and dv x d,dk x dk and dv x d,A) dk x dk and dv x dv
LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What is the shape of the final linear projection WO in multi-head attention?,Adv⇥d,Adv⇥dk,Adv⇥dv,Adv⇥hdv,A) Adv⇥d
LLM_cs124_week7_2025.pdf,56,"111, 112, 113",Where does the transformer get the input embeddings from?,The embedding matrix E is used to compute token and positional embeddings.,The output of the MultiHeadAttention function is used as input embeddings.,The input matrix X is directly used as input embeddings.,The FFN function is used to compute input embeddings.,A) The embedding matrix E is used to compute token and positional embeddings.
LLM_cs124_week7_2025.pdf,56,"111, 112, 113",What shape does the input X and output H have in a transformer block?,Both X and H have shape [N x d],"X has shape [N x d], H has shape [N x N]","X has shape [N x N], H has shape [N x d]","X has shape [N x d], H has shape [N x N]",A) Both X and H have shape [N x d]
LLM_cs124_week7_2025.pdf,56,"111, 112, 113",Which matrix is used to store the set of initial embeddings for all tokens in the vocabulary?,MultiHeadAttention,LayerNorm,X,Embedding matrix E,D) Embedding matrix E
LLM_cs124_week7_2025.pdf,56,"111, 112, 113",What is the role of the embedding matrix E in the transformer model?,It is used to compute the input token embeddings and input positional embeddings.,It is used to compute the output of the MultiHeadAttention function.,It is used to compute the output of the FFN function.,It is used to compute the input to the LayerNorm function.,A) It is used to compute the input token embeddings and input positional embeddings.
LLM_cs124_week7_2025.pdf,57,"113, 114",Where does the transformer get the input embeddings from?,The embedding matrix E is multiplied with the input X,The embedding matrix E is added to the input X,The embedding matrix E is subtracted from the input X,The embedding matrix E is multiplied with the output H,A) The embedding matrix E is multiplied with the input X
LLM_cs124_week7_2025.pdf,57,"113, 114",What shape does the input X and output H have in a transformer block?,Both of shape [N⇥(1⇥d)],Both of shape [N⇥d],Both of shape [N⇥(d⇥1)],Both of shape [N⇥(d⇥d)],B) Both of shape [N⇥d]
LLM_cs124_week7_2025.pdf,57,"113, 114",What are the two types of embeddings the transformer computes for each token?,Input token embedding and output position embedding,Input position embedding and output token embedding,Input token embedding and output token embedding,Input position embedding and output position embedding,A) Input token embedding and output position embedding
LLM_cs124_week7_2025.pdf,57,"113, 114",How does the transformer select token embeddings from the embedding matrix E?,By multiplying the embedding matrix with the input X,By adding the embedding matrix to the input X,By indexing the embedding matrix with the token indices,By subtracting the embedding matrix from the input X,C) By indexing the embedding matrix with the token indices
