pdf_name,chunk_number,total_chunks,pages,question,option_A,option_B,option_C,option_D,correct_answer
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",Which of the following best describes how large language models generate text?,By predicting the next word based on learned patterns,By memorizing exact phrases from the training data,By following strict grammatical rules,By randomly selecting words,A) By predicting the next word based on learned patterns
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",Why can large language models learn useful language knowledge even though they are pretrained only to predict words?,Because they are trained on a large amount of text,Because they use complex mathematical formulas,Because they are given explicit instructions during training,Because they are designed to understand context perfectly,A) Because they are trained on a large amount of text
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",How do large language models differ from simple n-gram models in terms of training?,Large language models are trained on a smaller dataset,Large language models are trained to guess the next word,Large language models do not learn from text,Large language models are only trained on specific topics,B) Large language models are trained to guess the next word
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",Which of the following scenarios best illustrates the capability of large language models?,Predicting the next word in a sentence with high accuracy,Generating coherent and contextually relevant paragraphs,Memorizing and repeating predefined responses,Following step-by-step instructions without deviation,B) Generating coherent and contextually relevant paragraphs
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3","In the context of large language models, what does the term 'pretrained' imply about their training process?",They are trained only once and cannot be further improved,They are trained to perform specific tasks like translation,They are trained to predict the next word in a sequence,They are trained to recognize images,C) They are trained to predict the next word in a sequence
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which type of large language model can generate text but cannot condition on future words during generation?,Decoder-only models,Encoder-decoder models,Bidirectional models,Transformer models,A) Decoder-only models
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",What is a key advantage of encoder models over decoder-only models?,They can generate text efficiently,They can condition on future words,They require less computational resources,They have stronger contextual understanding,D) They have stronger contextual understanding
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which of the following models is an example of a decoder-only architecture?,BERT,Flan-T5,GPT-3,HuBERT,C) GPT-3
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5","In the context of pretraining, which type of model architecture is typically used for tasks that require understanding of the entire sequence of input data?",Decoders,Encoders,Encoder-decoders,None of the above,B) Encoders
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",What is a common challenge in training encoder models compared to decoder-only models?,Generating text left to right,Conditioning on future context,Building strong representations,Efficient computation,C) Building strong representations
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7","Which of the following is a characteristic of decoder-only models like GPT, Claude, and Gemini?",They can condition on future words during generation.,They are trained using masked language modeling techniques.,They are typically used for machine translation tasks.,They generate text left-to-right without looking at future tokens.,D) They generate text left-to-right without looking at future tokens.
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",What is a key limitation of decoder-only models compared to encoder-decoder models?,They cannot generate text.,They require bidirectional context.,They cannot condition on future words during generation.,They are less accurate than encoder-decoder models.,C) They cannot condition on future words during generation.
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which pretraining technique is commonly used for encoder models like BERT?,Masked Language Modeling (MLM),Causal Language Modeling,Left-to-right autoregressive modeling,Sequence-to-sequence training,A) Masked Language Modeling (MLM)
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Why might an encoder-decoder model be preferred over a decoder-only model for a machine translation task?,Decoder-only models are computationally cheaper.,Encoder-decoder models can use bidirectional context.,Decoder-only models cannot handle sequential data.,Encoder-decoder models generate text from left to right.,B) Encoder-decoder models can use bidirectional context.
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",What is a potential advantage of using an encoder-decoder architecture for speech recognition tasks?,It can directly generate speech without converting to text first.,It requires less training data than other models.,It can map from acoustic inputs to text output effectively.,It is easier to fine-tune than other architectures.,C) It can map from acoustic inputs to text output effectively.
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Which of the following is a natural use case for encoder-decoder architectures according to the lecture?,Generating text conditioned on future words,Machine translation from English to French,Creating a model that only generates words left to right,Building a model that cannot generate sequences,B) Machine translation from English to French
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Why can't decoder-only models condition on future words during generation?,Because they are trained on bidirectional context,Because they predict words left to right without access to future information,Because they are designed to only generate one word at a time,Because they require additional input from the encoder,B) Because they predict words left to right without access to future information
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What is a key advantage of encoder architectures over decoder-only models mentioned in the lecture?,They can generate text from scratch,They can condition on future words,They are easier to train,They are less complex,B) They can condition on future words
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Which of the following is NOT a type of pretraining mentioned for neural architectures in the lecture?,Causal LLMs,Autoregressive LLMs,Bidirectional LLMs,Left-to-right LLMs,C) Bidirectional LLMs
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9","According to the lecture, what big idea can many tasks be turned into using modern language models?",Generating images,Predicting words,Classifying emotions,Translating videos,B) Predicting words
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",Which of the following is true about causal LLMs according to the lecture?,They can condition on future words during generation.,They generate words left to right without looking ahead.,They are also known as encoder-decoder models.,They require bidirectional attention mechanism.,B) They generate words left to right without looking ahead.
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11","In the context of sentiment analysis, how does a language model determine the sentiment of a sentence like 'I like Jackie Chan'?",By counting positive and negative words in the sentence.,By predicting the next word and checking if it is positive or negative.,By calculating the average sentiment score of the sentence.,By using a predefined dictionary of sentiments.,B) By predicting the next word and checking if it is positive or negative.
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",What is the primary reason for using a left-to-right (autoregressive) approach in text completion tasks as described in the lecture?,To allow the model to condition on future words.,To make the training process faster.,"To ensure that the model generates text sequentially, considering past context.",To reduce the computational complexity of the model.,"C) To ensure that the model generates text sequentially, considering past context."
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11","In the task of question answering, how is the language model trained to provide a textual answer to a given question?",By predicting the exact answer word-by-word.,By predicting the next word after a token indicating the start of an answer.,By classifying the question into different categories.,By summarizing the question and providing a general response.,B) By predicting the next word after a token indicating the start of an answer.
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11","Which of the following scenarios would be best suited for a decoder-only model, based on the lecture content?",A task that requires understanding the entire document before generating a summary.,"A task that involves generating a summary from a given text, where the model only needs to consider past context.",A task that necessitates bidirectional context for accurate predictions.,A task that requires the model to condition on future words while generating text.,"B) A task that involves generating a summary from a given text, where the model only needs to consider past context."
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13","Given the sentence 'I like Jackie Chan', which word would a language model most likely predict next to indicate positive sentiment?",dislike,enjoy,hate,negative,B) enjoy
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13","In the task of question answering, if a language model is given the question 'Who wrote the book “The Origin of Species”? A:', which word would it most likely predict next to complete the answer?",Charles,Einstein,Darwin,Newton,C) Darwin
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13","For text summarization, which token would typically follow a long text to signal the start of a summary?",summary,tl;dr,abstract,conclusion,B) tl;dr
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13","When performing sentiment analysis on the sentence 'I hate Jackie Chan', which word would a language model predict with higher probability to indicate negative sentiment?",positive,neutral,negative,like,C) negative
notes/LLM_cs124_week7_2025.pdf,6,"11, 12, 13","In the context of conditional generation for summarization, if a language model is given the text 'The Origin of Species was written by Charles', what word would it most likely predict next to complete the summary?",Darwin,Newton,Einstein,Galileo,A) Darwin
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15","Given a prefix 'The quick brown', which transformer component would compute the probability of the next word being 'fox'?",Input Embedding Layer,Self-Attention Mechanism,Feedforward Network,Output Softmax Layer,D) Output Softmax Layer
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",Which of the following best describes the role of transformers in language modeling according to the lecture?,They are used to reduce the size of the model for faster computation.,They stack neural networks to improve the prediction of word probabilities.,They solely focus on the input data without considering the context.,They are responsible for decoding the final output into human-readable text.,B) They stack neural networks to improve the prediction of word probabilities.
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15","If a large language model is trained to predict the next word after a given prefix, what is a critical aspect it needs to understand?",The exact meaning of each word in isolation,The grammatical structure of individual sentences,The context and relationships between words,The historical origin of the language being modeled,C) The context and relationships between words
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",What happens if we use a prefix 'I love' to compute the probability of the next word being 'you'? Which part of the transformer model ensures this prediction is context-aware?,The embedding layer converts words into fixed-size vectors.,The self-attention mechanism considers the entire sequence of words up to the current position.,The feedforward network processes each word independently.,The output softmax layer selects the most probable word.,B) The self-attention mechanism considers the entire sequence of words up to the current position.
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15","In the context of language modeling, why might a large language model struggle with predicting uncommon words in a given prefix like 'The mysterious'",Because the model only uses a fixed vocabulary size.,Due to the lack of sufficient training data for rare words.,The model cannot handle long prefixes effectively.,It overfits to common words and ignores less frequent ones.,B) Due to the lack of sufficient training data for rare words.
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What architectural change did the Transformer introduce that was not present in previous sequence transduction models?,Use of attention mechanisms,Inclusion of convolutional layers,Presence of recurrent layers,Employment of static word embeddings,A) Use of attention mechanisms
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Which of the following experiments demonstrated that the Transformer could generalize well to a different task?,English-to-German translation,English constituency parsing,English-to-French translation,Static Word Embeddings re-discovery,B) English constituency parsing
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What was a significant advantage of the Transformer model over previous models according to the paper?,It required more time to train,It achieved better performance with less training time,It used more complex recurrent networks,It included more convolutional layers,B) It achieved better performance with less training time
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Which component of the Transformer architecture was specifically designed by Noam Shazeer and later became a fundamental part of many subsequent models?,Scaled dot-product attention,Recurrent neural networks,Convolutional neural networks,Static word embeddings,A) Scaled dot-product attention
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17","Based on the timeline provided, in which year did the Transformer architecture first appear?",2003,2017,2008,2015,B) 2017
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",In which year did the use of GPUs significantly increase according to the timeline provided?,2003,2004-2006,2008,2012,D) 2012
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which of the following techniques was introduced in 2018 and involves using pre-trained models for various NLP tasks?,Static Word Embeddings,Multi-Task Learning,Contextual Word Embeddings and Pretraining,Attention Mechanism,C) Contextual Word Embeddings and Pretraining
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",What is the role of the 'Input Encoding' block in the Transformer architecture as described in the lecture?,Generates logits for next token prediction,Encodes input tokens into embeddings,Applies attention mechanism across tokens,Stacks transformer blocks together,B) Encodes input tokens into embeddings
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which of the following is a plausible reason for the re-discovery of Static Word Embeddings in 2013 after their initial introduction in 1990?,Advancements in hardware like GPUs,Introduction of the Transformer architecture,Evolution of neural language models,Increased availability of large datasets,A) Advancements in hardware like GPUs
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19","If a model uses Stacked Transformer Blocks, what is the typical sequence of operations that would occur?",Input Encoding -> Stacked Transformer Blocks -> Language Modeling Head -> Generate logits,Input Encoding -> Generate logits -> Stacked Transformer Blocks -> Language Modeling Head,Stacked Transformer Blocks -> Input Encoding -> Language Modeling Head -> Generate logits,Language Modeling Head -> Stacked Transformer Blocks -> Input Encoding -> Generate logits,A) Input Encoding -> Stacked Transformer Blocks -> Language Modeling Head -> Generate logits
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",Why do static embeddings like word2vec fail to capture the true meaning of a word in context?,Because they are too large to process efficiently.,Because they represent a word's meaning in isolation without considering context.,Because they can only represent numerical values.,Because they are generated using a simple lookup table.,B) Because they represent a word's meaning in isolation without considering context.
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",Which technique allows each word to have a different vector representing its meaning depending on the surrounding words?,Static Embeddings,One-hot Encoding,Contextual Embeddings,Bag-of-Words Model,C) Contextual Embeddings
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21","In the context of the sentence 'The chicken didn't cross the road because it was too tired', what does the contextual embedding for 'it' represent?",The chicken,The road,The chicken's tiredness,Itself as a pronoun,D) Itself as a pronoun
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",What is the primary advantage of using contextual embeddings over static embeddings in language models?,They require less computational resources.,They provide a more accurate representation of a word's meaning in context.,They can be precomputed and stored in a database.,They are easier to train.,B) They provide a more accurate representation of a word's meaning in context.
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",Which of the following best describes the role of attention mechanisms in computing contextual embeddings?,To assign a fixed importance score to each word in the sentence.,To dynamically assign importance scores to different parts of the input sequence based on relevance.,To ignore irrelevant words in the sentence.,To generate embeddings independently of the input sequence.,B) To dynamically assign importance scores to different parts of the input sequence based on relevance.
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23","In the sentence 'The chicken didn't cross the road because it was too tired', what does the word 'it' most likely refer to?",the chicken,the road,the chicken or the road,the weather,A) the chicken
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",Which of the following best describes the process of computing contextual embeddings?,Each word has a single fixed vector regardless of context.,Words are assigned vectors based solely on their frequency in the language.,Words receive different vectors depending on their context.,Contextual embeddings are static and do not change over time.,C) Words receive different vectors depending on their context.
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",What does the term 'attention' imply in the context of contextual embeddings?,Every word gives equal weight to all neighboring words.,Words selectively focus on certain neighboring words for better meaning representation.,Attention is used to ignore the context entirely.,It refers to the physical location of words in a sentence.,B) Words selectively focus on certain neighboring words for better meaning representation.
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",Why might 'it' in the sentence 'The chicken didn't cross the road because it was too wide' refer to the road rather than the chicken?,The chicken cannot be too wide.,The road is usually described as wide.,The chicken's size is irrelevant to the sentence.,The chicken is the subject of the sentence.,B) The road is usually described as wide.
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23","If we were to apply contextual embeddings to the sentence 'At this point in the sentence, it's probably referring to either the chicken or the street', which of the following would be true?",The word 'it' has a unique embedding for both the chicken and the street.,The word 'it' has the same embedding for both the chicken and the street.,The word 'it' does not have any embedding yet.,The word 'it' is not relevant to the sentence.,A) The word 'it' has a unique embedding for both the chicken and the street.
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25","In the context of attention mechanisms, why does the word 'road' in the sentence 'The chickendidn’tcrosstheroadbecauseitwastootired' likely receive less attention compared to other words?",Because it is a common word with no specific meaning.,Because it is a stopword and typically ignored by the model.,Because it is less relevant to explaining the chicken’s decision not to cross the road.,Because it is a proper noun and requires special handling.,C) Because it is less relevant to explaining the chicken’s decision not to cross the road.
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",Which of the following best describes how the attention mechanism computes the embedding for a token?,By averaging the embeddings of all the neighboring tokens.,By selecting the most important neighboring token and copying its embedding.,By performing a weighted sum of the embeddings of selected neighboring tokens.,By randomly choosing a set of neighboring tokens to integrate their embeddings.,C) By performing a weighted sum of the embeddings of selected neighboring tokens.
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25","If we apply self-attention at Layer k+1, which of the following statements is true regarding the columns corresponding to input tokens?",Each column represents the original word embedding without any attention mechanism applied.,"Each column represents the weighted sum of embeddings from the previous layer, considering attention scores.",Each column represents a fixed set of weights applied uniformly across all tokens.,Each column represents the raw text input without any processing.,"B) Each column represents the weighted sum of embeddings from the previous layer, considering attention scores."
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25","Given the sentence 'The chickendidn’tcrosstheroadbecauseitwastootired', how would an attention mechanism determine which tokens to attend to when computing the embedding for the word 'crossed'?",Only the word 'road' would be attended to since it is directly related to the action of crossing.,"The words 'chicken', 'didn’t', and 'tired' would be given the most attention as they provide context.",All words would be attended to equally as part of the self-attention process.,No words would be attended to as the word 'crossed' is the main subject.,"B) The words 'chicken', 'didn’t', and 'tired' would be given the most attention as they provide context."
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",What is the primary purpose of using a weighted sum in the attention mechanism for computing token embeddings?,To ensure that each token’s embedding is identical.,To reduce the dimensionality of the input embeddings.,To give more importance to certain tokens that are more relevant for the current token’s context.,To increase the computational complexity of the model.,C) To give more importance to certain tokens that are more relevant for the current token’s context.
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",What is the role of the key vector in the self-attention mechanism?,It is used to compare with the query vector to produce a score.,It is used to generate the final output embedding.,It is used to weight the value vectors in the final output.,It is used to define the context for the current token.,A) It is used to compare with the query vector to produce a score.
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",How does the softmax function affect the weights aij in the self-attention mechanism?,It ensures that the weights sum to one and represent a probability distribution.,It normalizes the scores to be between -1 and 1.,It increases the weights of the less similar vectors.,It assigns equal weights to all the input vectors.,A) It ensures that the weights sum to one and represent a probability distribution.
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27","If the dot product between the query vector qi and the key vector kj is high, what does it imply about the relationship between the two vectors?",The vectors are dissimilar.,The vectors are similar.,The vectors are orthogonal.,The vectors are unrelated.,B) The vectors are similar.
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",Which of the following best describes the information flow in a causal self-attention model according to the lecture?,Each element of the sequence attends to all the subsequent elements in the sequence.,Each element of the sequence attends to all the preceding elements in the sequence.,Each element of the sequence attends only to the current element.,Each element of the sequence attends to a random subset of the sequence.,B) Each element of the sequence attends to all the preceding elements in the sequence.
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27","In the self-attention mechanism, what is the output value ai computed as?",ai = xi · xj,"ai = softmax(score(xi, xj))",ai = Σj≥i aij xj,ai = Σj≤i aij xj,C) ai = Σj≥i aij xj
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","In the context of an attention head, what role does 'xi' play when it is being compared to previous elements?",query,key,value,context,A) query
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","If the 'query' vector for 'xi' has a high similarity score with a particular 'key', what is likely to happen to the corresponding 'value' during the weighted sum operation?",It will be ignored.,It will have a higher weight.,It will have a lower weight.,It will be zeroed out.,B) It will have a higher weight.
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","Given the sentence 'The chicken didn’t cross the road because it was too tired', which of the following best describes the role of 'too tired' in the context of an attention mechanism?",query,key,value,context,C) value
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","In the self-attention distribution, if the 'query' vector for 'xi' has low similarity scores with all 'keys', how would this affect the output of the weighted sum operation?",The output would be highly influenced by 'xi'.,The output would be highly influenced by 'keys' but not 'xi'.,The output would be highly influenced by 'values' but not 'xi'.,"The output would be equally influenced by all 'keys', 'query', and 'values'.",C) The output would be highly influenced by 'values' but not 'xi'.
notes/LLM_cs124_week7_2025.pdf,15,"29, 30, 31","Considering the sentence structure, if 'xi' is the last word 'tired' in the sentence, which of the following statements is true regarding its 'value' in the attention head?",Its value will be used to weigh all previous 'queries'.,Its value will only be used to weigh the 'query' immediately before it.,Its value will be ignored since it is the final token.,"Its value will be used to weigh itself, resulting in no change.",B) Its value will only be used to weigh the 'query' immediately before it.
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",How does attention in transformers affect the representation of words during processing?,It makes the representation of words constant across different layers.,It enriches the representation of words by incorporating context-specific information.,It reduces the dimensionality of word embeddings to speed up computation.,It eliminates the need for word embeddings altogether.,B) It enriches the representation of words by incorporating context-specific information.
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39","In the context of transformers, what is the outcome of using attention mechanism on tokens?",Each token's representation remains unchanged throughout the model.,Each token's representation varies based on the surrounding context.,Each token's representation becomes identical across all layers.,Each token's representation is determined solely by its position in the sequence.,B) Each token's representation varies based on the surrounding context.
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",Why is the enriched representation of words passed up layer by layer in transformers?,To decrease the computational complexity of the model.,To maintain the context-awareness of the representations through the network.,To reduce the memory usage of the model.,To simplify the architecture by reducing the number of layers.,B) To maintain the context-awareness of the representations through the network.
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",What is a key benefit of using attention in the Transformer Block compared to traditional neural networks?,It allows the model to focus on relevant parts of the input sequence.,It reduces the number of parameters needed in the model.,It eliminates the need for recurrent layers in the network.,It speeds up the training process significantly.,A) It allows the model to focus on relevant parts of the input sequence.
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",How does the attention mechanism in transformers differ from the standard feedforward neural network layers?,"It processes data sequentially, one token at a time.",It incorporates context from other tokens to enrich each token's representation.,It uses a fixed set of weights for all tokens regardless of context.,It does not require any input data to produce an output.,B) It incorporates context from other tokens to enrich each token's representation.
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","In the transformer block, what is the purpose of the Layer Norm operation?",To introduce non-linearity into the model,To normalize the input features before they are fed into the multi-head attention layer,To reduce the computational complexity of the model,To ensure the residuals maintain a consistent scale,D) To ensure the residuals maintain a consistent scale
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",How does the multi-head attention mechanism handle the input tokens to perform self-attention?,By directly comparing each token with every other token in the sequence,By splitting the input embeddings into multiple smaller sets and performing attention on each set separately,By using a single set of weights to compute the attention scores for all tokens,By concatenating all tokens into a single vector and then applying attention,B) By splitting the input embeddings into multiple smaller sets and performing attention on each set separately
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",What is the role of the feedforward network in the transformer block?,It computes the attention scores between different tokens,It processes the input tokens through two fully connected layers,It normalizes the input embeddings,It adds residual connections to the model,B) It processes the input tokens through two fully connected layers
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","If the input sequence length is 5 tokens, how many additional tokens would typically be added by the positional encoding mechanism?",0,1,5,Depends on the specific implementation,D) Depends on the specific implementation
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",Which of the following best describes the process of input encoding in the transformer model?,Converting the input text into a sequence of fixed-length vectors,Assigning random values to the input tokens,Decoding the output logits back into text,Generating the final output sequence of tokens,A) Converting the input text into a sequence of fixed-length vectors
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49","In the Transformer model, what is the purpose of adding positional embeddings to token embeddings?",To capture the order of words in the sentence,To reduce the dimensionality of the input,To increase the speed of training,To improve the computational efficiency of the model,A) To capture the order of words in the sentence
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49","If a sentence has 5 tokens, what will be the shape of the position embeddings matrix?",[5 × d],[1 × d],[d × d],[5 × 1],A) [5 × d]
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49",How does the Transformer handle the input and output representations for a language model task?,By using only token embeddings for both input and output,By using token embeddings for input and adding a language model head for output,By using positional embeddings for input and output,By concatenating token and positional embeddings for both input and output,B) By using token embeddings for input and adding a language model head for output
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49","If a Transformer model uses an embedding size of 256, what would be the shape of the matrix X for a sentence with 10 tokens?",[10 × 256],[256 × 10],[10 × 10],[256 × 256],A) [10 × 256]
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49","In the context of Transformers, what happens if the positional embedding is omitted for a sentence with varying lengths?",The model will still function correctly as long as the token embeddings are present,The model might struggle to understand the sequence order of words,The model will become more efficient in terms of computation,The model will automatically generate positional information,B) The model might struggle to understand the sequence order of words
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","In the given example, if the token 'Thanks' corresponds to index 5 in the vocabulary, which row in the embedding matrix E would you select to get its embedding?",Row 1,Row 5,Row 10532,Row 2224,B) Row 5
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","If the positional embedding for the second word in the sequence 'Thanks for all the' is added to the token embedding of the first word, which position does this operation refer to?",First position,Second position,Third position,Fourth position,B) Second position
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","Given the token embeddings for 'Thanks', 'for', 'all', and 'the' are selected from the embedding matrix E, what is the shape of each selected embedding?",[4 × d],[1 × d],[d × d],[N × d],B) [1 × d]
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","In the context of token and positional embeddings, if the vocabulary size |V| is 10,000 and the embedding dimension d is 300, what would be the shape of the embedding matrix E?",[10000 × 300],[300 × 10000],[10000 × 10000],[300 × 300],A) [10000 × 300]
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","If a sentence contains the words 'Thanks', 'for', 'all', and 'the', how many total embeddings (including both token and positional) will be created for this sentence assuming each word has a unique index in the vocabulary?",4,8,12,16,B) 8
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53","If the input string is 'Janet will back the bill', what would be the shape of the position embeddings matrix Epos used in the model?",[1 × 5],[5 × 1],[1 × 6],[6 × 1],C) [1 × 6]
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",How are the position embeddings learned during training according to the lecture?,They are fixed and do not change.,They are randomly initialized and updated during training.,They are manually set by the user.,They are derived from the token embeddings.,B) They are randomly initialized and updated during training.
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53","Given the tokens 'Janet', 'will', 'back', 'the', 'bill' with corresponding indices [10, 20, 30, 40, 50], what will be the shape of the composite embeddings X after adding word and position embeddings?",[1 × 5],[5 × 1],[1 × 10],[10 × 1],C) [1 × 10]
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",Why are both token and position embeddings added together in the Transformer model as described in the lecture?,To reduce the dimensionality of the input data.,To capture both the meaning of words and their positions in the sentence.,To make the model easier to train.,To increase the computational complexity of the model.,B) To capture both the meaning of words and their positions in the sentence.
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53","If the maximum length of sentences is set to 10 in the model, how many position embeddings will be present in the position embeddings matrix Epos?",5,10,15,20,B) 10
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",What is the role of the unembedding matrix in the language modeling head?,To map from a [1 x d] vector to a [1 x |V|] vector of logits,To apply softmax to a [1 x |V|] vector of logits,To convert the probability distribution back to word embeddings,To reduce the dimensionality of the input vector,A) To map from a [1 x d] vector to a [1 x |V|] vector of logits
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",How does the language modeling head transform the output of a transformer block into a probability distribution over the vocabulary?,By applying a convolutional filter,Using an unembedding matrix followed by softmax,Through recurrent neural network layers,By directly mapping the vector to the vocabulary size,B) Using an unembedding matrix followed by softmax
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",Why is the softmax function necessary after using the unembedding matrix in the language modeling head?,To ensure the output vector sums to one and represents a valid probability distribution,To increase the dimensionality of the output vector,To reduce the number of parameters in the model,To enhance the positional encoding of the words,A) To ensure the output vector sums to one and represents a valid probability distribution
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55","If the dimensionality of the word and position embeddings combined is 256 (d), what would be the size of the [1 x |V|] vector of logits before applying softmax?",1 x 256,1 x |V|,256 x |V|,|V| x 256,B) 1 x |V|
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55","In the context of the language modeling head, what does the term 'head' refer to?",The top part of the transformer architecture,Additional circuits added on top of the transformer for specific tasks,The input layer of the transformer block,The output layer of the word embeddings,B) Additional circuits added on top of the transformer for specific tasks
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What is the role of the unembedding matrix in the language modeling head?,It maps from a [1 x |V|] vector of logits to a [1 x d] vector.,It maps from a [1 x d] vector to a [1 x |V|] vector of logits.,It maps from a [1 x |V|] vector of probabilities to a [1 x d] vector.,It maps from a [1 x d] vector to a [1 x |V|] vector of probabilities.,B) It maps from a [1 x d] vector to a [1 x |V|] vector of logits.
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",How does the softmax function transform the output of the unembedding matrix?,From a [1 x |V|] vector of logits to a [1 x d] vector.,From a [1 x d] vector to a [1 x |V|] vector of logits.,From a [1 x |V|] vector of logits to a [1 x |V|] vector of probabilities.,From a [1 x d] vector to a [1 x |V|] vector of probabilities.,C) From a [1 x |V|] vector of logits to a [1 x |V|] vector of probabilities.
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57","What is the shape of the unembedding matrix if the vocabulary size is 10,000 and the embedding dimension is 512?",[10000 x 512],[512 x 10000],[10000 x 1],[512 x 1],B) [512 x 10000]
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57","If the input to the language model head is a [1 x 512] vector, what will be the final output?",[1 x 512],[1 x 10000],[1 x 512 x 10000],[10000 x 1],B) [1 x 10000]
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",Which of the following best describes the relationship between the embedding matrix and the unembedding matrix?,They both map from [1 x d] to [1 x |V|].,They both map from [1 x |V|] to [1 x d].,"The embedding matrix maps from [1 x d] to [1 x |V|], while the unembedding matrix maps from [1 x |V|] to [1 x d].","The embedding matrix maps from [1 x |V|] to [1 x d], while the unembedding matrix maps from [1 x d] to [1 x |V|].","D) The embedding matrix maps from [1 x |V|] to [1 x d], while the unembedding matrix maps from [1 x d] to [1 x |V|]."
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the role of the unembedding layer in the context of a transformer model?,To convert the output of the final transformer layer into a probability distribution over the vocabulary,To perform weight tying between the embedding and output layers,To map from a one-hot vector over the vocabulary to an embedding,To reduce the dimensionality of the output from the transformer layer,A) To convert the output of the final transformer layer into a probability distribution over the vocabulary
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",How does the unembedding layer contribute to the language modeling head?,By tying the weights of the embedding matrix to those of the output layer,By projecting the d-dimensional output embedding to a 1x|V| logits vector,By using the softmax function to increase the dimensionality of the output,By directly outputting the probabilities for each word without transformation,B) By projecting the d-dimensional output embedding to a 1x|V| logits vector
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",Why is the unembedding layer referred to as 'unembedding'?,Because it converts logits back to probabilities,Because it maps embeddings back to a vector over the vocabulary,Because it ties the embedding matrix to the output layer,Because it reduces the number of parameters in the model,B) Because it maps embeddings back to a vector over the vocabulary
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the shape of the logits vector produced by the unembedding layer?,[1 x d],[d x |V|],[1 x |V|],[|V| x 1],C) [1 x |V|]
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59","In the context of language modeling, what is the significance of the context window size in transformer models?",It determines the number of tokens the model can handle at once,It affects the amount of information the model considers when predicting the next word,It specifies the maximum length of the input sequence,It indicates the number of layers in the transformer model,B) It affects the amount of information the model considers when predicting the next word
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the role of the unembedding layer in the language modeling head of a transformer model?,It projects the output embedding to a probability distribution over the vocabulary.,It performs the reverse mapping from the output embedding to the vocabulary.,It computes the attention scores for the current token.,It normalizes the input embeddings before processing.,B) It performs the reverse mapping from the output embedding to the vocabulary.
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",How does the softmax function contribute to the language modeling head of a transformer?,It increases the computational efficiency of the model by reducing the dimensions of the output.,It turns the logits into a probability distribution over the vocabulary.,It applies positional encoding to the input tokens.,It ties the embedding matrix to the weight matrix.,B) It turns the logits into a probability distribution over the vocabulary.
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",Which of the following best describes the purpose of the linear layer in the language modeling head of a transformer model?,To compute the attention mechanism for the current token.,To project the output embedding to a logit vector.,To normalize the input embeddings.,To tie the embedding matrix to the weight matrix.,B) To project the output embedding to a logit vector.
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61","In the context of the transformer model, what is the significance of tying the embedding matrix to the transpose of the unembedding layer?",It allows the model to use a smaller vocabulary size.,It reduces the computational complexity of the model.,It ensures consistency between the embedding and unembedding processes.,It improves the model's ability to handle out-of-vocabulary words.,C) It ensures consistency between the embedding and unembedding processes.
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61","If the context window of a transformer model is set to 32K tokens, how would this affect the language modeling head during text generation?",It would decrease the accuracy of the model as it cannot capture long-range dependencies.,It would increase the computational load but improve the quality of generated text.,It would have no impact on the language modeling head as it only depends on the current token.,It would reduce the number of parameters needed for the model.,B) It would increase the computational load but improve the quality of generated text.
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",Which component is used to handle positional information in Transformers?,Positional Encoding,Language Model Head,Attention Mechanism,Embedding Layer,A) Positional Encoding
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",What is the primary purpose of pretraining a transformer model on vast amounts of text before applying it to specific tasks?,To increase its computational efficiency,To improve its general language understanding,To reduce its memory usage,To enhance its graphical capabilities,B) To improve its general language understanding
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63","In the context of large language models, what does the term 'pretraining' refer to?",Training the model on a small dataset for specific tasks,"Training the model on a large, diverse dataset before fine-tuning",Fine-tuning the model on a specific task after initial training,Training the model to recognize images rather than text,"B) Training the model on a large, diverse dataset before fine-tuning"
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",Which part of the transformer architecture is responsible for predicting the next word in a sequence during pretraining?,Positional Embeddings,Language Model Head,Feedforward Network,Self-Attention Mechanism,B) Language Model Head
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",Why is it important to use both position embeddings and the language model head in transformers?,To reduce the size of the model,To ensure the model understands the sequence and context,To make the model faster during inference,To prevent overfitting on the training data,B) To ensure the model understands the sequence and context
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",Which of the following best describes the self-supervised training algorithm used for pretraining language models?,Train the model to classify different types of text.,Train the model to predict the next word in a sentence.,Train the model to generate complete sentences from scratch.,Train the model to translate between languages.,B) Train the model to predict the next word in a sentence.
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65","During the pretraining phase, what is the primary task the model is trained to perform?",Classify text into predefined categories.,Predict the next word in a given sequence of text.,Translate text from one language to another.,Summarize long documents into shorter versions.,B) Predict the next word in a given sequence of text.
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65","In the context of language modeling, what does the output of the model represent?",A binary classification of input text.,A probability distribution over the vocabulary.,A fixed set of keywords related to the input text.,A sequence of predicted words without probabilities.,B) A probability distribution over the vocabulary.
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65","If the model predicts 'the' as the next word with a high probability, how would this affect the loss function during training?",Decrease the loss because the model's prediction matches the actual next word.,Increase the loss because the model's prediction does not match the actual next word.,Decrease the loss because the model's prediction is irrelevant to the actual next word.,Increase the loss because the model's prediction is too certain.,A) Decrease the loss because the model's prediction matches the actual next word.
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",Which of the following scenarios illustrates the application of a pre-trained language model to a new task?,Training a model to recognize handwritten digits.,Fine-tuning a model to generate weather forecasts based on historical data.,Teaching a robot to navigate through a maze.,Training a model to play chess.,B) Fine-tuning a model to generate weather forecasts based on historical data.
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67","In the context of language modeling, what does the cross-entropy loss measure?",The accuracy of the model's predictions,The difference between the model's predicted probability distribution and the actual probability distribution of the correct word,The number of correct words predicted by the model,The time it takes for the model to converge,B) The difference between the model's predicted probability distribution and the actual probability distribution of the correct word
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",What is the effect of 'teacher forcing' in the training process of a transformer model?,It increases the diversity of the model's predictions.,"It uses the ground truth of the next word at each step to compute the loss, rather than the model's prediction.",It reduces the computational complexity of the model.,It prevents overfitting by introducing noise.,"B) It uses the ground truth of the next word at each step to compute the loss, rather than the model's prediction."
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",Which of the following best describes how the cross-entropy loss is calculated for a single word in a sentence?,The sum of probabilities assigned to all possible words in the vocabulary.,The negative logarithm of the probability assigned to the correct word.,The average probability assigned to all words in the vocabulary.,The product of probabilities assigned to all previous words in the sentence.,B) The negative logarithm of the probability assigned to the correct word.
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",Why might a high cross-entropy loss indicate a problem with the model's predictions?,It suggests the model is perfectly predicting the correct words.,"It indicates the model is assigning a low probability to the correct word, which is undesirable.",It means the model has converged too quickly.,It shows the model is taking too long to make predictions.,"B) It indicates the model is assigning a low probability to the correct word, which is undesirable."
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67","In the context of teacher forcing, what happens at each subsequent token position after the first token?",The model predicts the next word without seeing any context.,The model sees the correct previous tokens and the correct next token to predict the next word.,The model sees the correct previous tokens but not the next token to predict the next word.,The model sees the incorrect previous tokens and the incorrect next token to predict the next word.,B) The model sees the correct previous tokens and the correct next token to predict the next word.
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71","Which corpus is mentioned as containing 156 billion tokens of English and is known for filtering out boilerplate, adult content, and toxicity?",webCommon Crawl,Colossal Clean Crawled Corpus (C4),Patent text documents,Wikipedia and news sites,B) Colossal Clean Crawled Corpus (C4)
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What type of content is explicitly stated to be removed during the preprocessing of the training data for LLMs?,Patent text,Wikipedia articles,News sites,"Boilerplate, adult content, and toxicity","D) Boilerplate, adult content, and toxicity"
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",Which statement best describes what a language model learns from pretraining on a large amount of text?,It learns specific factual information.,It gains the ability to understand complex mathematical concepts.,It acquires a broad understanding of language patterns and contexts.,It memorizes the exact content of the web pages.,C) It acquires a broad understanding of language patterns and contexts.
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71","Based on the lecture, which of the following sentences would likely be included in the training data for an LLM?",The doctor told me that he.,There are canines everywhere!,It wasn't just big it was enormous.,The author of 'A Room of One's Own' is Virginia Woolf.,A) The doctor told me that he.
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71","According to the lecture, why is the webCommon Crawl considered useful for training LLMs?",It provides a snapshot of the entire web with billions of pages.,It exclusively contains scientific literature.,It is a curated selection of high-quality texts.,It focuses on personal blogs and social media content.,A) It provides a snapshot of the entire web with billions of pages.
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",What is a potential legal issue when using web-scraped data for pretraining large language models?,Copyright infringement,Data privacy violations,Algorithmic bias,Data overfitting,A) Copyright infringement
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Which of the following is a concern related to privacy when collecting web data for pretraining language models?,Website owners indicating they do not want their site crawled,The use of public domain texts,The inclusion of general knowledge,The application of natural language processing algorithms,A) Website owners indicating they do not want their site crawled
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",What is an open legal question regarding web-scraped data used for pretraining language models?,Whether fair use doctrine applies in the US,If the data can be used without consent,Whether the data is copyrighted,If the data is publicly available,A) Whether fair use doctrine applies in the US
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Which of the following scenarios would likely raise concerns about data consent during pretraining of a language model?,Using publicly accessible Wikipedia articles,Scraping data from a website that has indicated it does not want to be crawled,Collecting data from a well-known public book repository,Gathering information from a government-owned database,B) Scraping data from a website that has indicated it does not want to be crawled
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Why is it important to consider privacy issues when collecting data for pretraining large language models?,To ensure the models are accurate,To avoid including sensitive personal information,To prevent the model from learning too much,To speed up the pretraining process,B) To avoid including sensitive personal information
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75","When fine-tuning a large language model using parameter-efficient fine-tuning (PEFT), what strategy is employed to reduce computational costs?",Freezing some parameters while training a subset of parameters on new data,Retraining all parameters on the new data,Training only the classification head on new data,Using supervised fine-tuning to directly predict command responses,A) Freezing some parameters while training a subset of parameters on new data
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75","In the context of evaluating large language models, how is perplexity defined mathematically?",The average log-probability of the test set,The inverse probability of the test set normalized by its length,The sum of probabilities assigned to the test set,The standard deviation of the probability distribution over the test set,B) The inverse probability of the test set normalized by its length
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",Which type of fine-tuning involves using a language model as a classifier for a specific task by adding a classification head to the top layer of the model?,Parameter-efficient fine-tuning,Supervised fine-tuning,Task-specific fine-tuning,Classification fine-tuning,C) Task-specific fine-tuning
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",What distinguishes supervised fine-tuning (SFT) from other forms of fine-tuning mentioned in the lecture?,It uses a classification head to predict class labels,It involves creating a dataset of prompts and desired responses,It freezes all parameters and trains only the classification head,It re-trains all parameters on a new dataset without supervision,B) It involves creating a dataset of prompts and desired responses
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",Why is retraining all parameters of a large language model very slow and expensive during fine-tuning?,Due to the high computational cost of training a large number of parameters,Because the language model does not provide enough data,Due to the lack of appropriate training algorithms,Because the model cannot handle new data,A) Due to the high computational cost of training a large number of parameters
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",What is the primary reason for using perplexity to evaluate large language models instead of raw probability?,Perplexity is easier to compute than raw probability.,"Perplexity normalizes for the length of the test set, making it a per-word metric.","Perplexity ranges from 0 to 1, while raw probability ranges from 1 to infinity.",Perplexity is less sensitive to the length of the test set.,"B) Perplexity normalizes for the length of the test set, making it a per-word metric."
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77","In the context of fine-tuning language models, what does parameter-efficient fine-tuning (PEFT) refer to?",Fine-tuning all the parameters of the model from scratch.,Freezing some parameters and training only a subset of parameters on new data.,Freezing the entire model and training only a classification head.,Training the model on unlabeled data to adapt to a new domain.,B) Freezing some parameters and training only a subset of parameters on new data.
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77","Which type of fine-tuning involves using a language model as a classifier for a specific task, such as sentiment analysis?",Parameter-efficient fine-tuning (PEFT),Supervised fine-tuning (SFT),Task-specific fine-tuning,Classiﬁcation head fine-tuning,D) Classiﬁcation head fine-tuning
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",What is the formula for calculating the perplexity of a model on a test set of n tokens?,Pq(w1:n) * 1/n,1 / (Pq(w1:n) * n),ns1Pq(w1:n),1 / Pq(w1:n),B) 1 / (Pq(w1:n) * n)
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77","In supervised fine-tuning (SFT), what is the typical process for creating the training dataset?",Generating random prompts and desired responses.,Using any available data without any specific labels.,Handcrafting a dataset of prompts and desired responses.,Freezing all parameters and training on new data.,C) Handcrafting a dataset of prompts and desired responses.
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79","When comparing two large language models using perplexity, which of the following scenarios would indicate a better model?",A higher perplexity score because it indicates a more complex model.,A lower perplexity score because it indicates a better fit to the data.,A higher perplexity score because it indicates a more diverse vocabulary.,A lower perplexity score because it indicates a less robust model.,B) A lower perplexity score because it indicates a better fit to the data.
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Why is perplexity considered sensitive to length/tokenization when comparing different language models?,A,B,C,D,B) B
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79","In the context of evaluating large language models, which factor is important to consider beyond perplexity?",A,B,C,D,B) B
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Which of the following is a valid benchmark for measuring fairness in large language models?,A,B,C,D,B) B
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79","If a large language model performs well on benchmarks but exhibits harmful stereotypes, what does this suggest about the model?",A,B,C,D,B) B
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",Which of the following best describes the phenomenon of 'hallucination' in large language models?,The model consistently provides accurate information.,The model generates information that it believes to be true but is not.,The model refuses to generate any responses.,The model requires extensive copyright permissions.,B) The model generates information that it believes to be true but is not.
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",Why is copyright a concern when using large language models?,The model consistently provides accurate information.,The model generates information that it believes to be true but is not.,The model refuses to generate any responses.,The model requires extensive copyright permissions.,B) The model generates information that it believes to be true but is not.
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81","In the context of large language models, how does hallucination differ from providing inaccurate information due to data bias?","Hallucination involves the model generating false information it believes to be true, while data bias leads to systematic errors in predictions.",Hallucination and data bias are the same phenomena.,"Data bias involves the model generating false information it believes to be true, while hallucination leads to systematic errors in predictions.","Hallucination involves the model refusing to generate any responses, while data bias leads to systematic errors in predictions.","A) Hallucination involves the model generating false information it believes to be true, while data bias leads to systematic errors in predictions."
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81","If a large language model is used to generate a report for a legal case, what precaution should be taken regarding copyright?",Ensure all sources used are properly cited and permissions are obtained if necessary.,Only use the model to generate factual information and avoid any references to copyrighted materials.,Ignore copyright concerns as the model generates its own content.,Use the model to generate as much content as possible to ensure comprehensive coverage.,A) Ensure all sources used are properly cited and permissions are obtained if necessary.
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",How might a researcher mitigate the risk of hallucination in a large language model being used for academic research?,By avoiding the use of any models and only relying on human experts.,By carefully evaluating the model's outputs and cross-referencing with reliable sources.,"By using the model exclusively for generating abstracts and conclusions, avoiding detailed analysis.",By requesting the model to provide only binary answers to avoid complexity.,B) By carefully evaluating the model's outputs and cross-referencing with reliable sources.
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",Which of the following best describes the primary concern of copyright law?,Protecting the privacy of creators,Ensuring fair use for public interest,Preventing unauthorized use of creative works,Regulating online behavior and interactions,C) Preventing unauthorized use of creative works
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",How does privacy law differ from copyright law in terms of its main focus?,Privacy focuses on protecting intellectual property rights,Privacy aims to protect personal information and data,Privacy deals with the regulation of online content,Privacy is concerned with the prevention of cyberbullying,B) Privacy aims to protect personal information and data
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83","In the context of online platforms, which issue would most likely fall under the category of toxicity and abuse rather than privacy or copyright?",Unlicensed use of copyrighted music,Harassment and bullying by users,Data breaches exposing user information,Unauthorized sharing of trade secrets,B) Harassment and bullying by users
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",Which of the following scenarios best illustrates a situation where copyright laws would be applicable?,A company monitors its employees' social media accounts,An individual uses a friend's password without permission,Someone downloads and distributes unauthorized copies of software,Users engage in hate speech in an online forum,C) Someone downloads and distributes unauthorized copies of software
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83","If a user is found to be repeatedly posting offensive content that violates community guidelines, which area of law is most directly involved in addressing this issue?",Copyright infringement,Privacy invasion,Toxicity and abuse,Trademark violation,C) Toxicity and abuse
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",Which of the following scenarios exemplifies a potential harm from Large Language Models as discussed in the lecture?,Improving customer service through chatbots,Generating misleading news headlines to spread misinformation,Assisting in language translation for international businesses,Enhancing educational resources for students,B) Generating misleading news headlines to spread misinformation
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85","According to the lecture, what is a key concern regarding the use of Large Language Models in relation to phishing attacks?",They can increase the speed of data processing.,They can generate highly convincing messages that may trick users into providing personal information.,They can improve the security of user data.,They can reduce the cost of cybersecurity measures.,B) They can generate highly convincing messages that may trick users into providing personal information.
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",The lecture mentions that fraud is one of the harms associated with large language models. Which of the following best illustrates this harm?,A bank using a language model to provide better customer support,A financial institution employing a language model to craft deceptive investment schemes,A company utilizing a language model to enhance its marketing strategies,A business using a language model to automate its customer service inquiries,B) A financial institution employing a language model to craft deceptive investment schemes
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",How does the lecture suggest that large language models can contribute to the spread of misinformation?,By accurately summarizing factual articles,"By generating content that mimics human writing styles, making it difficult to distinguish from real news",By improving the accuracy of predictive algorithms,By enhancing the efficiency of data storage systems,"B) By generating content that mimics human writing styles, making it difficult to distinguish from real news"
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85","In the context of the lecture, which of the following measures could potentially mitigate the risk of fraud involving large language models?",Implementing stricter password policies,Using advanced natural language processing techniques to detect suspicious patterns,Increasing the number of physical security guards,Expanding the network infrastructure,B) Using advanced natural language processing techniques to detect suspicious patterns
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91",What is the value of y after the forward pass with the given weights and inputs?,12,10,-2,4,A) 12
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","If the true value of y (ytrue) was 8 instead of 10, what would be the new loss?",4,6,8,16,D) 16
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","In the context of the backward pass, which of the following is a key node for gradient calculation?",x1,w1,b,x2,B) w1
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","If the input values were changed to (x1, x2) = (3, 2), what would be the new value of y during the forward pass?",9,7,5,11,A) 9
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91",Which of the following represents the correct sequence of steps in the forward pass?,"Compute y, then create computation graph, then compute loss","Create computation graph, then compute y, then compute loss","Compute loss, then compute y, then create computation graph","Compute y, then compute loss, then create computation graph","B) Create computation graph, then compute y, then compute loss"
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","In the context of multi-head attention, how does the dimensionality of the input and output vectors relate to the number of heads (A) and the dimensionality of the key and value embeddings (dk and dv)?","Input dimension is d, output dimension is hdv, where h = A and d = dk = dv","Input dimension is d, output dimension is d, where d = dk + dv","Input dimension is d, output dimension is d, where d = dk = dv","Input dimension is d, output dimension is hdv, where h = A and d = dk = dv / A","D) Input dimension is d, output dimension is hdv, where h = A and d = dk = dv / A"
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103",Which of the following best describes the role of the softmax function in the attention mechanism within a multi-head attention layer?,"It normalizes the scores to represent probabilities that sum to one, indicating the importance of each word in the context.",It scales the input vectors to match the output dimensions.,It concatenates the outputs from all heads before applying the final linear projection.,"It projects the input vectors into the key, query, and value spaces.","A) It normalizes the scores to represent probabilities that sum to one, indicating the importance of each word in the context."
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","If a transformer model uses 8 heads in a multi-head attention layer, each with key and value embeddings of dimension 64, what would be the dimensionality of the concatenated output from the heads before the final projection layer?",512,64,8,4096,A) 512
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103",How does the multi-head attention mechanism enhance the model's ability to capture complex relationships in the input sequence compared to a single-head attention mechanism?,"By using a single set of weights for keys, queries, and values, it simplifies the computation.","By projecting the input into multiple key, query, and value spaces, it allows the model to focus on different aspects of the context.","By reducing the dimensionality of the output, it speeds up the training process.","By increasing the number of parameters, it makes the model more prone to overfitting.","B) By projecting the input into multiple key, query, and value spaces, it allows the model to focus on different aspects of the context."
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","In the equation for the attention score, what does the denominator pdk represent?",The dimensionality of the output after concatenating the heads,The scaling factor to ensure the score is normalized,The number of heads in the multi-head attention layer,The dimensionality of the input vector,B) The scaling factor to ensure the score is normalized
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105","In the context of multi-head attention, what is the dimensionality of the key, query, and value embeddings for each head?","dk and dv, where dk = dv = 64","d and dk, where d = 512 and dk = 64","d and dv, where d = 512 and dv = 64","d and d, where d = 512","A) dk and dv, where dk = dv = 64"
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",How does the multi-head attention mechanism handle the computation of attention scores for multiple heads?,It concatenates the outputs of all heads and projects them down to the model dimension d.,It computes the attention scores independently for each head without any concatenation.,"It first concatenates the queries, keys, and values before applying attention.",It uses a single set of weights for all heads to compute the attention scores.,A) It concatenates the outputs of all heads and projects them down to the model dimension d.
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the role of the mask matrix M in the self-attention computation?,To ensure that the model pays equal attention to all tokens in the input sequence.,To prevent the model from using information about future tokens during the prediction of current tokens.,To increase the computational complexity of the attention mechanism.,To reduce the dimensionality of the input sequence.,B) To prevent the model from using information about future tokens during the prediction of current tokens.
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105","If the model dimension d is 512, and there are 8 heads in the multi-head attention layer, what is the total dimensionality of the output after concatenating the outputs of all heads?",512,4096,1024,64,B) 4096
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105","In the parallelized computation of attention, what is the shape of the QK| matrix?",[N x N],[N x d],[d x d],[d x N],A) [N x N]
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",What is the purpose of the mask function in the self-attention mechanism?,To ensure that the model only considers future context during prediction.,To increase the computational complexity of the model.,To allow the model to consider all possible token combinations equally.,To reduce the number of parameters in the model.,A) To ensure that the model only considers future context during prediction.
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",How does the mask matrix M affect the QK^T matrix in the self-attention computation?,"It sets values above the diagonal to -∞, making their softmax output 0.","It sets all values to 0, making the softmax output uniform.","It increases the values above the diagonal, enhancing the attention scores.",It leaves the values unchanged but reduces the overall dimensionality.,"A) It sets values above the diagonal to -∞, making their softmax output 0."
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109","In the multi-head attention mechanism, what is the role of the final linear projection WO?",To concatenate the outputs of all heads.,To transform the concatenated output back to the original output dimension.,To calculate the attention scores.,To compute the QK^T matrix.,B) To transform the concatenated output back to the original output dimension.
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",Why is attention considered quadratic in the length of the input sequence?,Because the softmax operation is applied to each token individually.,Because the dot product needs to be computed between every pair of tokens.,Because the number of parameters in the model scales with the sequence length.,Because the masking function reduces the computational complexity.,B) Because the dot product needs to be computed between every pair of tokens.
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109","If the input sequence length is N and there are A attention heads in a multi-head attention layer, what is the shape of the final output after applying the final linear projection WO?",N x d,N x A,N x Ad,N x dv,C) N x Ad
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What is the purpose of masking out the future in self-attention computation for language modeling?,"To ensure that the model can only attend to past tokens, making the prediction of the next word simpler.",To speed up the computation of attention scores.,To increase the model's capacity to learn long-range dependencies.,To reduce the dimensionality of the input embeddings.,"A) To ensure that the model can only attend to past tokens, making the prediction of the next word simpler."
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111","In the context of multi-head attention, what is the dimensionality of the weight matrices WQ, WK, and WV for a transformer model with d=512, dk=dv=64, and A=8?","WQ: [512x64], WK: [512x64], WV: [512x64]","WQ: [512x64], WK: [512x64], WV: [512x64] per head, total: [512x512]","WQ: [512x64], WK: [512x64], WV: [512x64] per head, total: [512x512]*8","WQ: [512x64], WK: [512x64], WV: [512x64] per head, total: [512x64]","C) WQ: [512x64], WK: [512x64], WV: [512x64] per head, total: [512x512]*8"
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",Why is attention considered quadratic in the length of the input?,Because the number of parameters in the transformer model increases quadratically with input length.,"Because each token needs to compute dot products with every other token in the input, leading to a quadratic growth in complexity.",Because the softmax function applied during attention computation grows quadratically.,Because the masking process introduces a quadratic number of zeros in the attention matrix.,"B) Because each token needs to compute dot products with every other token in the input, leading to a quadratic growth in complexity."
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",How does parallelizing multi-head attention help in handling long documents in transformer models?,By reducing the dimensionality of the input embeddings.,"By increasing the number of parameters in the model, allowing it to capture more complex patterns.","By computing multiple attention heads in parallel, reducing the overall computational cost while maintaining the ability to process long sequences.",By applying a linear transformation to the input embeddings before attention computation.,"C) By computing multiple attention heads in parallel, reducing the overall computational cost while maintaining the ability to process long sequences."
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",What is the final step in the multi-head attention mechanism after obtaining the outputs from A heads?,Concatenating the outputs of all heads and applying a linear projection.,Computing the dot product between the query and key vectors.,Applying a softmax function to the attention scores.,Masking the upper triangular portion of the attention matrix.,A) Concatenating the outputs of all heads and applying a linear projection.
