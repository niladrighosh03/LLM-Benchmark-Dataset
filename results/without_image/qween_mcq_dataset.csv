pdf_name,chunk_number,total_chunks,pages,question,option_A,option_B,option_C,option_D,correct_answer
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",How do large language models generate text according to the lecture?,By predicting the next sentence given the current context,By assigning probabilities to sequences of words and sampling the next word,By directly translating input text into another language,By memorizing entire texts and reproducing them,B) By assigning probabilities to sequences of words and sampling the next word
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",Why are large language models considered to learn a lot of useful language knowledge despite being pretrained only to predict words?,Because they are trained on a wide variety of texts which expose them to many linguistic patterns,Because they use complex mathematical formulas to understand language,Because they are specifically designed to understand human emotions,Because they can read and write multiple languages simultaneously,A) Because they are trained on a wide variety of texts which expose them to many linguistic patterns
notes/LLM_cs124_week7_2025.pdf,1,"1, 2, 3",Which of the following best describes the training process of large language models mentioned in the lecture?,They are trained to recognize images and convert them into text,They are trained by learning to predict the next word in a sequence,They are trained to play games and use language to strategize,They are trained to understand and respond to voice commands,B) They are trained by learning to predict the next word in a sequence
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5","Which of the following is a characteristic of decoder-only models like GPT, Claude, and Gemini?",Can condition on future words while generating text,Generate text by predicting words left to right,Build strong representations through bidirectional training,Are primarily used for encoder-decoder tasks,B) Generate text by predicting words left to right
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",What is the main limitation of decoder-only models compared to encoder models?,They cannot generate text,They cannot condition on future words while generating,They require more computational resources,They are less accurate in representation learning,B) They cannot condition on future words while generating
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which model architecture would be most suitable for a task requiring understanding of both past and future context in the input sequence?,Decoder-only model,Encoder model,Encoder-decoder model,None of the above,C) Encoder-decoder model
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",What is the primary difference between decoders and encoders in the context of large language models?,"Decoders can generate text, while encoders cannot","Encoders can condition on future words, while decoders cannot","Encoders generate text, while decoders predict it","Decoders focus on left-to-right prediction, while encoders focus on bidirectional context","A) Decoders can generate text, while encoders cannot"
notes/LLM_cs124_week7_2025.pdf,2,"3, 4, 5",Which of the following models would likely benefit from bidirectional training during pretraining?,A decoder-only model,An encoder model,An encoder-decoder model,None of the above,B) An encoder model
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",Which of the following is true about decoder-only models as discussed in today's lecture?,They can condition on future words while generating text.,"They are also known as causal LLMs, autoregressive LLMs, and left-to-right LLMs.",They are better suited for tasks that require bidirectional context.,They are typically trained using masked language modeling.,"B) They are also known as causal LLMs, autoregressive LLMs, and left-to-right LLMs."
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7",What is a key limitation of decoder-only models compared to encoder models?,Masked Language Models (MLMs),Bidirectional Encoder Representations from Transformers (BERT),Generative Pre-trained Transformer (GPT),Sequence-to-sequence models,B) Bidirectional Encoder Representations from Transformers (BERT)
notes/LLM_cs124_week7_2025.pdf,3,"5, 6, 7","Which model type is described as being good at mapping from one sequence to another, such as in machine translation or speech recognition?",Decoder-only models,Encoders,Encoder-Decoders,Language models,C) Encoder-Decoders
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Which of the following is a natural use case for encoder-decoder models according to the lecture?,Generating text from a given prompt,Predicting the next word in a sentence,Translating a sentence from one language to another,Classifying text into predefined categories,C) Translating a sentence from one language to another
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Why can't decoder-only models condition on future words during generation?,Because they don't have access to past context,Because they generate words sequentially and don't look ahead,Because they are bidirectional models,Because they are trained on static data without context,B) Because they generate words sequentially and don't look ahead
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",What is the main advantage of encoders over decoders as mentioned in the lecture?,They can generate text more efficiently,They can condition on future words,They require less training data,They are better for machine translation,B) They can condition on future words
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9",Which of the following best describes the big idea discussed in the lecture regarding language models?,Language models are primarily used for image recognition,Many language tasks can be reduced to predicting words,Encoder-decoder models are the most efficient for all language tasks,Bidirectional models are superior to unidirectional models,B) Many language tasks can be reduced to predicting words
notes/LLM_cs124_week7_2025.pdf,4,"7, 8, 9","What type of models are typically referred to when people mention 'LLMs' like ChatGPT, Claude, and Gemini?",Bidirectional models,Encoder-decoder models,Decoder-only models,Transformer models,C) Decoder-only models
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",Which of the following is true about causal LLMs?,They can condition on future words during generation.,They generate words in a left-to-right manner.,They are also known as encoder-decoder models.,They are less effective for sequence prediction tasks.,B) They generate words in a left-to-right manner.
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11","In the context of sentiment analysis using a language model, what does the model predict to determine the sentiment of a sentence?",The entire sentence again,The most probable next word after the given prefix,The grammatical structure of the sentence,The emotional tone of the sentence,B) The most probable next word after the given prefix
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",How does a language model trained for text completion differ from an encoder model in terms of handling input data?,It can only process fixed-length inputs.,"It generates output sequentially, one token at a time.",It requires all future tokens to be known beforehand.,It cannot handle variable-length inputs.,"B) It generates output sequentially, one token at a time."
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11","Which task can be effectively addressed by casting it as a word prediction problem, as described in the lecture?",Speech recognition,Image captioning,Question answering,Object detection,C) Question answering
notes/LLM_cs124_week7_2025.pdf,5,"9, 10, 11",What is the primary advantage of using a decoder-only model for tasks like text summarization?,It can directly output the final summary without generating intermediate tokens.,It can efficiently handle bidirectional contexts.,It requires less computational resources.,It can condition on future words while generating the summary.,A) It can directly output the final summary without generating intermediate tokens.
notes/LLM_cs124_week7_2025.pdf,7,"13, 14, 15",Which component is essential for computing the probability of a word following a given prefix in language modeling?,Stacks of neural networks,Random forest,Support vector machine,K-means clustering,A) Stacks of neural networks
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Which of the following best describes the core innovation of the Transformer architecture presented in the paper?,"It relies exclusively on attention mechanisms, eliminating recurrence and convolutions.",It uses complex recurrent networks to process sequences.,It primarily focuses on convolutional neural networks.,It incorporates both attention and convolutional layers.,"A) It relies exclusively on attention mechanisms, eliminating recurrence and convolutions."
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17","According to the paper, what was the performance improvement of the Transformer model on the WMT 2014 English-to-German translation task compared to the previous best results?",2 BLEU points,10 BLEU points,15 BLEU points,28 BLEU points,A) 2 BLEU points
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",Which of the following tasks did the authors demonstrate the generalizability of the Transformer model by applying it to besides machine translation?,Image classification,English constituency parsing,Speech recognition,Natural language generation,B) English constituency parsing
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17",What is the significance of the scaled dot-product attention mechanism in the Transformer architecture?,It simplifies the model by removing the need for positional encoding.,It reduces the computational complexity and helps prevent the issues of vanishing gradients.,It allows the model to focus on different positions in the input sequence based on relevance.,It increases the number of parameters in the model.,C) It allows the model to focus on different positions in the input sequence based on relevance.
notes/LLM_cs124_week7_2025.pdf,8,"15, 16, 17","Based on the paper, which of the following statements about the Transformer model is true regarding its training efficiency?",It requires significantly less time to train compared to previous models.,It takes longer to train due to its complex architecture.,It needs more data to achieve good performance.,It cannot be trained using GPUs.,A) It requires significantly less time to train compared to previous models.
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",In which year did static word embeddings first appear according to the timeline provided?,1990,2003,2008,2012,A) 1990
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which technology saw significant growth after 2012 according to the timeline?,Static Word Embeddings,Neural Language Model,GPUs,Multi-Task Learning,C) GPUs
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",What is the role of 'Input Encoding' in the transformer architecture described?,Decodes the output sequence,Encodes input tokens into a format suitable for processing,Generates logits directly from input tokens,Performs attention mechanism,B) Encodes input tokens into a format suitable for processing
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",Which of the following appeared in 2017 and fundamentally changed how models process text?,Static Word Embeddings,Attention,Transformer,Contextual Word Embeddings and Pretraining,C) Transformer
notes/LLM_cs124_week7_2025.pdf,9,"17, 18, 19",The term 'Next token' in the context of transformers refers to what aspect of the model?,The next layer in the transformer architecture,The next token the model predicts,The next embedding to be processed,The next step in the training algorithm,B) The next token the model predicts
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",What issue does the lecture highlight with static embeddings like word2vec?,They are too complex to compute.,They do not change with context.,They require too much memory.,They are computationally expensive.,B) They do not change with context.
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21","According to the lecture, what is the key advantage of contextual embeddings over static embeddings?",They are easier to train.,They can represent different meanings of a word in different contexts.,They use less computational resources.,They are pre-trained models.,B) They can represent different meanings of a word in different contexts.
notes/LLM_cs124_week7_2025.pdf,10,"19, 20, 21",Which technique is mentioned in the lecture as a method to compute contextual embeddings?,Clustering,Attention,Regression,K-means,B) Attention
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",What does the term 'contextual embedding' imply about the representation of words?,Each word has a single fixed vector regardless of its context.,Each word has a different vector that changes based on its context.,Words do not need vectors since their meanings are constant.,"Contextual embeddings only apply to nouns, not other parts of speech.",B) Each word has a different vector that changes based on its context.
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23","In the example sentence 'The chicken didn't cross the road because it [was too tired / was too wide]', what is the primary factor influencing the choice of the word 'it'?",The grammatical structure of the sentence.,The meaning of the surrounding words.,The length of the words 'too tired' versus 'too wide'.,The number of syllables in 'chicken' and 'road'.,B) The meaning of the surrounding words.
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23",Which of the following best describes how attention works in the context of contextual embeddings?,All words contribute equally to the meaning of a target word.,Words attend to some neighboring words more than others to build a contextual embedding.,Attention is used to assign random weights to words for simplicity.,Attention only considers the first and last words in a sentence.,B) Words attend to some neighboring words more than others to build a contextual embedding.
notes/LLM_cs124_week7_2025.pdf,11,"21, 22, 23","In the phrase 'At this point in the sentence, it’s probably referring to either the chicken or the street', which aspect of contextual embeddings is being highlighted?",The static nature of word meanings.,The dynamic adjustment of word meanings based on context.,The irrelevance of context in determining word meaning.,The uniformity of word vectors across different sentences.,B) The dynamic adjustment of word meanings based on context.
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25","In the context of attention mechanisms, why might a word 'attend' more to certain neighboring words rather than others?",To ignore irrelevant information,To enhance the embedding with relevant contextual information,To reduce computational complexity,To increase the length of the embedding vector,B) To enhance the embedding with relevant contextual information
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",Which of the following best describes the self-attention distribution in Layer k+1 according to the lecture?,A random selection of tokens from Layer k,A weighted sum of vectors from Layer k,An equal distribution of tokens across all layers,A fixed set of tokens unrelated to Layer k,B) A weighted sum of vectors from Layer k
notes/LLM_cs124_week7_2025.pdf,12,"23, 24, 25",How does the attention mechanism contribute to computing the embedding for a token at a particular layer?,By directly copying the previous layer's embedding,By averaging all the tokens' embeddings,By selectively integrating information from surrounding tokens,By randomly generating new information,C) By selectively integrating information from surrounding tokens
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",What is the role of the dot product in the self-attention mechanism?,To compute the similarity between two word vectors,To determine the next word in the sequence,To update the word vectors directly,To calculate the loss function,A) To compute the similarity between two word vectors
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",Which formula represents the score calculation in the self-attention mechanism?,"score(xi,xj) = xi · xj","score(xi,xj) = |xi - xj|","score(xi,xj) = sigmoid(xi · xj)","score(xi,xj) = max(xi, xj)","A) score(xi,xj) = xi · xj"
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27",What does the softmax function do in the context of self-attention?,It normalizes the scores into a probability distribution,It updates the word embeddings,It calculates the final output sequence,It determines the next word to predict,A) It normalizes the scores into a probability distribution
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27","How are the query, key, and value vectors derived from the input vectors in transformers?",By adding the input vectors,By multiplying the input vectors with weight matrices,By concatenating the input vectors,By dividing the input vectors,B) By multiplying the input vectors with weight matrices
notes/LLM_cs124_week7_2025.pdf,13,"25, 26, 27","What is the primary reason for introducing query, key, and value vectors in the self-attention mechanism?",To increase the computational complexity,To capture different roles in the attention process,To reduce the dimensionality of the input vectors,To simplify the attention mechanism,B) To capture different roles in the attention process
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29",What is the result of the dot product between two word vectors in the context of self-attention?,A scalar value indicating the similarity between the vectors,A binary value indicating whether the vectors are identical,A vector of the same dimensionality as the input vectors,A list of indices corresponding to similar words,A) A scalar value indicating the similarity between the vectors
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29",Which of the following best describes the role of the softmax function in the self-attention mechanism?,It projects the input vectors into a higher-dimensional space,It normalizes the scores to produce a probability distribution over the inputs,It computes the exact similarity score between two words,It updates the weights of the neural network,B) It normalizes the scores to produce a probability distribution over the inputs
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29","In the self-attention mechanism, what is the final output for a given input vector computed as?",The average of all input vectors,"A weighted sum of the value vectors, where weights are determined by the softmax function",The sum of all input vectors,The dot product of the query and key vectors,"B) A weighted sum of the value vectors, where weights are determined by the softmax function"
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29",What are the three roles each input embedding plays during the course of the attention process in transformers?,"Query, Key, and Value","Input, Output, and Context","Encoder, Decoder, and Transformer","Query, Value, and Context","A) Query, Key, and Value"
notes/LLM_cs124_week7_2025.pdf,14,"27, 28, 29",How does the dimensionality of the key and value vectors differ in the transformer architecture?,They are the same,Key vectors are twice the size of value vectors,Value vectors are twice the size of key vectors,They are independent and can be different,D) They are independent and can be different
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",Which of the following best describes the role of the 'query' matrix in the attention mechanism?,It projects each input vector into a representation of its role as a preceding input.,It projects each input vector into a representation of its role as the current element being compared to preceding inputs.,It projects each input vector into a representation of its role as a value of a preceding element.,It scales the dot product between the current element's key vector and the preceding element's query vector.,B) It projects each input vector into a representation of its role as the current element being compared to preceding inputs.
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33","In the simplified attention approach, why is the softmax weight likely to be highest for the current element xi itself?",Because xi is always the most important element.,"Because xi is very similar to itself, resulting in a high dot product.",Because xi has the largest projection values.,Because xi is the only element that has a key role.,"B) Because xi is very similar to itself, resulting in a high dot product."
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",How does the transformer address the potential numerical issues with large dot products in the attention mechanism?,By using a different similarity measure.,By scaling the dot product by the square root of the dimensionality of the query and key vectors.,By increasing the embedding size.,By removing the softmax normalization step.,B) By scaling the dot product by the square root of the dimensionality of the query and key vectors.
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33",Which equation correctly represents the computation of the weighted sum of value vectors to obtain the output for the current element?,"aij = softmax(score(xi, xj))",ai = Xj≤i aij vi,"score(xi, xj) = qi · kj pdk",qi = xiWQ; ki = xiWK; vi = xiWV,B) ai = Xj≤i aij vi
notes/LLM_cs124_week7_2025.pdf,16,"31, 32, 33","If the dimensionality of the query and key vectors is 64, what would be the denominator in the scaled dot-product formula?",sqrt(64),64,128,32,A) sqrt(64)
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",How does attention in transformers affect the representation of words?,It makes the representation of words static and context-independent.,It enriches the representation of words by incorporating contextual information.,It reduces the dimensionality of the word embeddings.,It eliminates the need for word embeddings altogether.,B) It enriches the representation of words by incorporating contextual information.
notes/LLM_cs124_week7_2025.pdf,19,"37, 38, 39",What happens to the embedding of a word when it passes through multiple layers of a transformer model?,It remains unchanged throughout the layers.,It becomes more general and less context-specific.,It gets updated with new contextual information at each layer.,It is discarded and reinitialized at each layer.,C) It gets updated with new contextual information at each layer.
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",What is the role of Layer Norm in the Transformer architecture?,It normalizes the input embeddings to ensure they are on the same scale.,It modifies the tokens directly before passing them to the next layer.,It applies a transformation to the multi-head attention outputs.,It normalizes the activations across the features dimension of the input tensor.,D) It normalizes the activations across the features dimension of the input tensor.
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",How does the residual connection in the Transformer architecture enhance training?,By allowing the model to learn additive relationships between layers.,By enabling parallel computation of different layers.,"By facilitating gradient flow through the network, mitigating vanishing gradients.",By reducing the computational complexity of the model.,"C) By facilitating gradient flow through the network, mitigating vanishing gradients."
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41","In the context of the Transformer architecture, what is the purpose of the Feedforward Network (FFN)?",To capture dependencies between different positions in the sequence.,To apply self-attention mechanisms across the entire sequence.,To process the output logits before the final prediction.,To introduce non-linearity and transform the representation space.,D) To introduce non-linearity and transform the representation space.
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",Which component in the Transformer architecture is responsible for capturing the context of a token from its surroundings?,Input Encoding,MultiHeadAttention,Feedforward,Layer Norm,B) MultiHeadAttention
notes/LLM_cs124_week7_2025.pdf,20,"39, 40, 41",What happens at the end of the Transformer block processing for each token in the sequence?,The token is discarded and replaced with a new one.,The token is passed through the final layer without modification.,The token is normalized and then passed to the next block.,The token is summed with the previous token's value.,C) The token is normalized and then passed to the next block.
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What happens to the initial embedding vector in a transformer block?,It is directly used as the final output.,It undergoes a series of transformations including layer normalization and residual connections.,It is discarded and replaced with a new embedding.,It is only used for the feedforward layer.,B) It undergoes a series of transformations including layer normalization and residual connections.
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",How is the feedforward layer defined in the context of the transformer block?,A single linear transformation followed by a non-linearity.,A fully-connected 2-layer network with ReLU activation.,A convolutional layer with multiple filters.,A recurrent neural network with hidden states.,B) A fully-connected 2-layer network with ReLU activation.
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",Which of the following best describes the role of the layer normalization in the transformer block?,It normalizes the input before it is passed through the attention and feedforward layers.,It normalizes the output after the attention and feedforward layers.,It is only applied at the beginning of the transformer block.,It is not used in the transformer architecture.,A) It normalizes the input before it is passed through the attention and feedforward layers.
notes/LLM_cs124_week7_2025.pdf,21,"41, 42, 43",What is the purpose of the residual stream in the transformer architecture?,To bypass certain layers and skip connections.,To add the input of a component to its output.,To normalize the vectors before they are processed.,To reduce the dimensionality of the input vectors.,B) To add the input of a component to its output.
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47","In the context of layer normalization, what is the first step performed on the embedding vector before normalization?",Subtracting the maximum value from each element,Calculating the mean and standard deviation of the vector,Multiplying each element by its index position,Normalizing the vector using a pre-defined scaling factor,B) Calculating the mean and standard deviation of the vector
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",Which of the following equations correctly represents the process of calculating the mean (µ) for a given embedding vector x of dimensionality d during layer normalization?,µ = ∑(xi / d) for i = 1 to d,µ = (∑(xi^2) / d) for i = 1 to d,µ = (∑(xi) / d) for i = 1 to d,µ = (∑(xi * d) / d) for i = 1 to d,C) µ = (∑(xi) / d) for i = 1 to d
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47",What role do the parameters g and b play in the final layer normalization equation LayerNorm(x)?,They are fixed values that do not change during training,They represent the gain and offset values that can be learned during training,They are used to scale and shift the input vector before normalization,They are placeholders for the mean and standard deviation of the input vector,B) They represent the gain and offset values that can be learned during training
notes/LLM_cs124_week7_2025.pdf,23,"45, 46, 47","According to the lecture, which part of the transformer block involves looking at information from other tokens, and how is this information incorporated?","Layer Norm, where information is directly added to the current token's embedding","MultiHeadAttention, where the output is added back to the current token’s embedding","Feedforward, where information from other tokens is used to modify the current token’s embedding","Residual Stream, where information from other tokens is passed through without modification","B) MultiHeadAttention, where the output is added back to the current token’s embedding"
notes/LLM_cs124_week7_2025.pdf,24,"47, 48, 49","In the context of transformer blocks, what is the purpose of combining token embeddings and positional embeddings?",To reduce the dimensionality of the input data,To capture both the semantic meaning and the position of each token in the sequence,To increase the computational complexity of the model,To facilitate parallel processing of different tokens,B) To capture both the semantic meaning and the position of each token in the sequence
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","What is the shape of the token embedding matrix E if the vocabulary size |V| is 10,000 and the dimension d is 300?",[10000 × 300],[300 × 10000],[10000 × 10000],[300 × 300],A) [10000 × 300]
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","If the positional embedding for the first token in a sentence is [0.1, -0.2, 0.3], what would be the shape of this positional embedding vector?",[1 × 3],[3 × 1],[1 × 1],[3 × 3],A) [1 × 3]
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","Given the word 'apple' is at index 5000 in the vocabulary, what will be the shape of its token embedding when extracted from the embedding matrix E?",[1 × 5000],[5000 × 5000],[1 × d],[5000 × d],C) [1 × d]
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","In the context of token embeddings, if we have a sentence 'I love to eat apples', which of the following represents the correct process of tokenization and embedding selection?",Tokenize -> Convert into vocab indices -> Select corresponding rows from E,Convert into vocab indices -> Tokenize -> Select corresponding rows from E,Select corresponding rows from E -> Tokenize -> Convert into vocab indices,Select corresponding rows from E -> Convert into vocab indices -> Tokenize,A) Tokenize -> Convert into vocab indices -> Select corresponding rows from E
notes/LLM_cs124_week7_2025.pdf,25,"49, 50, 51","If a sentence contains 4 words and each word has an embedding of shape [1 × 300], what will be the shape of the combined embeddings after adding token and positional embeddings together?",[4 × 300],[1 × 300],[4 × 600],[1 × 600],C) [4 × 600]
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",What is the process for converting the string 'Thanks for all the' into token embeddings using BPE?,"Tokenize the string, then map each token to its corresponding index in the vocabulary and obtain the embeddings by selecting the rows from the embedding matrix E.","Map each character to its corresponding index in the vocabulary, then obtain the embeddings by selecting the rows from the embedding matrix E.","Tokenize the string, then map each character to its corresponding index in the vocabulary and obtain the embeddings by selecting the rows from the embedding matrix E.",Directly use the string characters as indices in the embedding matrix E.,"A) Tokenize the string, then map each token to its corresponding index in the vocabulary and obtain the embeddings by selecting the rows from the embedding matrix E."
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",How are position embeddings learned according to the lecture?,They are randomly initialized and updated during training.,They are precomputed and fixed throughout the training process.,They are derived from the token embeddings and remain constant.,They are manually assigned before the start of the training process.,A) They are randomly initialized and updated during training.
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",What is the result of adding word embeddings and position embeddings in the context of the Transformer model?,CompositeEmbeddings,PositionEmbeddings,WordEmbeddings,TokenEmbeddings,A) CompositeEmbeddings
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53","In the given example, what would be the shape of the position embedding matrix Epos if the maximum sequence length is 10?",[1 × 10],[10 × 10],[10 × 1],[1 × 1],A) [1 × 10]
notes/LLM_cs124_week7_2025.pdf,26,"51, 52, 53",Which of the following statements best describes the role of the Transformer block in the context of the given lecture?,It is responsible for converting raw text into numerical representations through tokenization and embedding.,It processes the summed word and position embeddings to produce output representations.,It initializes the embeddings for both words and positions at the start of the training process.,It is used to compute the final predictions in a sequence-to-sequence model.,B) It processes the summed word and position embeddings to produce output representations.
notes/LLM_cs124_week7_2025.pdf,27,"53, 54, 55",How does the language modeling head transform the output of a transformer block into a probability distribution over the vocabulary?,By using an embedding matrix to map from a [1 x d] vector to a [1 x |V|] vector of logits and then applying softmax.,By directly mapping the [1 x d] vector to a [1 x |V|] probability distribution without any intermediate steps.,By using a unembedding matrix to map from a [1 x |V|] vector of logits to a [1 x d] vector and then applying softmax.,By using a softmax function alone to convert the [1 x d] vector into a probability distribution.,A) By using an embedding matrix to map from a [1 x d] vector to a [1 x |V|] vector of logits and then applying softmax.
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",What is the purpose of the unembedding matrix in the language modeling head?,To map from a [1 x |V|] vector of logits to a [1 x d] vector,To map from a [1 x d] vector to a [1 x |V|] vector of logits,To map from a [1 x |V|] vector of probabilities to a [1 x d] vector,To map from a [1 x d] vector to a [1 x |V|] vector of probabilities,B) To map from a [1 x d] vector to a [1 x |V|] vector of logits
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",Which of the following best describes the role of the softmax function in the language modeling head?,It maps a [1 x d] vector to a [1 x |V|] vector of logits.,It maps a [1 x |V|] vector of logits to a [1 x |V|] vector of probabilities.,It maps a [1 x |V|] vector of probabilities to a [1 x d] vector.,It maps a [1 x d] vector to a [1 x d] vector.,B) It maps a [1 x |V|] vector of logits to a [1 x |V|] vector of probabilities.
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57","If the embedding matrix E has a shape of [|V| × d], what is the shape of the unembedding matrix ET?",[1 x d],[1 x |V|],[d × |V|],[|V| × d],C) [d × |V|]
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57","In the context of the language modeling head, which sequence correctly represents the transformation process from input to output?",1 x d → unembedding → 1 x |V| → softmax → 1 x |V|,1 x |V| → unembedding → 1 x d → softmax → 1 x |V|,1 x |V| → embedding → 1 x d → softmax → 1 x |V|,1 x d → embedding → 1 x |V| → softmax → 1 x |V|,A) 1 x d → unembedding → 1 x |V| → softmax → 1 x |V|
notes/LLM_cs124_week7_2025.pdf,28,"55, 56, 57",Which of the following statements is true regarding the unembedding matrix ET?,Each row corresponds to a different token in the vocabulary.,Each column corresponds to a different token in the vocabulary.,Each element corresponds to a different token in the vocabulary.,Each row corresponds to a different dimension of the input vector.,B) Each column corresponds to a different token in the vocabulary.
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the shape of the unembedding matrix in relation to the embedding matrix?,Same as the embedding matrix,1 x |V|,|V| x d,d x |V|,D) d x |V|
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",Why is the unembedding layer considered a 'reverse mapping' compared to the embedding layer?,It maps from a one-hot vector to an embedding,It maps from an embedding to a one-hot vector,It maps from a probability vector to an embedding,It maps from an embedding to a probability vector,D) It maps from an embedding to a probability vector
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59","In the context of the language modeling head, what is the role of the softmax layer?",To map from an embedding to a probability vector,To map from a probability vector to an embedding,To turn logits into probabilities over the vocabulary,To turn probabilities into logits,C) To turn logits into probabilities over the vocabulary
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",What is the shape of the logits vector produced by the unembedding layer?,[|V| x 1],[1 x d],[|V| x d],[1 x |V|],D) [1 x |V|]
notes/LLM_cs124_week7_2025.pdf,29,"57, 58, 59",Which of the following best describes the function of the language modeling head in a Transformer architecture?,To encode the input sequence into embeddings,To decode the input sequence into a sequence of words,To predict the next word given the current context,To perform attention mechanism calculations,C) To predict the next word given the current context
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What is the role of the unembedding layer in the language modeling head of a transformer model?,It projects the output embedding to a logits vector.,It maps from an embedding to a probability distribution over the vocabulary.,It computes the attention scores for the current token.,It applies normalization to the transformer block output.,B) It maps from an embedding to a probability distribution over the vocabulary.
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",How does the unembedding layer relate to the embedding matrix in a transformer model?,It uses a separate set of weights to transform the output embedding to logits.,It is the same as the embedding matrix but applied in reverse.,It applies a softmax function before the dot product with the embedding matrix.,It performs a forward pass to generate the input tokens.,B) It is the same as the embedding matrix but applied in reverse.
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",What happens after the logits vector is obtained from the final transformer layer in the language modeling head?,The logits are directly used to sample the next word.,The logits are passed through a normalization layer.,The logits are used to update the input embeddings.,The logits are transformed into probabilities using a softmax function.,D) The logits are transformed into probabilities using a softmax function.
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61",Which of the following best describes the process of generating text using the language modeling head?,The model predicts the next word by sampling from the probability distribution of the logits.,The model updates the context window with each predicted word.,The model uses a fixed set of probabilities for all words in the vocabulary.,The model generates text by predicting the most frequent word in the vocabulary.,A) The model predicts the next word by sampling from the probability distribution of the logits.
notes/LLM_cs124_week7_2025.pdf,30,"59, 60, 61","In the context of the language modeling head, what is the purpose of the softmax function?",To normalize the input embeddings.,To convert the logits into a probability distribution over the vocabulary.,To compute the attention weights for the current token.,To apply a non-linear transformation to the hidden states.,B) To convert the logits into a probability distribution over the vocabulary.
notes/LLM_cs124_week7_2025.pdf,31,"61, 62, 63",Why is pretraining a transformer model on vast amounts of text considered the foundational step for its subsequent use in various tasks?,It enables the model to understand context and relationships within the text.,It significantly reduces the computational resources needed for training.,It allows the model to focus only on specific task-related data.,It guarantees the model will perform well on all future tasks without further tuning.,A) It enables the model to understand context and relationships within the text.
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the primary method used during self-supervised training of language models as described in the lecture?,Fine-tuning on specific tasks,Predicting the next word in a sequence,Classifying sentences into categories,Generating complete paragraphs from a prompt,B) Predicting the next word in a sequence
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",Which of the following best describes the output of a Language Model (LM) after pretraining according to the lecture?,A binary classification score for each word,A probability distribution over the vocabulary,A fixed value representing the likelihood of the input text,A sequence of predicted next words,B) A probability distribution over the vocabulary
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65","In the context of self-supervised training, what does the term 'self-supervised' imply about the training process?",The model is trained using external labels provided by humans,"The model is trained without any labels, only using the text itself",The model is trained on supervised datasets but without labels,The model is trained on unsupervised datasets but with labeled examples,"B) The model is trained without any labels, only using the text itself"
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65",What is the role of the 'Transformer Stack' in the context of language modeling as explained in the lecture?,To classify the sentiment of the input text,To generate the final probability distribution over the vocabulary,To preprocess the input text before feeding it into the model,To store the historical context of the conversation,B) To generate the final probability distribution over the vocabulary
notes/LLM_cs124_week7_2025.pdf,32,"63, 64, 65","According to the lecture, which of the following is a key benefit of pretraining a transformer model on a large corpus of text before applying it to new tasks?",It guarantees perfect accuracy on all downstream tasks,It enables the model to learn general language patterns that can be adapted to various tasks,It reduces the need for any further training on the specific task,It eliminates the need for any form of data preprocessing,B) It enables the model to learn general language patterns that can be adapted to various tasks
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",What does the cross-entropy loss measure in the context of language modeling?,The difference between the model's probability distribution and the target distribution,The accuracy of the model's predictions,The sum of probabilities assigned by the model to all words in the vocabulary,The number of tokens correctly predicted by the model,A) The difference between the model's probability distribution and the target distribution
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67","In the process of teacher forcing, what happens at each token position t?",The model predicts the next word without any input from previous tokens,The model sees the correct sequence of tokens up to t and uses them to predict the next word,The model only uses the context of the first token to predict subsequent words,The model ignores the correct sequence of tokens and predicts based on random selection,B) The model sees the correct sequence of tokens up to t and uses them to predict the next word
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",Which of the following best describes the purpose of the loss function in training a language model?,To ensure the model assigns a low probability to the correct word,To maximize the probability that the model assigns to the true word,To minimize the computational resources used by the model,To increase the diversity of words the model can predict,B) To maximize the probability that the model assigns to the true word
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",What is the formula for the cross-entropy loss for a single word in a sentence?,-log(p(wi)),log(p(wi)),p(wi),wi * p(wi),A) -log(p(wi))
notes/LLM_cs124_week7_2025.pdf,33,"65, 66, 67",Why is teacher forcing considered beneficial during the training of a language model?,It allows the model to predict words more accurately by using the correct previous tokens,It increases the randomness in the model’s predictions,It reduces the need for backpropagation,It decreases the computational complexity of the model,A) It allows the model to predict words more accurately by using the correct previous tokens
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69","In teacher forcing during training, what does the model use at each token position t+1?",The correct word wt+1,The predicted word from the previous step,Randomly sampled words,The average of all previous words,A) The correct word wt+1
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",Which of the following best describes the purpose of the StackedTransformerBlocks in a transformer model?,To reduce the dimensionality of input data,To process input tokens and produce logits for prediction,To filter out noise from the input data,To increase the speed of the model execution,B) To process input tokens and produce logits for prediction
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",What type of data is primarily used to train large language models like those described in the lecture?,Patent text documents,Wikipedia and news articles,Both A and B,Social media posts,C) Both A and B
notes/LLM_cs124_week7_2025.pdf,34,"67, 68, 69",Which of the following is true about the Colossal Clean Crawled Corpus (C4)?,It contains 156 million tokens,It includes filtered versions of Common Crawl data,It is a manually curated dataset,It focuses on toxic content,B) It includes filtered versions of Common Crawl data
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",Which of the following sources is most likely included in the Colossal Clean Crawled Corpus (C4)?,Patent text documents,Personal emails,Private medical records,Internal company memos,A) Patent text documents
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71","Why is filtering for boilerplate, adult content, and toxicity important during the preprocessing of web data for LLM training?",To reduce the size of the dataset,To ensure the quality and relevance of the training data,To increase the diversity of the dataset,To enhance the computational efficiency of training,B) To ensure the quality and relevance of the training data
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71","Based on the given information, which of the following statements best reflects the purpose of pretraining large language models (LLMs)?",To enable the models to perform specific tasks like translation or summarization,To allow the models to understand and generate human-like text,To make the models more secure against cyber threats,To improve the models' speed in processing queries,B) To allow the models to understand and generate human-like text
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",Which of the following examples would a language model trained on the webCommon Crawl most likely generate?,A detailed financial report,A summary of a novel,A paragraph about the weather,An academic paper,C) A paragraph about the weather
notes/LLM_cs124_week7_2025.pdf,35,"69, 70, 71",What type of data is predominantly found in the Colossal Clean Crawled Corpus (C4)?,Corporate financial reports,Wikipedia articles and news sites,Research papers and academic journals,Social media posts and forums,B) Wikipedia articles and news sites
notes/LLM_cs124_week7_2025.pdf,36,"71, 72, 73",Which of the following is a concern regarding the use of web-scraped data for pretraining large language models?,Ensuring the scraped data is free from copyrighted material,Guaranteeing the physical security of servers storing the data,Verifying the accuracy of the scraped data,Confirming the data contains only public information,A) Ensuring the scraped data is free from copyrighted material
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",What is the primary reason for using continued pre-training instead of retraining all parameters of a large language model?,It is faster and more cost-effective.,It requires less data for fine-tuning.,It allows for better preservation of pre-trained knowledge.,It ensures higher accuracy on all tasks.,A) It is faster and more cost-effective.
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75","In the context of fine-tuning large language models, what does PEFT stand for, and what is its main characteristic?",Parameter-Efficient Fine-Tuning; it freezes most parameters and trains a subset.,Post-Evaluation Fine-Tuning; it evaluates the model's performance after pre-training.,Parallel Efficient Fine-Tuning; it uses parallel processing to speed up training.,Precision Enhancement Fine-Tuning; it improves the precision of the model's predictions.,A) Parameter-Efficient Fine-Tuning; it freezes most parameters and trains a subset.
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",Which of the following methods involves adding a classification head to a language model for specific tasks like sentiment classification?,Parameter-efficient fine-tuning (PEFT),Supervised fine-tuning (SFT),Masked language modeling,Continued pre-training,B) Supervised fine-tuning (SFT)
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",What distinguishes supervised fine-tuning (SFT) from other fine-tuning methods mentioned in the lecture?,It uses unlabeled data for training.,It creates a special fine-tuning dataset by hand with supervised responses.,It focuses on improving the model's general language abilities.,It freezes the entire model and only updates the classification head.,B) It creates a special fine-tuning dataset by hand with supervised responses.
notes/LLM_cs124_week7_2025.pdf,37,"73, 74, 75",How is perplexity calculated for a model q on an unseen test set of n tokens w1:n?,Perplexity = Pq(w1:n) * 1/n,Perplexity = ns1Pq(w1:n),Perplexity = 2^(Pq(w1:n) / n),Perplexity = 1 / (n * Pq(w1:n)),D) Perplexity = 1 / (n * Pq(w1:n))
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",What does the perplexity of a language model represent in terms of its prediction of unseen text?,The probability that the model assigns to the test set.,"The inverse probability that the model assigns to the test set, normalized by the test set length.",The average log probability of the words in the test set.,The total probability of the entire test set.,"B) The inverse probability that the model assigns to the test set, normalized by the test set length."
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77","In the context of fine-tuning large language models, why is parameter-efficient fine-tuning preferred over retraining all parameters?",It is faster and less resource-intensive.,It requires more data for training.,It results in a larger final model size.,It is more prone to overfitting.,A) It is faster and less resource-intensive.
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",Which of the following methods involves training a language model to produce specific outputs based on given prompts?,Continued pre-training,Parameter-efficient fine-tuning,Supervised fine-tuning,Retraining all parameters,C) Supervised fine-tuning
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",What is the primary difference between pretraining and supervised fine-tuning in the context of large language models?,"Pretraining uses labeled data, while supervised fine-tuning uses unlabeled data.","Pretraining involves creating a special fine-tuning dataset, while supervised fine-tuning does not.","Pretraining is done on a small dataset, while supervised fine-tuning is done on a large dataset.","Pretraining builds the model, while supervised fine-tuning refines it for specific tasks.","D) Pretraining builds the model, while supervised fine-tuning refines it for specific tasks."
notes/LLM_cs124_week7_2025.pdf,38,"75, 76, 77",How does perplexity help in evaluating the quality of a language model's predictions?,By directly measuring the accuracy of the model's predictions.,"By indicating how surprised the model would be by the test set, given its probabilities.",By calculating the exact probability of the test set.,By measuring the model's computational efficiency.,"B) By indicating how surprised the model would be by the test set, given its probabilities."
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Which of the following best explains why perplexity is a useful metric for comparing language models?,It measures the energy usage of the model during training.,It indicates how well the model can predict unseen data based on its probability distribution.,It assesses the fairness of the model in terms of gendered and racial stereotypes.,It determines the size of the model in terms of parameters.,B) It indicates how well the model can predict unseen data based on its probability distribution.
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",Why is it important to use the same tokenizer when comparing the perplexity of different language models?,To ensure that the models are trained on the same number of GPUs.,To maintain consistent tokenization and avoid discrepancies in perplexity due to differences in tokenization.,To make sure that the models have similar energy usage characteristics.,To guarantee that the models are of the same size.,B) To maintain consistent tokenization and avoid discrepancies in perplexity due to differences in tokenization.
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79","If a language model has a lower perplexity on a given dataset, what does this indicate about its performance?",The model requires more energy to function.,The model has a higher probability of generating the word sequences in the dataset.,The model is larger and therefore more complex.,The model has a greater risk of being unfair in its predictions.,B) The model has a higher probability of generating the word sequences in the dataset.
notes/LLM_cs124_week7_2025.pdf,39,"77, 78, 79",What factor should be considered when evaluating the overall impact of a large language model beyond just its perplexity?,The size of the model in terms of parameters.,The energy usage and carbon footprint of the model.,The fairness benchmarks related to gender and racial stereotypes.,The length of time it takes to train the model.,B) The energy usage and carbon footprint of the model.
notes/LLM_cs124_week7_2025.pdf,40,"79, 80, 81",Which of the following best describes a potential harm of large language models as discussed in the lecture?,They can generate accurate and reliable information.,"They may produce hallucinations, providing incorrect information.",They are fully compliant with copyright laws.,They do not require any form of training data.,"B) They may produce hallucinations, providing incorrect information."
notes/LLM_cs124_week7_2025.pdf,41,"81, 82, 83",Which of the following best describes the principle of privacy in digital communication?,Ensuring that all communications are recorded for future reference,Allowing users to share their personal data freely without restrictions,Protecting the confidentiality and integrity of personal information,Guaranteeing that all users' data can be accessed by law enforcement agencies,C) Protecting the confidentiality and integrity of personal information
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",Which of the following is a common method used to spread misinformation through large language models?,Phishing,Fraud,Spamming,Dissemination of fake news,D) Dissemination of fake news
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",How might large language models contribute to phishing attacks?,By providing accurate financial advice,By generating realistic-looking emails that appear to be from legitimate sources,By predicting stock market trends,By improving cybersecurity measures,B) By generating realistic-looking emails that appear to be from legitimate sources
notes/LLM_cs124_week7_2025.pdf,42,"83, 84, 85",What is a potential harm of large language models in the context of fraud?,They can accurately predict weather patterns,They can assist in identifying and preventing fraudulent activities,They can generate misleading information that leads to financial loss,They can improve customer service by answering queries,C) They can generate misleading information that leads to financial loss
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","What is the value of y after the forward pass if the input (x1, x2) = (4, -3) and the initial weights are w1=2, w2=-1, b=1?",12,10,9,11,A) 12
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","If the true output ytrue is 10 and the computed output y is 12, what is the loss L calculated using the squared error loss function?",4,6,8,2,A) 4
notes/LLM_cs124_week7_2025.pdf,45,"89, 90, 91","In the backward pass, which of the following is a key node in the computation graph that requires gradient calculation for updating the model parameters?",x1,x2,b,ytrue,C) b
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93","In the computation graph, which node represents the output y before adding the bias b?",w1x1,w2x2,h1 + h2,ytrue - y,C) h1 + h2
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93",Which of the following correctly represents the gradient calculation step for updating the weight w1 during backpropagation?,dL/dw1 = dL/dh1 * dh1/dw1,dL/dw1 = dL/dy * dy/dh1 * dh1/dw1,dL/dw1 = dL/dh2 * dh2/dw1,dL/dw1 = dL/dy * dy/dw1,B) dL/dw1 = dL/dy * dy/dh1 * dh1/dw1
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93","If the true output ytrue is 5 and the predicted output y is 3, what is the value of the loss function L?",4,2,16,4,C) 16
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93","In the context of the computation graph, which of the following is an intermediate variable needed for the backward pass?",w1,w2,b,h1,D) h1
notes/LLM_cs124_week7_2025.pdf,46,"91, 92, 93","When constructing the computation graph for backpropagation, why are nodes h1 and h2 included instead of directly using x1 and x2?",To reduce the number of nodes in the graph,To allow for the calculation of gradients for w1 and w2 separately,To increase the complexity of the model,To make the graph more visually appealing,B) To allow for the calculation of gradients for w1 and w2 separately
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","In the multi-head attention mechanism, what is the dimensionality of the key and query embeddings for each head?",d,dk,dv,d/8,B) dk
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","If a transformer uses 8 heads (A=8) and the model dimension d is 512, what will be the dimensions of the weight matrices WQ, WK, and WV for each head?",512x512,64x64,512x64,64x512,C) 512x64
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103",What is the purpose of having multiple attention heads in a transformer model?,To reduce the computational complexity of the model,To allow the model to attend to different parts of the input for various purposes,To increase the number of parameters in the model,To simplify the attention mechanism,B) To allow the model to attend to different parts of the input for various purposes
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103","In the multi-head attention equation, what does the term 'softmax(scorec(xi,xj))' represent?",The probability distribution over all possible queries,The probability of xi being the most important query,The normalized importance scores for each query in relation to the key,The raw similarity score between xi and xj,C) The normalized importance scores for each query in relation to the key
notes/LLM_cs124_week7_2025.pdf,51,"101, 102, 103",What is the final output shape of the multi-head attention layer if the model dimension d is 512 and the number of heads A is 8?,[1 x 512],[1 x 4096],[1 x 64],[8 x 512],B) [1 x 4096]
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105","In the context of multi-head attention, what is the dimensionality of the key and query embeddings for each head?",dk,dv,d,hdv,A) dk
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the primary reason for introducing multiple attention heads in a transformer model?,To reduce the computational complexity,To allow the model to attend to different parts of the input for different purposes,To increase the number of parameters in the model,To simplify the training process,B) To allow the model to attend to different parts of the input for different purposes
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105","If a transformer model uses 8 attention heads, each with key and query embeddings of dimensionality 64, what will be the output dimensionality of the multi-head attention layer before the final projection?",512,64,4096,8,A) 512
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",What is the role of the masking matrix M in the self-attention computation of a transformer model?,To ensure that the model can attend to future tokens,To prevent the model from attending to future tokens,To increase the dimensionality of the attention scores,To reduce the dimensionality of the key and query embeddings,B) To prevent the model from attending to future tokens
notes/LLM_cs124_week7_2025.pdf,52,"103, 104, 105",How does the parallelization of the attention mechanism in a transformer model work for a single head?,By sequentially computing the attention scores for each token pair,By using a single matrix multiplication to compute all query-key comparisons simultaneously,By calculating the attention scores using a loop over the sequence length,By dividing the input sequence into smaller segments and processing them independently,B) By using a single matrix multiplication to compute all query-key comparisons simultaneously
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107","If we have an input sequence of 1024 tokens and each token's embedding dimension is 512, what would be the shape of the matrix X before parallelizing the computation of the attention mechanism?",[1024 × 512],[512 × 1024],[1024 × 1024],[512 × 512],A) [1024 × 512]
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",What transformation does the mask matrix M apply to the QK| matrix in the context of self-attention computation?,It sets all values to zero.,It sets the diagonal elements to zero.,It sets the upper-triangular portion to -∞.,It sets the lower-triangular portion to 1.,C) It sets the upper-triangular portion to -∞.
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107","In the equation A = softmax(mask(QK^T/pdk))V, what is the purpose of the mask function?",To increase the computational complexity.,To prevent the model from considering future tokens.,To enhance the embedding dimensions.,To reduce the number of parameters.,B) To prevent the model from considering future tokens.
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107",How does parallelizing the computation of the attention mechanism affect the efficiency of the transformer model?,It increases the computational cost.,It reduces the memory usage but increases the computational cost.,It allows for faster computation and better utilization of hardware resources.,It decreases the model's ability to handle longer sequences.,C) It allows for faster computation and better utilization of hardware resources.
notes/LLM_cs124_week7_2025.pdf,53,"105, 106, 107","If there are 8 attention heads in a transformer model and the embedding dimension is 512, what would be the final output dimension of the multi-head attention layer before the final linear projection?",4096,512,1024,8192,A) 4096
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",What is the purpose of the mask function in the self-attention mechanism?,To prevent the model from attending to future tokens during training.,To increase the computational complexity of the model.,To reduce the dimensionality of the input embeddings.,To parallelize the attention mechanism across different heads.,A) To prevent the model from attending to future tokens during training.
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",How does the mask function modify the QK^T matrix in the context of self-attention?,By setting all values to zero.,By setting the diagonal elements to -∞.,By setting the lower triangular elements to -∞.,By setting the upper triangular elements to -∞.,D) By setting the upper triangular elements to -∞.
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109","In the multi-head attention mechanism, what is the role of the final linear projection WO?",To compute the dot product between queries and keys.,To concatenate the outputs of all attention heads.,To transform the output back to the original dimensionality of the model.,To introduce the masking operation.,C) To transform the output back to the original dimensionality of the model.
notes/LLM_cs124_week7_2025.pdf,54,"107, 108, 109",Why is the attention mechanism considered quadratic in the length of the input sequence?,Because it requires a linear transformation for each token.,Because it involves computing dot products between all pairs of tokens.,Because it uses a masking technique to prevent looking ahead.,Because it concatenates the outputs of multiple attention heads.,B) Because it involves computing dot products between all pairs of tokens.
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",Why is masking out the future used in the self-attention mechanism of transformers?,To prevent the model from using information about future tokens during prediction,To increase the computational complexity of the model,To enable parallel processing of tokens,To reduce the number of parameters in the model,A) To prevent the model from using information about future tokens during prediction
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111","In the context of the Transformer model, what does the mask matrix Mij do when i > j?","Sets Mij to -∞, making the corresponding attention score zero","Sets Mij to 1, indicating full attention","Sets Mij to 0, indicating no attention",Leaves Mij unchanged,"A) Sets Mij to -∞, making the corresponding attention score zero"
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111","If the input sequence length is N, what is the time complexity of computing self-attention without any optimizations?",O(N),O(N^2),O(N^3),O(log N),B) O(N^2)
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111","In multi-head attention, what is the role of the final linear projection WO?",It concatenates the outputs of all heads,It computes the dot product of queries and keys,It reshapes the output to match the original input dimensions,It initializes the weights for the attention mechanism,C) It reshapes the output to match the original input dimensions
notes/LLM_cs124_week7_2025.pdf,55,"109, 110, 111",How many weight layers are required for a single attention head in a Transformer model?,1,2,3,4,C) 3
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",What is the shape of the input matrix X to the transformer block?,[N×d],[N×N],[d×d],[d×N],A) [N×d]
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",How is the positional embedding incorporated into the transformer input?,By directly adding to the token embedding,By concatenating with the token embedding,By replacing the token embedding,By applying a separate attention mechanism,A) By directly adding to the token embedding
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",What is the purpose of the MultiHeadAttention function in the transformer block?,To reduce the dimensionality of the input embeddings,To compute self-attention among the input tokens,To perform feed-forward network operations,To normalize the input embeddings,B) To compute self-attention among the input tokens
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",Which of the following best describes the operation of T3 in the given sequence of equations?,Applying the feed-forward network to the normalized output,Adding the normalized output to the original input,Computing the self-attention for the input tokens,Normalizing the sum of the input and the output of the multi-head attention,D) Normalizing the sum of the input and the output of the multi-head attention
notes/LLM_cs124_week7_2025.pdf,57,"113, 114",What is the final step in the transformer block computation according to the given equations?,Computing the multi-head attention,Applying the feed-forward network,Adding the output of the feed-forward network to the normalized output of the multi-head attention,Normalizing the sum of the input and the output of the feed-forward network,C) Adding the output of the feed-forward network to the normalized output of the multi-head attention
