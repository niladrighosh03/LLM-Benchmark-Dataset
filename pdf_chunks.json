[
  {
    "chunk_number": 1,
    "pages": [
      1,
      2,
      3
    ],
    "text": "Large Language ModelsIntroduction to Large Language Models\nLanguage models•Remember the simple n-gram language model•Assigns probabilities to sequences of words•Generate text by sampling possible next words•Is trained on counts computed from lots of text•Large language models are similar and different:•Assigns probabilities to sequences of words•Generate text by sampling possible next words•Are trained by learning to guess the next word\nLarge language models•Even through pretrained only to predict words•Learn a lot of useful language knowledge•Since training on a lot of text"
  },
  {
    "chunk_number": 2,
    "pages": [
      3,
      4,
      5
    ],
    "text": "Large language models•Even through pretrained only to predict words•Learn a lot of useful language knowledge•Since training on a lot of text\nThree architectures for large language modelsDecoders   Encoders     Encoder-decodersGPT, Claude,  BERT family,  Flan-T5, WhisperLlama    HuBERTMixtralPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?Pretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?Pretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?\nToday's lecture: decoder-only modelsWhat people usually mean when they say \"LLM\" (GPT, Claude, Gemini, etc)Also called:•Causal LLMs•Autoregressive LLMs•Left-to-right LLMs•Predict words left to rightPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?"
  },
  {
    "chunk_number": 3,
    "pages": [
      5,
      6,
      7
    ],
    "text": "Today's lecture: decoder-only modelsWhat people usually mean when they say \"LLM\" (GPT, Claude, Gemini, etc)Also called:•Causal LLMs•Autoregressive LLMs•Left-to-right LLMs•Predict words left to rightPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?\nEncodersMany varieties!•Popular: Masked Language Models (MLMs)•BERT family (what you used in PA5!)•Trained by predicting words from surrounding words on both sides•Are usually finetuned (trained on supervised data) for classification tasks.Pretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?\nEncoder-Decoders•Trained to map from one sequence to another•Very popular for:•machine translation (map from one language to another)•speech recognition (map from acoustics to words)Pretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?"
  },
  {
    "chunk_number": 4,
    "pages": [
      7,
      8,
      9
    ],
    "text": "Encoder-Decoders•Trained to map from one sequence to another•Very popular for:•machine translation (map from one language to another)•speech recognition (map from acoustics to words)Pretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?\nBig ideaMany tasks can be turned into tasks of predicting words!\nToday's lecture: decoder-only modelsWhat people usually mean when they say \"LLM\" (ChatGPT, Claude, Gemini, etc)Also called:•Causal LLMs•Autoregressive LLMs•Left-to-right LLMs•Predict words left to rightPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?"
  },
  {
    "chunk_number": 5,
    "pages": [
      9,
      10,
      11
    ],
    "text": "Today's lecture: decoder-only modelsWhat people usually mean when they say \"LLM\" (ChatGPT, Claude, Gemini, etc)Also called:•Causal LLMs•Autoregressive LLMs•Left-to-right LLMs•Predict words left to rightPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?\nThe intuition (before we see the details)1. Given the current prefix So long and thanks for all2. Use the LM to compute a probability distribution over all words in V , conditioned on all the prior wordsP(aardvark | So long and thanks for all)P(abaft | So long and thanks for all)…   3. Choose one to generateSo long and thanks for all the\nMany practical NLP tasks can be cast as word prediction!Sentiment analysis: “I like Jackie Chan”1.We give the language model this string:The sentiment of the sentence \"I like Jackie Chan\" is: 2.And see what word it thinks comes next:10.1•LARGELANGUAGEMODELS WITHTRANSFORMERS3\nPreﬁx TextCompletion Text\nEncoderTransformerBlocksSoftmax\nlongall\nandthanksforallthe\nthe…UUUnencoder layerLanguage ModelingHeadlogits\nSo\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+…\nFigure 10.1Left-to-right (also called autoregressive) text completion with transformer-based large languagemodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.word “negative” to see which is higher:P(positive|The sentiment of the sentence ‘‘I like Jackie Chan\" is:)P(negative|The sentiment of the sentence ‘‘I like Jackie Chan\" is:)If the word “positive” is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider questionanswering, in which the system is given a question (for example a question witha simple factual answer) and must give a textual answer; we introduce this task indetail in Chapter 15. We can cast the task of question answering as word predictionby giving a language model a question and a token likeA:suggesting that an answershould come next:Q: Who wrote the book ‘‘The Origin of Species\"? A:If we ask a language model to compute the probability distribution over possiblenext words given this preﬁx:P(w|Q: Who wrote the book ‘‘The Origin of Species\"? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book ‘‘The Origin of Species\"? A: Charles)we might now see thatDarwinis the most probable token, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it. Wecan cast summarization as language modeling by giving a large language model atext, and follow the text by a token liketl;dr; this token is short for something like"
  },
  {
    "chunk_number": 6,
    "pages": [
      11,
      12,
      13
    ],
    "text": "Many practical NLP tasks can be cast as word prediction!Sentiment analysis: “I like Jackie Chan”1.We give the language model this string:The sentiment of the sentence \"I like Jackie Chan\" is: 2.And see what word it thinks comes next:10.1•LARGELANGUAGEMODELS WITHTRANSFORMERS3\nPreﬁx TextCompletion Text\nEncoderTransformerBlocksSoftmax\nlongall\nandthanksforallthe\nthe…UUUnencoder layerLanguage ModelingHeadlogits\nSo\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+…\nFigure 10.1Left-to-right (also called autoregressive) text completion with transformer-based large languagemodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.word “negative” to see which is higher:P(positive|The sentiment of the sentence ‘‘I like Jackie Chan\" is:)P(negative|The sentiment of the sentence ‘‘I like Jackie Chan\" is:)If the word “positive” is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider questionanswering, in which the system is given a question (for example a question witha simple factual answer) and must give a textual answer; we introduce this task indetail in Chapter 15. We can cast the task of question answering as word predictionby giving a language model a question and a token likeA:suggesting that an answershould come next:Q: Who wrote the book ‘‘The Origin of Species\"? A:If we ask a language model to compute the probability distribution over possiblenext words given this preﬁx:P(w|Q: Who wrote the book ‘‘The Origin of Species\"? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book ‘‘The Origin of Species\"? A: Charles)we might now see thatDarwinis the most probable token, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it. Wecan cast summarization as language modeling by giving a large language model atext, and follow the text by a token liketl;dr; this token is short for something like\nFraming lots of tasks as conditional generationQA: “Who wrote The Origin of Species”1.We give the language model this string:2.And see what word it thinks comes next:Charles3.Now iterate:20CHAPTER10•TRANSFORMERS ANDLARGELANGUAGEMODELS\nPreﬁx TextCompletion Text\nInputEmbeddingsTransformerBlocksSample from Softmax\nSolongall\nandthanksforallthe\nthe…linear layer\nFigure 10.15Autoregressive text completion with transformer-based large language models.word “negative” to see which is higher:P(positive|The sentiment of the sentence “I like Jackie Chan” is:)P(negative|The sentiment of the sentence “I like Jackie Chan” is:)If the word “positive” is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider the taskof answering simple questions, a task we return to in Chapter 14. In this task thesystem is given some question and must give a textual answer. We can cast the taskof question answering as word prediction by giving a language model a question anda token likeA:suggesting that an answer should come next:Q: Who wrote the book ‘‘The Origin of Species\"? A:If we ask a language model to computeP(w|Q: Who wrote the book “The Origin of Species”? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book “The Origin of Species”? A: Charles)we might now see thatDarwinis the most probable word, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it.We can cast summarization as language modeling by giving a large language modela text, and follow the text by a token liketl;dr; this token is short for somethinglike ‘too long; don’t read’ and in recent years people often use this token, especiallyin informal work emails, when they are going to give a short summary. We canthen do conditional generation: give the language model this preﬁx, and then ask10.1•LARGELANGUAGEMODELS WITHTRANSFORMERS3\nPreﬁx TextCompletion Text\nEncoderTransformerBlocksSoftmax\nlongall\nandthanksforallthe\nthe…UUUnencoder layerLanguage ModelingHeadlogits\nSo\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+…\nFigure 10.1Left-to-right (also called autoregressive) text completion with transformer-based large languagemodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.word “negative” to see which is higher:P(positive|The sentiment of the sentence ‘‘I like Jackie Chan\" is:)P(negative|The sentiment of the sentence ‘‘I like Jackie Chan\" is:)If the word “positive” is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider questionanswering, in which the system is given a question (for example a question witha simple factual answer) and must give a textual answer; we introduce this task indetail in Chapter 15. We can cast the task of question answering as word predictionby giving a language model a question and a token likeA:suggesting that an answershould come next:Q: Who wrote the book ‘‘The Origin of Species\"? A:If we ask a language model to compute the probability distribution over possiblenext words given this preﬁx:P(w|Q: Who wrote the book ‘‘The Origin of Species\"? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book ‘‘The Origin of Species\"? A: Charles)we might now see thatDarwinis the most probable token, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it. Wecan cast summarization as language modeling by giving a large language model atext, and follow the text by a token liketl;dr; this token is short for something like10.1•LARGELANGUAGEMODELS WITHTRANSFORMERS3\nPreﬁx TextCompletion Text\nEncoderTransformerBlocksSoftmax\nlongall\nandthanksforallthe\nthe…UUUnencoder layerLanguage ModelingHeadlogits\nSo\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+…\nFigure 10.1Left-to-right (also called autoregressive) text completion with transformer-based large languagemodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.word “negative” to see which is higher:P(positive|The sentiment of the sentence ‘‘I like Jackie Chan\" is:)P(negative|The sentiment of the sentence ‘‘I like Jackie Chan\" is:)If the word “positive” is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider questionanswering, in which the system is given a question (for example a question witha simple factual answer) and must give a textual answer; we introduce this task indetail in Chapter 15. We can cast the task of question answering as word predictionby giving a language model a question and a token likeA:suggesting that an answershould come next:Q: Who wrote the book ‘‘The Origin of Species\"? A:If we ask a language model to compute the probability distribution over possiblenext words given this preﬁx:P(w|Q: Who wrote the book ‘‘The Origin of Species\"? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book ‘‘The Origin of Species\"? A: Charles)we might now see thatDarwinis the most probable token, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it. Wecan cast summarization as language modeling by giving a large language model atext, and follow the text by a token liketl;dr; this token is short for something like\nSo to do language modelingWe just need a systemThat can take a prefix string of words sAnd compute the probability that word w follows sModern LMs are built out of stacks of neural networks called transformers"
  },
  {
    "chunk_number": 7,
    "pages": [
      13,
      14,
      15
    ],
    "text": "So to do language modelingWe just need a systemThat can take a prefix string of words sAnd compute the probability that word w follows sModern LMs are built out of stacks of neural networks called transformers\nLarge Language ModelsIntroduction to Large Language Models\nTransformersIntroduction to Transformers"
  },
  {
    "chunk_number": 8,
    "pages": [
      15,
      16,
      17
    ],
    "text": "TransformersIntroduction to Transformers\nLLMs are built out of transformersTransformer: a specific kind of network architecture, like a fancier feedforward network, but based on attentionProvided proper attribution is provided, Google hereby grants permission toreproduce the tables and ﬁgures in this paper solely for use in journalistic orscholarly works.Attention Is All You NeedAshish Vaswani⇤Google Brainavaswani@google.comNoam Shazeer⇤Google Brainnoam@google.comNiki Parmar⇤Google Researchnikip@google.comJakob Uszkoreit⇤Google Researchusz@google.comLlion Jones⇤Google Researchllion@google.comAidan N. Gomez⇤†University of Torontoaidan@cs.toronto.eduŁukasz Kaiser⇤Google Brainlukaszkaiser@google.comIllia Polosukhin⇤‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder. The bestperforming models also connect the encoder and decoder through an attentionmechanism. We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely. Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring signiﬁcantlyless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature. We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.⇤Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models andhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, andefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\nA very approximate timeline1990 Static Word Embeddings2003 Neural Language Model2004-6 GPUs begin to be used2008 Multi-Task Learning2012 GPUs take off2013 Static Word Embeddings re-discovered2015 Attention2017 Transformer2018 Contextual Word Embeddings and Pretraining2019 Prompting2022 ChatGPT"
  },
  {
    "chunk_number": 9,
    "pages": [
      17,
      18,
      19
    ],
    "text": "A very approximate timeline1990 Static Word Embeddings2003 Neural Language Model2004-6 GPUs begin to be used2008 Multi-Task Learning2012 GPUs take off2013 Static Word Embeddings re-discovered2015 Attention2017 Transformer2018 Contextual Word Embeddings and Pretraining2019 Prompting2022 ChatGPT\nTransformersAttention\nInstead of starting with the big picture\nStackedTransformerBlocksSolongandthanksforlongandthanksforNext tokenall……\n…\nU\nInput tokensx1x2LanguageModelingHead\nx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+……………\nU\nU\nU\nU…logitslogitslogitslogitslogits\nStackedTransformerBlocksSolongandthanksforlongandthanksforNext tokenall……\n…\nU\nInput tokensx1x2LanguageModelingHead\nx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+……………\nU\nU\nU\nU…logitslogitslogitslogitslogitsLet's consider the embeddings for an individual word from a particular layer"
  },
  {
    "chunk_number": 10,
    "pages": [
      19,
      20,
      21
    ],
    "text": "Instead of starting with the big picture\nStackedTransformerBlocksSolongandthanksforlongandthanksforNext tokenall……\n…\nU\nInput tokensx1x2LanguageModelingHead\nx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+……………\nU\nU\nU\nU…logitslogitslogitslogitslogits\nStackedTransformerBlocksSolongandthanksforlongandthanksforNext tokenall……\n…\nU\nInput tokensx1x2LanguageModelingHead\nx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+……………\nU\nU\nU\nU…logitslogitslogitslogitslogitsLet's consider the embeddings for an individual word from a particular layer\nProblem with static embeddings (word2vec)They are static!  The embedding for a word doesn't reflect how its meaning changes in context. The chicken didn't cross the road because it was too tiredWhat is the meaning represented in the static embedding for \"it\"? \nContextual Embeddings•Intuition: a representation of meaning of a word should be different in different contexts!•Contextual Embedding: each word has a different vector that expresses different meanings depending on the surrounding words•How to compute contextual embeddings?•Attention"
  },
  {
    "chunk_number": 11,
    "pages": [
      21,
      22,
      23
    ],
    "text": "Contextual Embeddings•Intuition: a representation of meaning of a word should be different in different contexts!•Contextual Embedding: each word has a different vector that expresses different meanings depending on the surrounding words•How to compute contextual embeddings?•Attention\nContextual EmbeddingsThe chicken didn't cross the road because itWhat should be the properties of \"it\"?The chicken didn't cross the road because it was too tiredThe chicken didn't cross the road because it was too wideAt this point in the sentence, it's probably referring to either the chicken or the street \nIntuition of attentionBuild up the contextual embedding from a word by selectively integrating information from all the neighboring wordsWe say that a word \"attends to\" some neighboring words more than others"
  },
  {
    "chunk_number": 12,
    "pages": [
      23,
      24,
      25
    ],
    "text": "Intuition of attentionBuild up the contextual embedding from a word by selectively integrating information from all the neighboring wordsWe say that a word \"attends to\" some neighboring words more than others\nIntuition of attention: testThechickendidn’tcrosstheroadbecauseitwastootiredThechickendidn’tcrosstheroadbecauseitwastootiredLayer k+1Layer kself-attention distributioncolumns corresponding to input tokens\nAttention definitionA mechanism for helping compute the embedding for a token by selectively attending to and integrating information from surrounding tokens (at the previous layer).More formally: a method for doing a weighted sum of vectors."
  },
  {
    "chunk_number": 13,
    "pages": [
      25,
      26,
      27
    ],
    "text": "Attention definitionA mechanism for helping compute the embedding for a token by selectively attending to and integrating information from surrounding tokens (at the previous layer).More formally: a method for doing a weighted sum of vectors.\nAttention is left-to-right\nattentionattentionSelf-AttentionLayerattentionattentionattentiona1a2a3a4a5x3x4x5x1x2\nSimplified version of attention: a sum of prior words weighted by their similarity with the current wordGiven a sequence of token embeddings: x1 x2   x3   x4   x5   x6   x7   xiProduce: ai = a weighted sum of x1 through x7 (and xi)Weighted by their similarity to xi10.1•THETRANSFORMER:ASELF-ATTENTIONNETWORK5\nSelf-AttentionLayerx1a1\nx2a2a3a4a5\nx3x4x5Figure 10.2Information ﬂow in a causal (or masked) self-attention model. In processingeach element of the sequence, the model attends to all the inputs up to, and including, thecurrent one. Unlike RNNs, the computations at each time step are independent of all theother steps and therefore can be performed in parallel.10.1.3 Self-attention more formallyWe’ve given the intuition of self-attention (as a way to compute representations of aword at a given layer by integrating information from words at the previous layer)and we’ve deﬁned context as all the prior words in the input. Let’s now introducethe self-attention computation itself.The core intuition of attention is the idea ofcomparingan item of interest to acollection of other items in a way that reveals their relevance in the current context.In the case of self-attention for language, the set of comparisons are to other words(or tokens) within a given sequence. The result of these comparisons is then used tocompute an output sequence for the current input sequence. For example, returningto Fig.10.2, the computation ofa3is based on a set of comparisons between theinputx3and its preceding elementsx1andx2, and tox3itself.How shall we compare words to other words? Since our representations forwords are vectors, we’ll make use of our old friend thedot productthat we usedfor computing word similarity in Chapter 6, and also played a role in attention inChapter 9. Let’s refer to the result of this comparison between wordsiandjas ascore (we’ll be updating this equation to add attention to the computation of thisscore):Verson 1:score(xi,xj)=xi·xj(10.4)The result of a dot product is a scalar value ranging from\u0000•to•, the largerthe value the more similar the vectors that are being compared. Continuing with ourexample, the ﬁrst step in computingy3would be to compute three scores:x3·x1,x3·x2andx3·x3. Then to make effective use of these scores, we’ll normalize themwith a softmax to create a vector of weights,aij, that indicates the proportionalrelevance of each input to the input elementithat is the current focus of attention.aij=softmax(score(xi,xj))8ji(10.5)=exp(score(xi,xj))Pik=1exp(score(xi,xk))8ji(10.6)Of course, the softmax weight will likely be highest for the current focus elementi, sincevecxiis very similar to itself, resulting in a high dot product. But othercontext words may also be similar toi, and the softmax will also assign some weightto those words.Given the proportional scores ina, we generate an output valueaiby summing10.1•THETRANSFORMER:ASELF-ATTENTIONNETWORK5\nSelf-AttentionLayerx1a1\nx2a2a3a4a5\nx3x4x5Figure 10.2Information ﬂow in a causal (or masked) self-attention model. In processingeach element of the sequence, the model attends to all the inputs up to, and including, thecurrent one. Unlike RNNs, the computations at each time step are independent of all theother steps and therefore can be performed in parallel.10.1.3 Self-attention more formallyWe’ve given the intuition of self-attention (as a way to compute representations of aword at a given layer by integrating information from words at the previous layer)and we’ve deﬁned context as all the prior words in the input. Let’s now introducethe self-attention computation itself.The core intuition of attention is the idea ofcomparingan item of interest to acollection of other items in a way that reveals their relevance in the current context.In the case of self-attention for language, the set of comparisons are to other words(or tokens) within a given sequence. The result of these comparisons is then used tocompute an output sequence for the current input sequence. For example, returningto Fig.10.2, the computation ofa3is based on a set of comparisons between theinputx3and its preceding elementsx1andx2, and tox3itself.How shall we compare words to other words? Since our representations forwords are vectors, we’ll make use of our old friend thedot productthat we usedfor computing word similarity in Chapter 6, and also played a role in attention inChapter 9. Let’s refer to the result of this comparison between wordsiandjas ascore (we’ll be updating this equation to add attention to the computation of thisscore):Verson 1:score(xi,xj)=xi·xj(10.4)The result of a dot product is a scalar value ranging from\u0000•to•, the largerthe value the more similar the vectors that are being compared. Continuing with ourexample, the ﬁrst step in computingy3would be to compute three scores:x3·x1,x3·x2andx3·x3. Then to make effective use of these scores, we’ll normalize themwith a softmax to create a vector of weights,aij, that indicates the proportionalrelevance of each input to the input elementithat is the current focus of attention.aij=softmax(score(xi,xj))8ji(10.5)=exp(score(xi,xj))Pik=1exp(score(xi,xk))8ji(10.6)Of course, the softmax weight will likely be highest for the current focus elementi, sincevecxiis very similar to itself, resulting in a high dot product. But othercontext words may also be similar toi, and the softmax will also assign some weightto those words.Given the proportional scores ina, we generate an output valueaiby summing6CHAPTER10•TRANSFORMERS ANDLARGELANGUAGEMODELSthe inputs seen so far, each weighted by itsavalue.ai=Xjiaijxj(10.7)The steps embodied in Equations10.4through10.7represent the core of anattention-based approach: a set of comparisons to relevant items in some context,a normalization of those scores to provide a probability distribution, followed by aweighted sum using this distribution. The outputais the result of this straightfor-ward computation over the inputs.This kind of simple attention can be useful, and indeed we saw in Chapter 9how to use this simple idea of attention for LSTM-based encoder-decoder modelsfor machine translation. But transformers allow us to create a more sophisticatedway of representing how words can contribute to the representation of longer inputs.Consider the three different roles that each input embedding plays during the courseof the attention process.•Asthe current focus of attentionwhen being compared to all of the otherpreceding inputs. We’ll refer to this role as aquery.query•In its role asa preceding inputbeing compared to the current focus of atten-tion. We’ll refer to this role as akey.key•And ﬁnally, as avalueused to compute the output for the current focus ofvalueattention.To capture these three different roles, transformers introduce weight matricesWQ,WK, andWV. These weights will be used to project each input vectorxiintoa representation of its role as a key, query, or value.qi=xiWQ;ki=xiWK;vi=xiWV(10.8)The inputsxand outputsyof transformers, as well as the intermediate vectors afterthe various layers like the attention output vectora, all have the same dimensionality1⇥d. We’ll have a dimensiondkfor the key and query vectors, and a separatedimensiondvfor the value vectors. In the original transformer work (Vaswani et al.,2017),dwas 512,dkanddvwere both 64. The shapes of the transform matrices arethenWQ2Rd⇥dk,WK2Rd⇥dk, andWV2Rd⇥dv.Given these projections, the score between a current focus of attention,xi, andan element in the preceding context,xj, consists of a dot product between its queryvectorqiand the preceding element’s key vectorskj. This dot product has the rightshape since both the query and the key are of dimensionality 1⇥dk. Let’s updateour previous comparison calculation to reﬂect this, replacing Eq.10.4with Eq.10.9:Verson 2:score(xi,xj)=qi·kj(10.9)The ensuing softmax calculation resulting inai,jremains the same, but the outputcalculation foraiis now based on a weighted sum over the value vectorsv.ai=Xjiaijvj(10.10)Again, the softmax weightaijwill likely be highest for the current focus elementi, and so the value foryiwill be most inﬂuenced byvi. But the model will also payattention to other contextual words if they are similar toi, allowing their values to"
  },
  {
    "chunk_number": 14,
    "pages": [
      27,
      28,
      29
    ],
    "text": "Simplified version of attention: a sum of prior words weighted by their similarity with the current wordGiven a sequence of token embeddings: x1 x2   x3   x4   x5   x6   x7   xiProduce: ai = a weighted sum of x1 through x7 (and xi)Weighted by their similarity to xi10.1•THETRANSFORMER:ASELF-ATTENTIONNETWORK5\nSelf-AttentionLayerx1a1\nx2a2a3a4a5\nx3x4x5Figure 10.2Information ﬂow in a causal (or masked) self-attention model. In processingeach element of the sequence, the model attends to all the inputs up to, and including, thecurrent one. Unlike RNNs, the computations at each time step are independent of all theother steps and therefore can be performed in parallel.10.1.3 Self-attention more formallyWe’ve given the intuition of self-attention (as a way to compute representations of aword at a given layer by integrating information from words at the previous layer)and we’ve deﬁned context as all the prior words in the input. Let’s now introducethe self-attention computation itself.The core intuition of attention is the idea ofcomparingan item of interest to acollection of other items in a way that reveals their relevance in the current context.In the case of self-attention for language, the set of comparisons are to other words(or tokens) within a given sequence. The result of these comparisons is then used tocompute an output sequence for the current input sequence. For example, returningto Fig.10.2, the computation ofa3is based on a set of comparisons between theinputx3and its preceding elementsx1andx2, and tox3itself.How shall we compare words to other words? Since our representations forwords are vectors, we’ll make use of our old friend thedot productthat we usedfor computing word similarity in Chapter 6, and also played a role in attention inChapter 9. Let’s refer to the result of this comparison between wordsiandjas ascore (we’ll be updating this equation to add attention to the computation of thisscore):Verson 1:score(xi,xj)=xi·xj(10.4)The result of a dot product is a scalar value ranging from\u0000•to•, the largerthe value the more similar the vectors that are being compared. Continuing with ourexample, the ﬁrst step in computingy3would be to compute three scores:x3·x1,x3·x2andx3·x3. Then to make effective use of these scores, we’ll normalize themwith a softmax to create a vector of weights,aij, that indicates the proportionalrelevance of each input to the input elementithat is the current focus of attention.aij=softmax(score(xi,xj))8ji(10.5)=exp(score(xi,xj))Pik=1exp(score(xi,xk))8ji(10.6)Of course, the softmax weight will likely be highest for the current focus elementi, sincevecxiis very similar to itself, resulting in a high dot product. But othercontext words may also be similar toi, and the softmax will also assign some weightto those words.Given the proportional scores ina, we generate an output valueaiby summing10.1•THETRANSFORMER:ASELF-ATTENTIONNETWORK5\nSelf-AttentionLayerx1a1\nx2a2a3a4a5\nx3x4x5Figure 10.2Information ﬂow in a causal (or masked) self-attention model. In processingeach element of the sequence, the model attends to all the inputs up to, and including, thecurrent one. Unlike RNNs, the computations at each time step are independent of all theother steps and therefore can be performed in parallel.10.1.3 Self-attention more formallyWe’ve given the intuition of self-attention (as a way to compute representations of aword at a given layer by integrating information from words at the previous layer)and we’ve deﬁned context as all the prior words in the input. Let’s now introducethe self-attention computation itself.The core intuition of attention is the idea ofcomparingan item of interest to acollection of other items in a way that reveals their relevance in the current context.In the case of self-attention for language, the set of comparisons are to other words(or tokens) within a given sequence. The result of these comparisons is then used tocompute an output sequence for the current input sequence. For example, returningto Fig.10.2, the computation ofa3is based on a set of comparisons between theinputx3and its preceding elementsx1andx2, and tox3itself.How shall we compare words to other words? Since our representations forwords are vectors, we’ll make use of our old friend thedot productthat we usedfor computing word similarity in Chapter 6, and also played a role in attention inChapter 9. Let’s refer to the result of this comparison between wordsiandjas ascore (we’ll be updating this equation to add attention to the computation of thisscore):Verson 1:score(xi,xj)=xi·xj(10.4)The result of a dot product is a scalar value ranging from\u0000•to•, the largerthe value the more similar the vectors that are being compared. Continuing with ourexample, the ﬁrst step in computingy3would be to compute three scores:x3·x1,x3·x2andx3·x3. Then to make effective use of these scores, we’ll normalize themwith a softmax to create a vector of weights,aij, that indicates the proportionalrelevance of each input to the input elementithat is the current focus of attention.aij=softmax(score(xi,xj))8ji(10.5)=exp(score(xi,xj))Pik=1exp(score(xi,xk))8ji(10.6)Of course, the softmax weight will likely be highest for the current focus elementi, sincevecxiis very similar to itself, resulting in a high dot product. But othercontext words may also be similar toi, and the softmax will also assign some weightto those words.Given the proportional scores ina, we generate an output valueaiby summing6CHAPTER10•TRANSFORMERS ANDLARGELANGUAGEMODELSthe inputs seen so far, each weighted by itsavalue.ai=Xjiaijxj(10.7)The steps embodied in Equations10.4through10.7represent the core of anattention-based approach: a set of comparisons to relevant items in some context,a normalization of those scores to provide a probability distribution, followed by aweighted sum using this distribution. The outputais the result of this straightfor-ward computation over the inputs.This kind of simple attention can be useful, and indeed we saw in Chapter 9how to use this simple idea of attention for LSTM-based encoder-decoder modelsfor machine translation. But transformers allow us to create a more sophisticatedway of representing how words can contribute to the representation of longer inputs.Consider the three different roles that each input embedding plays during the courseof the attention process.•Asthe current focus of attentionwhen being compared to all of the otherpreceding inputs. We’ll refer to this role as aquery.query•In its role asa preceding inputbeing compared to the current focus of atten-tion. We’ll refer to this role as akey.key•And ﬁnally, as avalueused to compute the output for the current focus ofvalueattention.To capture these three different roles, transformers introduce weight matricesWQ,WK, andWV. These weights will be used to project each input vectorxiintoa representation of its role as a key, query, or value.qi=xiWQ;ki=xiWK;vi=xiWV(10.8)The inputsxand outputsyof transformers, as well as the intermediate vectors afterthe various layers like the attention output vectora, all have the same dimensionality1⇥d. We’ll have a dimensiondkfor the key and query vectors, and a separatedimensiondvfor the value vectors. In the original transformer work (Vaswani et al.,2017),dwas 512,dkanddvwere both 64. The shapes of the transform matrices arethenWQ2Rd⇥dk,WK2Rd⇥dk, andWV2Rd⇥dv.Given these projections, the score between a current focus of attention,xi, andan element in the preceding context,xj, consists of a dot product between its queryvectorqiand the preceding element’s key vectorskj. This dot product has the rightshape since both the query and the key are of dimensionality 1⇥dk. Let’s updateour previous comparison calculation to reﬂect this, replacing Eq.10.4with Eq.10.9:Verson 2:score(xi,xj)=qi·kj(10.9)The ensuing softmax calculation resulting inai,jremains the same, but the outputcalculation foraiis now based on a weighted sum over the value vectorsv.ai=Xjiaijvj(10.10)Again, the softmax weightaijwill likely be highest for the current focus elementi, and so the value foryiwill be most inﬂuenced byvi. But the model will also payattention to other contextual words if they are similar toi, allowing their values to\nIntuition of attention: test\nx1  x2  x3  x4  x5  x6  x7   xiThechickendidn’tcrosstheroadbecauseitwastootiredThechickendidn’tcrosstheroadbecauseitwastootiredLayer k+1Layer kself-attention distributioncolumns corresponding to input tokens\nAn Actual Attention Head: slightly more complicatedHigh-level idea: instead of using vectors (like xi and x4) directly, we'll represent 3 separate roles each vector xi plays:•query: As the current element being compared to the preceding inputs. •key: as a preceding input that is being compared to the current element to determine a similarity•value: a value of a preceding element that gets weighted and summed "
  },
  {
    "chunk_number": 15,
    "pages": [
      29,
      30,
      31
    ],
    "text": "An Actual Attention Head: slightly more complicatedHigh-level idea: instead of using vectors (like xi and x4) directly, we'll represent 3 separate roles each vector xi plays:•query: As the current element being compared to the preceding inputs. •key: as a preceding input that is being compared to the current element to determine a similarity•value: a value of a preceding element that gets weighted and summed \nThechickendidn’tcrosstheroadbecauseitwastootiredThechickendidn’tcrosstheroadbecauseitwastootiredLayer k+1Layer kself-attention distributioncolumns corresponding to input tokensAttention intuition\nx1  x2  x3  x4  x5  x6  x7   xiqueryvalues\nThechickendidn’tcrosstheroadbecauseitwastootiredThechickendidn’tcrosstheroadbecauseitwastootiredLayer k+1Layer kself-attention distributioncolumns corresponding to input tokensAttention intuition\nx1  x2  x3  x4  x5  x6  x7   xiqueryvalues"
  },
  {
    "chunk_number": 16,
    "pages": [
      31,
      32,
      33
    ],
    "text": "Thechickendidn’tcrosstheroadbecauseitwastootiredThechickendidn’tcrosstheroadbecauseitwastootiredLayer k+1Layer kself-attention distributioncolumns corresponding to input tokensAttention intuition\nx1  x2  x3  x4  x5  x6  x7   xiqueryvalues\nThechickendidn’tcrosstheroadbecauseitwastootiredThechickendidn’tcrosstheroadbecauseitwastootiredLayer k+1Layer kself-attention distributioncolumns corresponding to input tokensIntuition of attention: \nx1  x2  x3  x4  x5  x6  x7  xiqueryvalueskvkvkvkvkvkvkvkeyskv\nAn Actual Attention Head: slightly more complicatedWe'll use matrices to project each vector xi into a representation of its role as query, key, value:•query: WQ•key: WK•value: WV9.1•ATTENTION5the softmax weight will likely be highest forxi, sincexiis very similar to itself,resulting in a high dot product. But other context words may also be similar toi, andthe softmax will also assign some weight to those words. Then we use these weightsas theavalues in Eq.9.6to compute the weighted sum that is oura3.The simpliﬁed attention in equations9.6–9.8demonstrates the attention-basedapproach to computingai: compare thexito prior vectors, normalize those scoresinto a probability distribution used to weight the sum of the prior vector. But nowwe’re ready to remove the simpliﬁcations.A single attention head using query, key, and value matricesNow that we’veseen a simple intuition of attention, let’s introduce the actualattention head, theattention headversion of attention that’s used in transformers. (The wordheadis often used inheadtransformers to refer to speciﬁc structured layers). The attention head allows us todistinctly represent three different roles that each input embedding plays during thecourse of the attention process:•Asthe current elementbeing compared to the preceding inputs. We’ll refer tothis role as aquery.query•In its role asa preceding inputthat is being compared to the current elementto determine a similarity weight. We’ll refer to this role as akey.key•And ﬁnally, as avalueof a preceding element that gets weighted and summedvalueup to compute the output for the current element.To capture these three different roles, transformers introduce weight matricesWQ,WK, andWV. These weights will project each input vectorxiinto a represen-tation of its role as a key, query, or value:qi=xiWQ;ki=xiWK;vi=xiWV(9.9)Given these projections, when we are computing the similarity of the current ele-mentxiwith some prior elementxj, we’ll use the dot product between the currentelement’squeryvectorqiand the preceding element’skeyvectorkj. Furthermore,the result of a dot product can be an arbitrarily large (positive or negative) value, andexponentiating large values can lead to numerical issues and loss of gradients duringtraining. To avoid this, we scale the dot product by a factor related to the size of theembeddings, via diving by the square root of the dimensionality of the query andkey vectors (dk). We thus replace the simpliﬁed Eq.9.7with Eq.9.11. The ensuingsoftmax calculation resulting inaijremains the same, but the output calculation foraiis now based on a weighted sum over the value vectorsv(Eq.9.13).Here’s a ﬁnal set of equations for computing self-attention for a single self-attention output vectoraifrom a single input vectorxi. This version of attentioncomputesaiby summing thevaluesof the prior elements, each weighted by thesimilarity of itskeyto thequeryfrom the current element:qi=xiWQ;kj=xjWK;vj=xjWV(9.10)score(xi,xj)=qi·kjpdk(9.11)aij=softmax(score(xi,xj))8ji(9.12)ai=Xjiaijvj(9.13)We illustrate this in Fig.9.4for the case of calculating the value of the third outputa3in a sequence."
  },
  {
    "chunk_number": 17,
    "pages": [
      33,
      34,
      35
    ],
    "text": "An Actual Attention Head: slightly more complicatedWe'll use matrices to project each vector xi into a representation of its role as query, key, value:•query: WQ•key: WK•value: WV9.1•ATTENTION5the softmax weight will likely be highest forxi, sincexiis very similar to itself,resulting in a high dot product. But other context words may also be similar toi, andthe softmax will also assign some weight to those words. Then we use these weightsas theavalues in Eq.9.6to compute the weighted sum that is oura3.The simpliﬁed attention in equations9.6–9.8demonstrates the attention-basedapproach to computingai: compare thexito prior vectors, normalize those scoresinto a probability distribution used to weight the sum of the prior vector. But nowwe’re ready to remove the simpliﬁcations.A single attention head using query, key, and value matricesNow that we’veseen a simple intuition of attention, let’s introduce the actualattention head, theattention headversion of attention that’s used in transformers. (The wordheadis often used inheadtransformers to refer to speciﬁc structured layers). The attention head allows us todistinctly represent three different roles that each input embedding plays during thecourse of the attention process:•Asthe current elementbeing compared to the preceding inputs. We’ll refer tothis role as aquery.query•In its role asa preceding inputthat is being compared to the current elementto determine a similarity weight. We’ll refer to this role as akey.key•And ﬁnally, as avalueof a preceding element that gets weighted and summedvalueup to compute the output for the current element.To capture these three different roles, transformers introduce weight matricesWQ,WK, andWV. These weights will project each input vectorxiinto a represen-tation of its role as a key, query, or value:qi=xiWQ;ki=xiWK;vi=xiWV(9.9)Given these projections, when we are computing the similarity of the current ele-mentxiwith some prior elementxj, we’ll use the dot product between the currentelement’squeryvectorqiand the preceding element’skeyvectorkj. Furthermore,the result of a dot product can be an arbitrarily large (positive or negative) value, andexponentiating large values can lead to numerical issues and loss of gradients duringtraining. To avoid this, we scale the dot product by a factor related to the size of theembeddings, via diving by the square root of the dimensionality of the query andkey vectors (dk). We thus replace the simpliﬁed Eq.9.7with Eq.9.11. The ensuingsoftmax calculation resulting inaijremains the same, but the output calculation foraiis now based on a weighted sum over the value vectorsv(Eq.9.13).Here’s a ﬁnal set of equations for computing self-attention for a single self-attention output vectoraifrom a single input vectorxi. This version of attentioncomputesaiby summing thevaluesof the prior elements, each weighted by thesimilarity of itskeyto thequeryfrom the current element:qi=xiWQ;kj=xjWK;vj=xjWV(9.10)score(xi,xj)=qi·kjpdk(9.11)aij=softmax(score(xi,xj))8ji(9.12)ai=Xjiaijvj(9.13)We illustrate this in Fig.9.4for the case of calculating the value of the third outputa3in a sequence.\nAn Actual Attention Head: slightly more complicatedGiven these 3 representation of xiTo compute  similarity of current element xi with some prior element xjWe’ll use dot product between  qi and kj. And instead of summing up xj ,  we'll sum up vj9.1•ATTENTION5the softmax weight will likely be highest forxi, sincexiis very similar to itself,resulting in a high dot product. But other context words may also be similar toi, andthe softmax will also assign some weight to those words. Then we use these weightsas theavalues in Eq.9.6to compute the weighted sum that is oura3.The simpliﬁed attention in equations9.6–9.8demonstrates the attention-basedapproach to computingai: compare thexito prior vectors, normalize those scoresinto a probability distribution used to weight the sum of the prior vector. But nowwe’re ready to remove the simpliﬁcations.A single attention head using query, key, and value matricesNow that we’veseen a simple intuition of attention, let’s introduce the actualattention head, theattention headversion of attention that’s used in transformers. (The wordheadis often used inheadtransformers to refer to speciﬁc structured layers). The attention head allows us todistinctly represent three different roles that each input embedding plays during thecourse of the attention process:•Asthe current elementbeing compared to the preceding inputs. We’ll refer tothis role as aquery.query•In its role asa preceding inputthat is being compared to the current elementto determine a similarity weight. We’ll refer to this role as akey.key•And ﬁnally, as avalueof a preceding element that gets weighted and summedvalueup to compute the output for the current element.To capture these three different roles, transformers introduce weight matricesWQ,WK, andWV. These weights will project each input vectorxiinto a represen-tation of its role as a key, query, or value:qi=xiWQ;ki=xiWK;vi=xiWV(9.9)Given these projections, when we are computing the similarity of the current ele-mentxiwith some prior elementxj, we’ll use the dot product between the currentelement’squeryvectorqiand the preceding element’skeyvectorkj. Furthermore,the result of a dot product can be an arbitrarily large (positive or negative) value, andexponentiating large values can lead to numerical issues and loss of gradients duringtraining. To avoid this, we scale the dot product by a factor related to the size of theembeddings, via diving by the square root of the dimensionality of the query andkey vectors (dk). We thus replace the simpliﬁed Eq.9.7with Eq.9.11. The ensuingsoftmax calculation resulting inaijremains the same, but the output calculation foraiis now based on a weighted sum over the value vectorsv(Eq.9.13).Here’s a ﬁnal set of equations for computing self-attention for a single self-attention output vectoraifrom a single input vectorxi. This version of attentioncomputesaiby summing thevaluesof the prior elements, each weighted by thesimilarity of itskeyto thequeryfrom the current element:qi=xiWQ;kj=xjWK;vj=xjWV(9.10)score(xi,xj)=qi·kjpdk(9.11)aij=softmax(score(xi,xj))8ji(9.12)ai=Xjiaijvj(9.13)We illustrate this in Fig.9.4for the case of calculating the value of the third outputa3in a sequence.\nFinal equations for one attention head9.1•ATTENTION5the softmax weight will likely be highest forxi, sincexiis very similar to itself,resulting in a high dot product. But other context words may also be similar toi, andthe softmax will also assign some weight to those words. Then we use these weightsas theavalues in Eq.9.6to compute the weighted sum that is oura3.The simpliﬁed attention in equations9.6–9.8demonstrates the attention-basedapproach to computingai: compare thexito prior vectors, normalize those scoresinto a probability distribution used to weight the sum of the prior vector. But nowwe’re ready to remove the simpliﬁcations.A single attention head using query, key, and value matricesNow that we’veseen a simple intuition of attention, let’s introduce the actualattention head, theattention headversion of attention that’s used in transformers. (The wordheadis often used inheadtransformers to refer to speciﬁc structured layers). The attention head allows us todistinctly represent three different roles that each input embedding plays during thecourse of the attention process:•Asthe current elementbeing compared to the preceding inputs. We’ll refer tothis role as aquery.query•In its role asa preceding inputthat is being compared to the current elementto determine a similarity weight. We’ll refer to this role as akey.key•And ﬁnally, as avalueof a preceding element that gets weighted and summedvalueup to compute the output for the current element.To capture these three different roles, transformers introduce weight matricesWQ,WK, andWV. These weights will project each input vectorxiinto a represen-tation of its role as a key, query, or value:qi=xiWQ;ki=xiWK;vi=xiWV(9.9)Given these projections, when we are computing the similarity of the current ele-mentxiwith some prior elementxj, we’ll use the dot product between the currentelement’squeryvectorqiand the preceding element’skeyvectorkj. Furthermore,the result of a dot product can be an arbitrarily large (positive or negative) value, andexponentiating large values can lead to numerical issues and loss of gradients duringtraining. To avoid this, we scale the dot product by a factor related to the size of theembeddings, via dividing by the square root of the dimensionality of the query andkey vectors (dk). We thus replace the simpliﬁed Eq.9.7with Eq.9.11. The ensuingsoftmax calculation resulting inaijremains the same, but the output calculation forheadiis now based on a weighted sum over the value vectorsv(Eq.9.13).Here’s a ﬁnal set of equations for computing self-attention for a single self-attention output vectoraifrom a single input vectorxi. This version of attentioncomputesaiby summing thevaluesof the prior elements, each weighted by thesimilarity of itskeyto thequeryfrom the current element:qi=xiWQ;kj=xjWK;vj=xjWV(9.10)score(xi,xj)=qi·kjpdk(9.11)aij=softmax(score(xi,xj))8ji(9.12)headi=Xjiaijvj(9.13)ai=headiWO(9.14)"
  },
  {
    "chunk_number": 18,
    "pages": [
      35,
      36,
      37
    ],
    "text": "Final equations for one attention head9.1•ATTENTION5the softmax weight will likely be highest forxi, sincexiis very similar to itself,resulting in a high dot product. But other context words may also be similar toi, andthe softmax will also assign some weight to those words. Then we use these weightsas theavalues in Eq.9.6to compute the weighted sum that is oura3.The simpliﬁed attention in equations9.6–9.8demonstrates the attention-basedapproach to computingai: compare thexito prior vectors, normalize those scoresinto a probability distribution used to weight the sum of the prior vector. But nowwe’re ready to remove the simpliﬁcations.A single attention head using query, key, and value matricesNow that we’veseen a simple intuition of attention, let’s introduce the actualattention head, theattention headversion of attention that’s used in transformers. (The wordheadis often used inheadtransformers to refer to speciﬁc structured layers). The attention head allows us todistinctly represent three different roles that each input embedding plays during thecourse of the attention process:•Asthe current elementbeing compared to the preceding inputs. We’ll refer tothis role as aquery.query•In its role asa preceding inputthat is being compared to the current elementto determine a similarity weight. We’ll refer to this role as akey.key•And ﬁnally, as avalueof a preceding element that gets weighted and summedvalueup to compute the output for the current element.To capture these three different roles, transformers introduce weight matricesWQ,WK, andWV. These weights will project each input vectorxiinto a represen-tation of its role as a key, query, or value:qi=xiWQ;ki=xiWK;vi=xiWV(9.9)Given these projections, when we are computing the similarity of the current ele-mentxiwith some prior elementxj, we’ll use the dot product between the currentelement’squeryvectorqiand the preceding element’skeyvectorkj. Furthermore,the result of a dot product can be an arbitrarily large (positive or negative) value, andexponentiating large values can lead to numerical issues and loss of gradients duringtraining. To avoid this, we scale the dot product by a factor related to the size of theembeddings, via dividing by the square root of the dimensionality of the query andkey vectors (dk). We thus replace the simpliﬁed Eq.9.7with Eq.9.11. The ensuingsoftmax calculation resulting inaijremains the same, but the output calculation forheadiis now based on a weighted sum over the value vectorsv(Eq.9.13).Here’s a ﬁnal set of equations for computing self-attention for a single self-attention output vectoraifrom a single input vectorxi. This version of attentioncomputesaiby summing thevaluesof the prior elements, each weighted by thesimilarity of itskeyto thequeryfrom the current element:qi=xiWQ;kj=xjWK;vj=xjWV(9.10)score(xi,xj)=qi·kjpdk(9.11)aij=softmax(score(xi,xj))8ji(9.12)headi=Xjiaijvj(9.13)ai=headiWO(9.14)\n6CHAPTER9•THETRANSFORMER\n6. Sum the weighted value vectors4. Turn into 𝛼i,j weights via softmaxa3\n1. Generate key, query, value vectors2. Compare x3’s query withthe keys for x1, x2, and x38. Output of self-attention××\nx1kqvWKWQWV5. Weigh each value vector÷√dk3. Divide scalar score by √dk÷√dk÷√dk𝛼3,1𝛼3,2𝛼3,3\nx2kqvWKWQWVx3kqvWKWQWVWO\n[1 × d][1 × d][dv × d][1 × dv][1 × dv][1 × dv][1 × dv]\n[1 × dv][1 × dv][1 x dv]7. Reshape to [1 x d] \n[1 × d][1 × d]Figure 9.4Calculating the value ofa3, the third element of a sequence using causal (left-to-right) self-attention.We illustrate this in Fig.9.4for the case of calculating the value of the third outputa3in a sequence.Note that we’ve also introduced one more matrix,WO, which is right-multipliedby the attention head. This is necessary to reshape the output of the head. The inputto attentionxiand the output from attentionaiboth have the same dimensionality[1⇥d]. We often calldthemodel dimensionality, and indeed as we’ll discuss inSection9.2the outputhiof each transformer block, as well as the intermediate vec-tors inside the transformer block also have the same dimensionality[1⇥d]. Havingeverything be the same dimensionality makes the transformer very modular.So let’s talk shapes. How do we get from[1⇥d]at the input to[1⇥d]at theoutput? Let’s look at all the internal shapes. We’ll have a dimensiondkfor the keyand query vectors. The query vector and the key vector are both dimensionality1⇥dk, so we can take their dot productqi·kjto produce a scalar. We’ll have aseparate dimensiondvfor the value vectors. The transform matrixWQhas shape[d⇥dk],WKis[d⇥dk], andWVis[d⇥dv]. So the output ofheadiin equationEq.9.13is of shape[1⇥dv]. To get the desired output shape[1⇥d]we’ll need toreshape the head output, and soWOis of shape[dv⇥d]. In the original transformerwork (Vaswani et al.,2017),dwas 512,dkanddvwere both 64.Multi-head AttentionEquations9.11-9.13describe a singleattention head. Butactually, transformers use multiple attention heads. The intuition is that each headmight be attending to the context for different purposes: heads might be special-ized to represent different linguistic relationships between context elements and thecurrent token, or to look for particular kinds of patterns in the context.So inmulti-head attentionwe haveAseparate attention heads that reside inmulti-headattentionparallel layers at the same depth in a model, each with its own set of parameters thatallows the head to model different aspects of the relationships among inputs. ThusCalculating the value of a3\nSummaryAttention is a method for enriching the representation of a token by incorporating contextual informationThe result: the embedding for each word will be different in different contexts!And enriched representation can be passed up layer by layer."
  },
  {
    "chunk_number": 19,
    "pages": [
      37,
      38,
      39
    ],
    "text": "SummaryAttention is a method for enriching the representation of a token by incorporating contextual informationThe result: the embedding for each word will be different in different contexts!And enriched representation can be passed up layer by layer.\nTransformersAttention\nTransformersThe Transformer Block"
  },
  {
    "chunk_number": 20,
    "pages": [
      39,
      40,
      41
    ],
    "text": "TransformersThe Transformer Block\nStackedTransformerBlocksSolongandthanksforlongandthanksforNext tokenall……\n…\nU\nInput tokensx1x2LanguageModelingHead\nx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+……………\nU\nU\nU\nU…logitslogitslogitslogitslogitsTransformer language model\nThe residual stream: each token gets passed up and modified\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+……"
  },
  {
    "chunk_number": 21,
    "pages": [
      41,
      42,
      43
    ],
    "text": "The residual stream: each token gets passed up and modified\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+……\nWe'll need nonlinearities, so a feedforward layer\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+……8CHAPTER9•THETRANSFORMER\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1\n+……ResidualStream\nFigure 9.6The architecture of a transformer block showing theresidual stream. Thisﬁgure shows theprenormversion of the architecture, in which the layer norms happen beforethe attention and feedforward layers rather than after.components read their input from the residual stream and add their output back intothe stream.The input at the bottom of the stream is an embedding for a token, which hasdimensionalityd. This initial embedding gets passed up (byresidual connections),and is progressively added to by the other components of the transformer: theat-tention layerthat we have seen, and thefeedforward layerthat we will introduce.Before the attention and feedforward layer is a computation called thelayer norm.Thus the initial vector is passed through a layer norm and attention layer, andthe result is added back into the stream, in this case to the original input vectorxi. And then this summed vector is again passed through another layer norm and afeedforward layer, and the output of those is added back into the residual, and we’llusehito refer to the resulting output of the transformer block for tokeni. (In earlierdescriptions the residual stream was often described using a different metaphor asresidual connectionsthat add the input of a component to its output, but the residualstream is a more perspicuous way of visualizing the transformer.)We’ve already seen the attention layer, so let’s now introduce the feedforwardand layer norm computations in the context of processing a single inputxiat tokenpositioni.Feedforward layerThe feedforward layer is a fully-connected 2-layer network,i.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weightsare the same for each token positioni, but are different from layer to layer. Itis common to make the dimensionalitydffof the hidden layer of the feedforwardnetwork be larger than the model dimensionalityd. (For example in the originaltransformer model,d=512 anddff=2048.)FFN(xi)=ReLU(xiW1+b1)W2+b2(9.20)Layer NormAt two stages in the transformer block wenormalizethe vector (Baet al.,2016). This process, calledlayer norm(short for layer normalization), is onelayer norm\nLayer norm: the vector xi is normalized twice\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+……"
  },
  {
    "chunk_number": 22,
    "pages": [
      43,
      44,
      45
    ],
    "text": "Layer norm: the vector xi is normalized twice\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+……\nLayer NormLayer norm is a variation of the z-score from statistics, applied to a single vector in a hidden layer 9.2•TRANSFORMERBLOCKS9of many forms of normalization that can be used to improve training performancein deep neural networks by keeping the values of a hidden layer in a range thatfacilitates gradient-based training.Layer norm is a variation of thez-scorefrom statistics, applied to a single vec-tor in a hidden layer. That is, the term layer norm is a bit confusing; layer normisnotapplied to an entire transformer layer, but just to the embedding vector of asingle token. Thus the input to layer norm is a single vector of dimensionalitydand the output is that vector normalized, again of dimensionalityd. The ﬁrst step inlayer normalization is to calculate the mean,µ, and standard deviation,s, over theelements of the vector to be normalized. Given an embedding vectorxof dimen-sionalityd, these values are calculated as follows.µ=1ddXi=1xi(9.21)s=vuut1ddXi=1(xi\u0000µ)2(9.22)Given these values, the vector components are normalized by subtracting the meanfrom each and dividing by the standard deviation. The result of this computation isa new vector with zero mean and a standard deviation of one.ˆx=(x\u0000µ)s(9.23)Finally, in the standard implementation of layer normalization, two learnable param-eters,gandb, representing gain and offset values, are introduced.LayerNorm(x)=g(x\u0000µ)s+b(9.24)Putting it all togetherThe function computed by a transformer block can be ex-pressed by breaking it down with one equation for each component computation,usingt(of shape[1⇥d]) to stand for transformer and superscripts to demarcateeach computation inside the block:t1i=LayerNorm(xi)(9.25)t2i=MultiHeadAttention(t1i,⇥x11,···,x1N⇤)(9.26)t3i=t2i+xi(9.27)t4i=LayerNorm(t3i)(9.28)t5i=FFN(t4i)(9.29)hi=t5i+t3i(9.30)Notice that the only component that takes as input information from other tokens(other residual streams) is multi-head attention, which (as we see from (9.27)) looksat all the neighboring tokens in the context. The output from attention, however, isthen added into this token’s embedding stream. In fact,Elhage et al.(2021) show thatwe can view attention heads as literally moving information from the residual streamof a neighboring token into the current stream. The high-dimensional embeddingspace at each position thus contains information about the current token and aboutneighboring tokens, albeit in different subspaces of the vector space. Fig.9.7showsa visualization of this movement.9.2•TRANSFORMERBLOCKS9of many forms of normalization that can be used to improve training performancein deep neural networks by keeping the values of a hidden layer in a range thatfacilitates gradient-based training.Layer norm is a variation of thez-scorefrom statistics, applied to a single vec-tor in a hidden layer. That is, the term layer norm is a bit confusing; layer normisnotapplied to an entire transformer layer, but just to the embedding vector of asingle token. Thus the input to layer norm is a single vector of dimensionalitydand the output is that vector normalized, again of dimensionalityd. The ﬁrst step inlayer normalization is to calculate the mean,µ, and standard deviation,s, over theelements of the vector to be normalized. Given an embedding vectorxof dimen-sionalityd, these values are calculated as follows.µ=1ddXi=1xi(9.21)s=vuut1ddXi=1(xi\u0000µ)2(9.22)Given these values, the vector components are normalized by subtracting the meanfrom each and dividing by the standard deviation. The result of this computation isa new vector with zero mean and a standard deviation of one.ˆx=(x\u0000µ)s(9.23)Finally, in the standard implementation of layer normalization, two learnable param-eters,gandb, representing gain and offset values, are introduced.LayerNorm(x)=g(x\u0000µ)s+b(9.24)Putting it all togetherThe function computed by a transformer block can be ex-pressed by breaking it down with one equation for each component computation,usingt(of shape[1⇥d]) to stand for transformer and superscripts to demarcateeach computation inside the block:t1i=LayerNorm(xi)(9.25)t2i=MultiHeadAttention(t1i,⇥x11,···,x1N⇤)(9.26)t3i=t2i+xi(9.27)t4i=LayerNorm(t3i)(9.28)t5i=FFN(t4i)(9.29)hi=t5i+t3i(9.30)Notice that the only component that takes as input information from other tokens(other residual streams) is multi-head attention, which (as we see from (9.27)) looksat all the neighboring tokens in the context. The output from attention, however, isthen added into this token’s embedding stream. In fact,Elhage et al.(2021) show thatwe can view attention heads as literally moving information from the residual streamof a neighboring token into the current stream. The high-dimensional embeddingspace at each position thus contains information about the current token and aboutneighboring tokens, albeit in different subspaces of the vector space. Fig.9.7showsa visualization of this movement.9.2•TRANSFORMERBLOCKS9of many forms of normalization that can be used to improve training performancein deep neural networks by keeping the values of a hidden layer in a range thatfacilitates gradient-based training.Layer norm is a variation of thez-scorefrom statistics, applied to a single vec-tor in a hidden layer. That is, the term layer norm is a bit confusing; layer normisnotapplied to an entire transformer layer, but just to the embedding vector of asingle token. Thus the input to layer norm is a single vector of dimensionalitydand the output is that vector normalized, again of dimensionalityd. The ﬁrst step inlayer normalization is to calculate the mean,µ, and standard deviation,s, over theelements of the vector to be normalized. Given an embedding vectorxof dimen-sionalityd, these values are calculated as follows.µ=1ddXi=1xi(9.21)s=vuut1ddXi=1(xi\u0000µ)2(9.22)Given these values, the vector components are normalized by subtracting the meanfrom each and dividing by the standard deviation. The result of this computation isa new vector with zero mean and a standard deviation of one.ˆx=(x\u0000µ)s(9.23)Finally, in the standard implementation of layer normalization, two learnable param-eters,gandb, representing gain and offset values, are introduced.LayerNorm(x)=g(x\u0000µ)s+b(9.24)Putting it all togetherThe function computed by a transformer block can be ex-pressed by breaking it down with one equation for each component computation,usingt(of shape[1⇥d]) to stand for transformer and superscripts to demarcateeach computation inside the block:t1i=LayerNorm(xi)(9.25)t2i=MultiHeadAttention(t1i,⇥x11,···,x1N⇤)(9.26)t3i=t2i+xi(9.27)t4i=LayerNorm(t3i)(9.28)t5i=FFN(t4i)(9.29)hi=t5i+t3i(9.30)Notice that the only component that takes as input information from other tokens(other residual streams) is multi-head attention, which (as we see from (9.27)) looksat all the neighboring tokens in the context. The output from attention, however, isthen added into this token’s embedding stream. In fact,Elhage et al.(2021) show thatwe can view attention heads as literally moving information from the residual streamof a neighboring token into the current stream. The high-dimensional embeddingspace at each position thus contains information about the current token and aboutneighboring tokens, albeit in different subspaces of the vector space. Fig.9.7showsa visualization of this movement.\nPutting together a single transformer block9.2•TRANSFORMERBLOCKS9of many forms of normalization that can be used to improve training performancein deep neural networks by keeping the values of a hidden layer in a range thatfacilitates gradient-based training.Layer norm is a variation of thez-scorefrom statistics, applied to a single vec-tor in a hidden layer. That is, the term layer norm is a bit confusing; layer normisnotapplied to an entire transformer layer, but just to the embedding vector of asingle token. Thus the input to layer norm is a single vector of dimensionalitydand the output is that vector normalized, again of dimensionalityd. The ﬁrst step inlayer normalization is to calculate the mean,µ, and standard deviation,s, over theelements of the vector to be normalized. Given an embedding vectorxof dimen-sionalityd, these values are calculated as follows.µ=1ddXi=1xi(9.21)s=vuut1ddXi=1(xi\u0000µ)2(9.22)Given these values, the vector components are normalized by subtracting the meanfrom each and dividing by the standard deviation. The result of this computation isa new vector with zero mean and a standard deviation of one.ˆx=(x\u0000µ)s(9.23)Finally, in the standard implementation of layer normalization, two learnable param-eters,gandb, representing gain and offset values, are introduced.LayerNorm(x)=g(x\u0000µ)s+b(9.24)Putting it all togetherThe function computed by a transformer block can be ex-pressed by breaking it down with one equation for each component computation,usingt(of shape[1⇥d]) to stand for transformer and superscripts to demarcateeach computation inside the block:t1i=LayerNorm(xi)(9.25)t2i=MultiHeadAttention(t1i,⇥x11,···,x1N⇤)(9.26)t3i=t2i+xi(9.27)t4i=LayerNorm(t3i)(9.28)t5i=FFN(t4i)(9.29)hi=t5i+t3i(9.30)Notice that the only component that takes as input information from other tokens(other residual streams) is multi-head attention, which (as we see from (9.27)) looksat all the neighboring tokens in the context. The output from attention, however, isthen added into this token’s embedding stream. In fact,Elhage et al.(2021) show thatwe can view attention heads as literally moving information from the residual streamof a neighboring token into the current stream. The high-dimensional embeddingspace at each position thus contains information about the current token and aboutneighboring tokens, albeit in different subspaces of the vector space. Fig.9.7showsa visualization of this movement.Layer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+……"
  },
  {
    "chunk_number": 23,
    "pages": [
      45,
      46,
      47
    ],
    "text": "Putting together a single transformer block9.2•TRANSFORMERBLOCKS9of many forms of normalization that can be used to improve training performancein deep neural networks by keeping the values of a hidden layer in a range thatfacilitates gradient-based training.Layer norm is a variation of thez-scorefrom statistics, applied to a single vec-tor in a hidden layer. That is, the term layer norm is a bit confusing; layer normisnotapplied to an entire transformer layer, but just to the embedding vector of asingle token. Thus the input to layer norm is a single vector of dimensionalitydand the output is that vector normalized, again of dimensionalityd. The ﬁrst step inlayer normalization is to calculate the mean,µ, and standard deviation,s, over theelements of the vector to be normalized. Given an embedding vectorxof dimen-sionalityd, these values are calculated as follows.µ=1ddXi=1xi(9.21)s=vuut1ddXi=1(xi\u0000µ)2(9.22)Given these values, the vector components are normalized by subtracting the meanfrom each and dividing by the standard deviation. The result of this computation isa new vector with zero mean and a standard deviation of one.ˆx=(x\u0000µ)s(9.23)Finally, in the standard implementation of layer normalization, two learnable param-eters,gandb, representing gain and offset values, are introduced.LayerNorm(x)=g(x\u0000µ)s+b(9.24)Putting it all togetherThe function computed by a transformer block can be ex-pressed by breaking it down with one equation for each component computation,usingt(of shape[1⇥d]) to stand for transformer and superscripts to demarcateeach computation inside the block:t1i=LayerNorm(xi)(9.25)t2i=MultiHeadAttention(t1i,⇥x11,···,x1N⇤)(9.26)t3i=t2i+xi(9.27)t4i=LayerNorm(t3i)(9.28)t5i=FFN(t4i)(9.29)hi=t5i+t3i(9.30)Notice that the only component that takes as input information from other tokens(other residual streams) is multi-head attention, which (as we see from (9.27)) looksat all the neighboring tokens in the context. The output from attention, however, isthen added into this token’s embedding stream. In fact,Elhage et al.(2021) show thatwe can view attention heads as literally moving information from the residual streamof a neighboring token into the current stream. The high-dimensional embeddingspace at each position thus contains information about the current token and aboutneighboring tokens, albeit in different subspaces of the vector space. Fig.9.7showsa visualization of this movement.Layer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+……\nA transformer is a stack of these blocksso all the vectors are of the same dimensionality d\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+……Layer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+……\nBlock 1Block 2\nTransformersThe Transformer Block"
  },
  {
    "chunk_number": 24,
    "pages": [
      47,
      48,
      49
    ],
    "text": "TransformersThe Transformer Block\nTransformersInput and output: Position embeddings and the Language Model Head\nToken and Position EmbeddingsThe matrix X (of shape [N × d]) has an embedding for each word in the context. Each embedding of shape [1× d] is created by adding two distinct embedding for each input•token embedding•positional embedding"
  },
  {
    "chunk_number": 25,
    "pages": [
      49,
      50,
      51
    ],
    "text": "Token and Position EmbeddingsThe matrix X (of shape [N × d]) has an embedding for each word in the context. Each embedding of shape [1× d] is created by adding two distinct embedding for each input•token embedding•positional embedding\nEmbedding matrix EEWord 1Word 2Word 3Word 4Vocabulary VWord |V|d dimensional embedding for each word1dEmbedding matrix E has shape [|V | ×  d ]. •One row for each of the |V | tokens in the vocabulary. •Each word embedding is a row vector of shape [1 × d]\nToken EmbeddingsGiven:  string \"Thanks for all the\"1. Tokenize with BPE and convert into vocab indicesw = [5, 4000, 10532, 2224] 2. Select the corresponding rows from E, each row an embedding•  (row 5, row 4000, row 10532, row 2224). "
  },
  {
    "chunk_number": 26,
    "pages": [
      51,
      52,
      53
    ],
    "text": "Token EmbeddingsGiven:  string \"Thanks for all the\"1. Tokenize with BPE and convert into vocab indicesw = [5, 4000, 10532, 2224] 2. Select the corresponding rows from E, each row an embedding•  (row 5, row 4000, row 10532, row 2224). \nPosition EmbeddingsThere are many methods, the simplest is absolute position.Goal: learn a position embedding matrix Epos of shape [1 × N ]. Start with randomly initialized embeddings•one for each integer up to some maximum length. •i.e., just as we have an embedding for token fish, we’ll have an embedding for position 3 and position 17.•As with word embeddings, these position embeddings are learned along with other parameters during training. \nEach x is just the sum of word and position embeddings\nX = CompositeEmbeddings(word + position)Transformer BlockJanet1will2back3Janetwillbackthebillthe4bill5\n+++++PositionEmbeddingsWordEmbeddings"
  },
  {
    "chunk_number": 27,
    "pages": [
      53,
      54,
      55
    ],
    "text": "Each x is just the sum of word and position embeddings\nX = CompositeEmbeddings(word + position)Transformer BlockJanet1will2back3Janetwillbackthebillthe4bill5\n+++++PositionEmbeddingsWordEmbeddings\nLanguage Modeling HeadEach transformer block outputs  a [1 x d] vector.How do we turn it into a probability distribution over the vocabulary?  This is the job of the language modeling head•\"Head\" means \"extra circuits on top of a transformer\"•The job of the language modeling head:•Map from a [1 x d] vector•To a [1 x |V|]  probability distribution over vocab\nThe job of the language modeling head:•Map•From a [1 x d] vector•To a [1 x |V|]  probability distribution over vocabWe do this in two steps1.Use the \"unembedding matrix\" to map from [1 x d] vector to a [1 x |V|] vector of logits2.Use softmax to map from a [1 x |V|] vector of logits to a a [1 x |V|] vector of probabilities"
  },
  {
    "chunk_number": 28,
    "pages": [
      55,
      56,
      57
    ],
    "text": "The job of the language modeling head:•Map•From a [1 x d] vector•To a [1 x |V|]  probability distribution over vocabWe do this in two steps1.Use the \"unembedding matrix\" to map from [1 x d] vector to a [1 x |V|] vector of logits2.Use softmax to map from a [1 x |V|] vector of logits to a a [1 x |V|] vector of probabilities\nLanguage modeling head\nLayer LTransformerBlockSoftmax over vocabulary VUnembedding layer…1 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding    layer = ETy1y2y|V|…u1u2u|V|…Language Model Headtakes hLN and outputs adistribution over vocabulary V\nUnembedding matrix ETEmbedding matrix E has shape [|V | ×  d ]. Unembedding matrix has shape [d × |V | ]. •One column for each of the |V | tokens in the vocabulary. ET|V|1d1d1×logitshL=1|V|"
  },
  {
    "chunk_number": 29,
    "pages": [
      57,
      58,
      59
    ],
    "text": "Unembedding matrix ETEmbedding matrix E has shape [|V | ×  d ]. Unembedding matrix has shape [d × |V | ]. •One column for each of the |V | tokens in the vocabulary. ET|V|1d1d1×logitshL=1|V|\nLanguage modeling head\nLayer LTransformerBlockSoftmax over vocabulary VUnembedding layer…1 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding    layer = ETy1y2y|V|…u1u2u|V|…Language Model Headtakes hLN and outputs adistribution over vocabulary VUnembedding layer:  linear layer projects from hLN (shape [1 × d]) to logit vector Why \"unembedding\"? Tied to ETWeight tying, we use the same weights for two different matricesUnembedding layer maps from an embedding to a 1x|V| vector of logits\nLanguage modeling head\nLayer LTransformerBlockSoftmax over vocabulary VUnembedding layer…1 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding    layer = ETy1y2y|V|…u1u2u|V|…Language Model Headtakes hLN and outputs adistribution over vocabulary VLogits, the score vector uOne score for each of the |V | possible words in the vocabulary V . Shape 1 × |V |. Softmax turns the logits into probabilities over vocabulary. Shape 1 × |V |. 16CHAPTER9•THETRANSFORMERlanguage models of Chapter 3 compute the probability of a word given counts ofits occurrence with then\u00001 prior words. The context is thus of sizen\u00001. Fortransformer language models, the context is the size of the transformer’s contextwindow, which can be quite large: 2K, 4K, even 32K tokens for very large models.The job of the language modeling head is to take the output of the ﬁnal trans-former layer from the last tokenNand use it to predict the upcoming word at posi-tionN+1. Fig.9.14shows how to accomplish this task, taking the output of the lasttoken at the last layer (thed-dimensional output embedding of shape[1⇥d]) andproducing a probability distribution over words (from which we will choose one togenerate).\nLayer LTransformerBlockSoftmax over vocabulary VUnembedding layer…1 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding layerU = ETy1y2y|V|…u1u2u|V|…Language Model Headtakes hLN and outputs adistribution over vocabulary V\nFigure 9.14The language modeling head: the circuit at the top of a transformer that maps from the outputembedding for tokenNfrom the last transformer layer (hLN) to a probability distribution over words in thevocabularyV.The ﬁrst module in Fig.9.14is a linear layer, whose job is to project from theoutputhLN, which represents the output token embedding at positionNfrom the ﬁnalblockL, (hence of shape[1⇥d]) to thelogitvector, or score vector, that will have alogitsingle score for each of the|V|possible words in the vocabularyV. The logit vectoruis thus of dimensionality 1⇥|V|.This linear layer can be learned, but more commonly we tie this matrix to (thetranspose of) the embedding matrixE. Recall that inweight tying, we use theweight tyingsame weights for two different matrices in the model. Thus at the input stage of thetransformer the embedding matrix (of shape[|V|⇥d]) is used to map from a one-hotvector over the vocabulary (of shape[1⇥|V|]) to an embedding (of shape[1⇥d]).And then in the language model head,ET, the transpose of the embedding matrix (ofshape[d⇥|V|]) is used to map back from an embedding (shape[1⇥d]) to a vectorover the vocabulary (shape [1⇥|V|]). In the learning process,Ewill be optimized tobe good at doing both of these mappings. We therefore sometimes call the transposeETtheunembeddinglayer because it is performing this reverse mapping.unembeddingA softmax layer turns the logitsuinto the probabilitiesyover the vocabulary.u=hLNET(9.44)y=softmax(u)(9.45)We can use these probabilities to do things like help assign a probability to agiven text. But the most important usage to generate text, which we do bysampling"
  },
  {
    "chunk_number": 30,
    "pages": [
      59,
      60,
      61
    ],
    "text": "Language modeling head\nLayer LTransformerBlockSoftmax over vocabulary VUnembedding layer…1 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding    layer = ETy1y2y|V|…u1u2u|V|…Language Model Headtakes hLN and outputs adistribution over vocabulary VLogits, the score vector uOne score for each of the |V | possible words in the vocabulary V . Shape 1 × |V |. Softmax turns the logits into probabilities over vocabulary. Shape 1 × |V |. 16CHAPTER9•THETRANSFORMERlanguage models of Chapter 3 compute the probability of a word given counts ofits occurrence with then\u00001 prior words. The context is thus of sizen\u00001. Fortransformer language models, the context is the size of the transformer’s contextwindow, which can be quite large: 2K, 4K, even 32K tokens for very large models.The job of the language modeling head is to take the output of the ﬁnal trans-former layer from the last tokenNand use it to predict the upcoming word at posi-tionN+1. Fig.9.14shows how to accomplish this task, taking the output of the lasttoken at the last layer (thed-dimensional output embedding of shape[1⇥d]) andproducing a probability distribution over words (from which we will choose one togenerate).\nLayer LTransformerBlockSoftmax over vocabulary VUnembedding layer…1 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding layerU = ETy1y2y|V|…u1u2u|V|…Language Model Headtakes hLN and outputs adistribution over vocabulary V\nFigure 9.14The language modeling head: the circuit at the top of a transformer that maps from the outputembedding for tokenNfrom the last transformer layer (hLN) to a probability distribution over words in thevocabularyV.The ﬁrst module in Fig.9.14is a linear layer, whose job is to project from theoutputhLN, which represents the output token embedding at positionNfrom the ﬁnalblockL, (hence of shape[1⇥d]) to thelogitvector, or score vector, that will have alogitsingle score for each of the|V|possible words in the vocabularyV. The logit vectoruis thus of dimensionality 1⇥|V|.This linear layer can be learned, but more commonly we tie this matrix to (thetranspose of) the embedding matrixE. Recall that inweight tying, we use theweight tyingsame weights for two different matrices in the model. Thus at the input stage of thetransformer the embedding matrix (of shape[|V|⇥d]) is used to map from a one-hotvector over the vocabulary (of shape[1⇥|V|]) to an embedding (of shape[1⇥d]).And then in the language model head,ET, the transpose of the embedding matrix (ofshape[d⇥|V|]) is used to map back from an embedding (shape[1⇥d]) to a vectorover the vocabulary (shape [1⇥|V|]). In the learning process,Ewill be optimized tobe good at doing both of these mappings. We therefore sometimes call the transposeETtheunembeddinglayer because it is performing this reverse mapping.unembeddingA softmax layer turns the logitsuinto the probabilitiesyover the vocabulary.u=hLNET(9.44)y=softmax(u)(9.45)We can use these probabilities to do things like help assign a probability to agiven text. But the most important usage to generate text, which we do bysampling\nThe final transformermodel\nwiSample token togenerate at position i+1\nfeedforwardlayer normattentionlayer norm\nU\nInput tokenLanguageModelingHead\nInputEncoding\nEi+…logits\nfeedforwardlayer normattentionlayer normLayer 1Layer 2h1i  =  x2ix1ih2i  =  x3ifeedforwardlayer normattentionlayer normhLi  hL-1i  =  xLiy1y2y|V|…Token probabilitiesu1u2u|V|…softmaxwi+1\nLayer L\nwiSample token togenerate at position i+1\nfeedforwardlayer normattentionlayer norm\nU\nInput tokenLanguageModelingHead\nInputEncoding\nEi+…logits\nfeedforwardlayer normattentionlayer normLayer 1Layer 2h1i  =  x2ix1ih2i  =  x3ifeedforwardlayer normattentionlayer normhLi  hL-1i  =  xLiy1y2y|V|…Token probabilitiesu1u2u|V|…softmaxwi+1\nLayer L\nTransformersInput and output: Position embeddings and the Language Model Head"
  },
  {
    "chunk_number": 31,
    "pages": [
      61,
      62,
      63
    ],
    "text": "TransformersInput and output: Position embeddings and the Language Model Head\nLarge Language ModelsPretraining Large Language Models\nPretrainingThe big idea that underlies all the amazing performance of language modelsFirst pretrain a transformer model on enormous amounts of textThen apply it to new tasks."
  },
  {
    "chunk_number": 32,
    "pages": [
      63,
      64,
      65
    ],
    "text": "PretrainingThe big idea that underlies all the amazing performance of language modelsFirst pretrain a transformer model on enormous amounts of textThen apply it to new tasks.\nSelf-supervised training algorithmWe just train them to predict the next word!1.Take a corpus of text 2.At each time step t i.ask the model to predict the next word ii.train the model using gradient descent to minimize the error in this prediction\"Self-supervised\" because it just uses the next word as the label!\nThe output of the LLM: Probability distribution over the vocabulary: possible next wordsTransformerStackLanguage modeling head\nSo long and thanks for allP(aardvark)P(abaft)P(able)…P(the)…P(zebra)Correct word:theLoss function:How high is this probability"
  },
  {
    "chunk_number": 33,
    "pages": [
      65,
      66,
      67
    ],
    "text": "The output of the LLM: Probability distribution over the vocabulary: possible next wordsTransformerStackLanguage modeling head\nSo long and thanks for allP(aardvark)P(abaft)P(able)…P(the)…P(zebra)Correct word:theLoss function:How high is this probability\nCross-entropy loss•Same loss from regression + neural nets•We want to assign a high probability to true word w•= high loss if model assigns too low a probability to w•CE Loss : The negative log probability that the model assigns to the true word wt•LCE\t=\t-log\tp(wi)•CE Loss for a whole sentence:•!\"∑#$!\"−log𝑝(𝑤#)•If loss is high (model assigns too low a probability to w)•We move model weights to give higher probability to w\nTeacher forcing•At each token position t, model sees correct tokens w1:t, •Computes  loss (neg log prob) for the next token wt+1 •At next token position t+1 we ignore what model predicted for wt+1 •Instead we take the correct word wt+1, add it to context, move on"
  },
  {
    "chunk_number": 34,
    "pages": [
      67,
      68,
      69
    ],
    "text": "Teacher forcing•At each token position t, model sees correct tokens w1:t, •Computes  loss (neg log prob) for the next token wt+1 •At next token position t+1 we ignore what model predicted for wt+1 •Instead we take the correct word wt+1, add it to context, move on\nTraining a transformer language model\nlongandthanksforNext tokenallLoss…=\n<latexit sha1_base64=\"AovqpaL476UmJ1EU1xZPgDZ70tQ=\">AAAB9nicbVDLSsNAFL2pr1pfURcu3AwWwY0lEakui25cVrAPaEqYTCbt0EkmzEzEEvIrbkTcKPgZ/oJ/Y9Jm09YDA4dzznDvPV7MmdKW9WtU1tY3Nreq27Wd3b39A/PwqKtEIgntEMGF7HtYUc4i2tFMc9qPJcWhx2nPm9wXfu+ZSsVE9KSnMR2GeBSxgBGsc8k1Ty4dLkZo6qZOiPVYhimO/CyruWbdalgzoFVil6QOJdqu+eP4giQhjTThWKmBbcV6mGKpGeE0qzmJojEmEzyi6WztDJ3nko8CIfMXaTRTF3I4VGoaenmy2E0te4X4nzdIdHA7TFkUJ5pGZD4oSDjSAhUdIJ9JSjSf5gQTyfINERljiYnOmypOt5cPXSXdq4bdbDQfr+utu7KEKpzCGVyADTfQggdoQwcIZPAGn/BlvBivxrvxMY9WjPLPMSzA+P4DPEiSHA==</latexit>\u0000logyand\nStackedTransformerBlocksSolongandthanksfor……\n…\nU\nInput tokensx1x2LanguageModelingHeadx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+……………\nU\nU\nU\nU…logitslogitslogitslogitslogits…\n<latexit sha1_base64=\"q3ZgXDyG7qtkT7t8hT47RdlwYG4=\">AAAB+XicbVDLSsNAFJ3UV62vWHe6GVsEN5bERXUlBUVcVrAPaEqYTCft0MlMmJkIIQT8AT/CTRE3Cv6Ev+DfmLTdtPXAwOGcM9x7jxcyqrRl/RqFtfWNza3idmlnd2//wDwst5WIJCYtLJiQXQ8pwignLU01I91QEhR4jHS88W3ud56JVFTwJx2HpB+gIac+xUhnkmseXzhMDGHsJk6A9EgGiR4hPlZpWnLNqlWzpoCrxJ6TauP0tXw3qdw0XfPHGQgcBYRrzJBSPdsKdT9BUlPMSFpyIkVChMdoSJLp5ik8y6QB9IXMHtdwqi7kUKBUHHhZMl9PLXu5+J/Xi7R/3U8oDyNNOJ4N8iMGtYB5DXBAJcGaxRlBWNJsQ4hHSCKss7Ly0+3lQ1dJ+7Jm12v1x6yDezBDEZyACjgHNrgCDfAAmqAFMHgBE/AJvozEeDPejY9ZtGDM/xyBBRjff79pldo=</latexit>\u0000logythanks\nLLMs are mainly trained on the webCommon Crawl: snapshots of the entire web produced by the non- profit Common Crawl with billions of pagesColossal Clean Crawled Corpus (C4; Raffel et al. 2020), 156 billion tokens of English,  filtered What's in it? Mostly patent text documents, Wikipedia, and news sites It's filtered to remove boilerplate, adult content, toxicity"
  },
  {
    "chunk_number": 35,
    "pages": [
      69,
      70,
      71
    ],
    "text": "LLMs are mainly trained on the webCommon Crawl: snapshots of the entire web produced by the non- profit Common Crawl with billions of pagesColossal Clean Crawled Corpus (C4; Raffel et al. 2020), 156 billion tokens of English,  filtered What's in it? Mostly patent text documents, Wikipedia, and news sites It's filtered to remove boilerplate, adult content, toxicity\nWhat does a model learn from pretraining?•There are canines everywhere! One dog in the front room, and two dogs•It wasn't just big it was enormous•The author of \"A Room of One's Own\" is Virginia Woolf•The doctor told me that he•The square root of 4 is 2\nBig ideaText contains enormous amounts of knowledgePretraining on lots of text with all that knowledge is what gives language models their ability to do so much"
  },
  {
    "chunk_number": 36,
    "pages": [
      71,
      72,
      73
    ],
    "text": "Big ideaText contains enormous amounts of knowledgePretraining on lots of text with all that knowledge is what gives language models their ability to do so much\nBut there are problems with scraping from the webCopyright: much of the text in these datasets is copyrighted•Not clear if fair use doctrine in US allows for this use•This remains an open legal questionData consent•Website owners can indicate they don't want their site crawledPrivacy: •Websites can contain private IP addresses and phone numbers\nLarge Language ModelsPretraining Large Language Models: Algorithm and Data"
  },
  {
    "chunk_number": 37,
    "pages": [
      73,
      74,
      75
    ],
    "text": "Large Language ModelsPretraining Large Language Models: Algorithm and Data\nLarge Language ModelsEvaluating Large Language Models\nPerplexityJust as for n-gram grammars, we use perplexity to measure how well the LM predicts unseen textReminder: the perplexity of a model θ on an unseen test set is the inverse probability that θ assigns to the test set, normalized by the test set length. For a test set of n tokens w1:n the perplexity is :12CHAPTER10•LARGELANGUAGEMODELSthe pretraining data, and so you’ll sometimes see this method calledcontinued pre-training.continuedpretrainingRetraining all the parameters of the model is very slow and expensive when thelanguage model is huge. So instead we canfreezesome of the parameters (i.e., leavefreezethem unchanged from their pretrained value) and train only a subset of parameterson the new data. In Section10.5.3we’ll describe this second variety of ﬁnetun-ing, calledparameter-efﬁcient ﬁnetuning, orPEFT. because we efﬁciently selectspeciﬁc parameters to update when ﬁnetuning, and leave the rest in their pretrainedvalues.In Chapter 11 we’ll introduce a third kind of ﬁnetuning, also parameter-efﬁcient.In this version, the goal is to use a language model as a kind of classiﬁer or labelerfor a speciﬁc task. For example we might train the model to be a sentiment classiﬁer.We do this by adding extra neural circuitry (an extrahead) after the top layer of themodel. This classiﬁcation head takes as input some of the top layer embeddings ofthe transformer and produces as output a classiﬁcation. In this method, most com-monly used with masked language models like BERT, we freeze the entire pretrainedmodel and only train the classiﬁcation head on some new data, usually labeled withsome class that we want to predict.Finally, in Chapter 12 we’ll introduce a fourth kind of ﬁnetuning, that is a cru-cial component of the largest language models:supervised ﬁnetuningorSFT. SFTis often used forinstruction ﬁnetuning, in which we want a pretrained languagemodel to learn to follow text instructions, for example to answer questions or followa command to write something. Here we create a dataset of prompts and desiredresponses (for example questions and their answers, or commands and their ful-ﬁllments), and we train the language model using the normal cross-entropy loss topredict each token in the instruction prompt iteratively, essentially training it to pro-duce the desired response from the command in the prompt. It’s called supervisedbecause unlike in pretraining, where we just take any data and predict the words init, we build the special ﬁnetuning dataset by hand, creating supervised responses toeach command.Often everything that happens after pretraining is lumped together aspost-training;we’ll discuss the various parts of post-training in Chapter 12 and Chapter 13.10.4 Evaluating Large Language ModelsPerplexityAs we ﬁrst saw in Chapter 3, one way to evaluate language models isto measure how well they predict unseen text. Intuitively, good models are those thatassign higher probabilities to unseen data (are less surprised when encountering thenew words).We instantiate this intuition by usingperplexityto measure the quality of aperplexitylanguage model. Recall from page??that the perplexity of a modelqon an unseentest set is the inverse probability thatqassigns to the test set, normalized by the testset length. For a test set ofntokensw1:n, the perplexity isPerplexityq(w1:n)=Pq(w1:n)\u00001n=ns1Pq(w1:n)(10.7)To visualize how perplexity can be computed as a function of the probabilities the"
  },
  {
    "chunk_number": 38,
    "pages": [
      75,
      76,
      77
    ],
    "text": "PerplexityJust as for n-gram grammars, we use perplexity to measure how well the LM predicts unseen textReminder: the perplexity of a model θ on an unseen test set is the inverse probability that θ assigns to the test set, normalized by the test set length. For a test set of n tokens w1:n the perplexity is :12CHAPTER10•LARGELANGUAGEMODELSthe pretraining data, and so you’ll sometimes see this method calledcontinued pre-training.continuedpretrainingRetraining all the parameters of the model is very slow and expensive when thelanguage model is huge. So instead we canfreezesome of the parameters (i.e., leavefreezethem unchanged from their pretrained value) and train only a subset of parameterson the new data. In Section10.5.3we’ll describe this second variety of ﬁnetun-ing, calledparameter-efﬁcient ﬁnetuning, orPEFT. because we efﬁciently selectspeciﬁc parameters to update when ﬁnetuning, and leave the rest in their pretrainedvalues.In Chapter 11 we’ll introduce a third kind of ﬁnetuning, also parameter-efﬁcient.In this version, the goal is to use a language model as a kind of classiﬁer or labelerfor a speciﬁc task. For example we might train the model to be a sentiment classiﬁer.We do this by adding extra neural circuitry (an extrahead) after the top layer of themodel. This classiﬁcation head takes as input some of the top layer embeddings ofthe transformer and produces as output a classiﬁcation. In this method, most com-monly used with masked language models like BERT, we freeze the entire pretrainedmodel and only train the classiﬁcation head on some new data, usually labeled withsome class that we want to predict.Finally, in Chapter 12 we’ll introduce a fourth kind of ﬁnetuning, that is a cru-cial component of the largest language models:supervised ﬁnetuningorSFT. SFTis often used forinstruction ﬁnetuning, in which we want a pretrained languagemodel to learn to follow text instructions, for example to answer questions or followa command to write something. Here we create a dataset of prompts and desiredresponses (for example questions and their answers, or commands and their ful-ﬁllments), and we train the language model using the normal cross-entropy loss topredict each token in the instruction prompt iteratively, essentially training it to pro-duce the desired response from the command in the prompt. It’s called supervisedbecause unlike in pretraining, where we just take any data and predict the words init, we build the special ﬁnetuning dataset by hand, creating supervised responses toeach command.Often everything that happens after pretraining is lumped together aspost-training;we’ll discuss the various parts of post-training in Chapter 12 and Chapter 13.10.4 Evaluating Large Language ModelsPerplexityAs we ﬁrst saw in Chapter 3, one way to evaluate language models isto measure how well they predict unseen text. Intuitively, good models are those thatassign higher probabilities to unseen data (are less surprised when encountering thenew words).We instantiate this intuition by usingperplexityto measure the quality of aperplexitylanguage model. Recall from page??that the perplexity of a modelqon an unseentest set is the inverse probability thatqassigns to the test set, normalized by the testset length. For a test set ofntokensw1:n, the perplexity isPerplexityq(w1:n)=Pq(w1:n)\u00001n=ns1Pq(w1:n)(10.7)To visualize how perplexity can be computed as a function of the probabilities the\n•Probability depends on size of test set•Probability gets smaller the longer the text•Better: a metric that is per-word, normalized by length•Perplexity is the inverse probability of the test set, normalized by the number of words(The inverse comes from the original definition of perplexity from cross-entropy rate in information theory)Probability range is  [0,1], perplexity range is [1,∞]Why perplexity instead of raw probability of the test set?\nPerplexity•The higher the probability of the word sequence, the lower the perplexity.•Thus the lower the perplexity of a model on the data, the better the model. •Minimizing perplexity is the same as maximizing probabilityAlso: perplexity is sensitive to length/tokenization so best used when comparing LMs that use the same tokenizer.  "
  },
  {
    "chunk_number": 39,
    "pages": [
      77,
      78,
      79
    ],
    "text": "Perplexity•The higher the probability of the word sequence, the lower the perplexity.•Thus the lower the perplexity of a model on the data, the better the model. •Minimizing perplexity is the same as maximizing probabilityAlso: perplexity is sensitive to length/tokenization so best used when comparing LMs that use the same tokenizer.  \nMany other factors that we evaluate, like:Size Big models take lots of GPUs and time to train, memory to storeEnergy usageCan measure kWh or kilograms of CO2 emitted FairnessBenchmarks measure gendered and racial stereotypes, or decreased performance for language from or about some groups. \nLarge Language ModelsHarms of Large Language Models"
  },
  {
    "chunk_number": 40,
    "pages": [
      79,
      80,
      81
    ],
    "text": "Large Language ModelsHarms of Large Language Models\nHallucination\n\nCopyright\n"
  },
  {
    "chunk_number": 41,
    "pages": [
      81,
      82,
      83
    ],
    "text": "Copyright\n\nPrivacy\n\nToxicity and Abuse\n"
  },
  {
    "chunk_number": 42,
    "pages": [
      83,
      84,
      85
    ],
    "text": "Toxicity and Abuse\n\nFraud, Misinformation, Phishing\n\nLarge Language ModelsHarms of Large Language Models"
  },
  {
    "chunk_number": 43,
    "pages": [
      85,
      86,
      87
    ],
    "text": "Large Language ModelsHarms of Large Language Models\nNeural NetworksA quick walkthough of a backprop example(that should help with PA6)\nIntuition of neural net training (from Canvas lecture)Goal: update network weights to minimize lossFor every training tuple (𝑥,𝑦𝑡𝑟𝑢𝑒)1.Run forward computation to find our estimate 𝑦2.Compute loss 𝐿\tbetween 𝑦𝑡𝑟𝑢𝑒 and the estimated 𝑦3.Run backward computation; update weights to decrease 𝐿•For each weight w in the network•Assess how much blame it deserves for the current loss: !\"!#•(Using chain rule to compute partial derivatives)•Update w:    wnew=w−η!\"!#   Where η=.01New network will have lower loss on example"
  },
  {
    "chunk_number": 44,
    "pages": [
      87,
      88,
      89
    ],
    "text": "Intuition of neural net training (from Canvas lecture)Goal: update network weights to minimize lossFor every training tuple (𝑥,𝑦𝑡𝑟𝑢𝑒)1.Run forward computation to find our estimate 𝑦2.Compute loss 𝐿\tbetween 𝑦𝑡𝑟𝑢𝑒 and the estimated 𝑦3.Run backward computation; update weights to decrease 𝐿•For each weight w in the network•Assess how much blame it deserves for the current loss: !\"!#•(Using chain rule to compute partial derivatives)•Update w:    wnew=w−η!\"!#   Where η=.01New network will have lower loss on example\nExample for today: simple 1-layer network, no activation function, squared lossy=w1x1+w2x2+bL=(ytrue−y)2Initial\tweights:w1=2,\tw2=−1,\tb=1We'll\ttrain\ton\tone\texample:(x1,\tx2,\tytrue)\t=\t(4,\t−3,\t10)\n2-114-3ytrue=10\n1. Forward pass: compute yy=w1x1+w2x2+bInitial weights:w1=2, w2=−1, b=1Input:(x1, x2) = (4, −3)y =  2*4 + -1*-3 + 1   = 12\n2-114-3"
  },
  {
    "chunk_number": 45,
    "pages": [
      89,
      90,
      91
    ],
    "text": "1. Forward pass: compute yy=w1x1+w2x2+bInitial weights:w1=2, w2=−1, b=1Input:(x1, x2) = (4, −3)y =  2*4 + -1*-3 + 1   = 12\n2-114-3\n2. Compute Lossy=w1x1+w2x2+b= 12L=(ytrue−y)2= (10-12)2=4\n2-114-3ytrue=10y=12\n3. Backward pass: Create computation graph\nFirst: make computation graph including intermediate variablesThe key nodes are the parameters of the model that we need gradients to update"
  },
  {
    "chunk_number": 46,
    "pages": [
      91,
      92,
      93
    ],
    "text": "3. Backward pass: Create computation graph\nFirst: make computation graph including intermediate variablesThe key nodes are the parameters of the model that we need gradients to update\n3. Backward pass: Create computation graph\nWe're going to make nodes for w1, w2, b, which we need to updatePlus we'll need the nodes between them and the loss LWe'll create 2 convenient intermediate nodes h1=w1x1 and h2=w2x2Why not x1, x2?w1w2bh1=w1x1y=h1+ h2+ bL=(ytrue- y)2h2=w2x2\n3. Backprop:  compute loss gradients for weightsWe want %&%'!,%&%'\", !\"!$!\"!#!\t= !\"!$!$!%!!%!!#!\tw1w2bh1=w1x1y=h1+ h2+ bL=(ytrue- y)2h2=w2x2"
  },
  {
    "chunk_number": 47,
    "pages": [
      93,
      94,
      95
    ],
    "text": "3. Backprop:  compute loss gradients for weightsWe want %&%'!,%&%'\", !\"!$!\"!#!\t= !\"!$!$!%!!%!!#!\tw1w2bh1=w1x1y=h1+ h2+ bL=(ytrue- y)2h2=w2x2\n3. Backprop:  compute loss gradients for weightsWe want %&%'!,%&%'\", !\"!$!\"!#!\t= !\"!$!$!%!!%!!#!\tw1w2bh1=w1x1y=h1+ h2+ bL=(ytrue- y)2h2=w2x2L=(ytrue−y)2y\t=\th1\t+\th2\t+\tbh1\t=\tw1x1\n3. Backprop:  compute loss gradients for weightsWe want %&%'!,%&%'\", !\"!$!\"!#!\t= !\"!$!$!%!!%!!#!\tw1w2bh1=w1x1y=h1+ h2+ bL=(ytrue- y)2h2=w2x2L=(ytrue−y)2y\t=\th1\t+\th2\t+\tbh1\t=\tw1x1!\"!%\t=\t2\t(ytrue-y)\t*\t-1\t\t=\t2y\t–\t2ytrue!%!&!\t=1!&!!#!\t=\tx1"
  },
  {
    "chunk_number": 48,
    "pages": [
      95,
      96,
      97
    ],
    "text": "3. Backprop:  compute loss gradients for weightsWe want %&%'!,%&%'\", !\"!$!\"!#!\t= !\"!$!$!%!!%!!#!\tw1w2bh1=w1x1y=h1+ h2+ bL=(ytrue- y)2h2=w2x2L=(ytrue−y)2y\t=\th1\t+\th2\t+\tbh1\t=\tw1x1!\"!%\t=\t2\t(ytrue-y)\t*\t-1\t\t=\t2y\t–\t2ytrue!%!&!\t=1!&!!#!\t=\tx1\n3. Backprop:  compute loss gradients for weightsWe want %&%'!,%&%'\", !\"!$!\"!#!\t= !\"!$!$!%!!%!!#!\tw1w2bh1=w1x1y=h1+ h2+ bL=(ytrue- y)2h2=w2x2L=(ytrue−y)2y\t=\th1\t+\th2\t+\tbh1\t=\tw1x1!\"!%\t=\t2\t(ytrue-y)\t*\t-1\t\t\t\t\t\t=\t2(y\t–\tytrue)=\t4!%!&!\t=1!&!!#!\t=\tx1\t=\t4!\"!#!\t= 4∗1\t∗4=16x1\t=\t4x2\t=\t-3y\t=\t12ytrue\t=\t10w1=2w2=-1b=1\n3. Backprop:  compute loss gradients for weightsWe want %&%'!,%&%'\", !\"!$!\"!#\"\t= !\"!$!$!%\"!%\"!#\"\tw1w2bh1=w1x1y=h1+ h2+ bL=(ytrue- y)2h2=w2x2L=(ytrue−y)2y\t=\th1\t+\th2\t+\tbh2\t=\tw2x2!\"!%\t=\t2\t(ytrue-y)\t*\t-1\t\t\t\t\t\t=\t2(y\t–\tytrue)=\t4!%!&\"\t=1!&!!#\"\t=\tx2\t=\t-3!\"!#\"\t= 4∗1\t∗−3=−12x1\t=\t4x2\t=\t-3y\t=\t12ytrue\t=\t10w1=2w2=-1b=1"
  },
  {
    "chunk_number": 49,
    "pages": [
      97,
      98,
      99
    ],
    "text": "3. Backprop:  compute loss gradients for weightsWe want %&%'!,%&%'\", !\"!$!\"!#\"\t= !\"!$!$!%\"!%\"!#\"\tw1w2bh1=w1x1y=h1+ h2+ bL=(ytrue- y)2h2=w2x2L=(ytrue−y)2y\t=\th1\t+\th2\t+\tbh2\t=\tw2x2!\"!%\t=\t2\t(ytrue-y)\t*\t-1\t\t\t\t\t\t=\t2(y\t–\tytrue)=\t4!%!&\"\t=1!&!!#\"\t=\tx2\t=\t-3!\"!#\"\t= 4∗1\t∗−3=−12x1\t=\t4x2\t=\t-3y\t=\t12ytrue\t=\t10w1=2w2=-1b=1\n3. Backprop:  compute loss gradients for weightsWe want %&%'!,%&%'\", !\"!$!\"!&\t= !\"!$!$!&\tw1w2bh1=w1x1y=h1+ h2+ bL=(ytrue- y)2h2=w2x2L=(ytrue−y)2y\t=\th1\t+\th2\t+\tb!\"!%\t=\t2\t(ytrue-y)\t*\t-1\t\t\t\t\t\t=\t2(y\t–\tytrue)=\t4!%!$\t=1\n!\"!&\t= 4∗1\t=4x1\t=\t4x2\t=\t-3y\t=\t12ytrue\t=\t10w1=2w2=-1b=1\n3. Backprop:  compute new weights𝜕𝐿𝜕𝑤1=16x1\t=\t4x2\t=\t-3y\t=\t12ytrue\t=\t10w1=2w2=-1b=1η=.01w1\t=\tw1\t−\tη()(*!\t=\t2−0.01∗16\t=\t1.84w2\t=\tw2\t−\tη()(*\"\t=\t−1−0.01∗−12\t=\t−0.88b\t=\tb\t−\tη()(+\t=\t1−0.01∗4\t=\t0.96𝜕𝐿𝜕𝑤2=−12𝜕𝐿𝜕𝑏=4\n1.84-0.880.96"
  },
  {
    "chunk_number": 50,
    "pages": [
      99,
      100,
      101
    ],
    "text": "3. Backprop:  compute new weights𝜕𝐿𝜕𝑤1=16x1\t=\t4x2\t=\t-3y\t=\t12ytrue\t=\t10w1=2w2=-1b=1η=.01w1\t=\tw1\t−\tη()(*!\t=\t2−0.01∗16\t=\t1.84w2\t=\tw2\t−\tη()(*\"\t=\t−1−0.01∗−12\t=\t−0.88b\t=\tb\t−\tη()(+\t=\t1−0.01∗4\t=\t0.96𝜕𝐿𝜕𝑤2=−12𝜕𝐿𝜕𝑏=4\n1.84-0.880.96\nIf we rerun forward pass, result should be closer to ytrue=10y=w1x1+w2x2+b= 10.96\n1.84-0.880.964-3Success!!!\nNeural NetworksA backprop example"
  },
  {
    "chunk_number": 51,
    "pages": [
      101,
      102,
      103
    ],
    "text": "Neural NetworksA backprop example\nTransformersAdvanced: (I won't get to this on Tuesday) Multihead Attention and Parallelizing the Attention Computation\nActual Attention: slightly more complicated•Instead of one attention head, we'll have lots of them!•Intuition: each head might be attending to the context for different purposes•Different linguistic relationships or patterns in the context9.1•ATTENTION7each headiin a self-attention layer has its own set of key, query and value matrices:WKi,WQiandWVi. These are used to project the inputs into separate key, value,and query embeddings for each head.When using multiple heads the model dimensiondis still used for the inputand output, the key and query embeddings have dimensionalitydk, and the valueembeddings are of dimensionalitydv(again, in the original transformer paperdk=dv=64,A=8, andd=512). Thus for each headi, we have weight layersWQiofshape[d⇥dk],WKiof shape[d⇥dk], andWViof shape[d⇥dv].Below are the equations for attention augmented with multiple heads; Fig.9.5shows an intuition.qci=xiWQc;kcj=xjWKc;vcj=xjWVc;8c1cA(9.15)scorec(xi,xj)=qci·kcjpdk(9.16)acij=softmax(scorec(xi,xj))8ji(9.17)headci=Xjiacijvcj(9.18)ai=(head1\u0000head2...\u0000headA)WO(9.19)MultiHeadAttention(xi,[x1,···,xN]) =ai(9.20)The output of each of theAheads is of shape 1⇥dv, and so the output of themulti-head layer withAheads consists ofAvectors of shape 1⇥dv. These areconcatenated to produce a single output with dimensionality 1⇥hdv. Then we useyet another linear projectionWO2RAdv⇥dto reshape it, resulting in the multi-headattention vectoraiwith the correct output shape[1⇥d]at each inputi.\naixi-1xixi-2xi-3WK1Head 1WV1WQ1……WK2Head 2WV2WQ2WK8Head 8WV8WQ8aiWO  [hdv x d][1 x dv ][1 x d]\n[1 x d][1 x hdv ]Project down to dConcatenate OutputsEach headattends diﬀerentlyto context…[1 x dv ]\nFigure 9.5The multi-head attention computation for inputxi, producing outputai. A multi-head attentionlayer hasAheads, each with its own key, query and value weight matrices. The outputs from each of the headsare concatenated and then projected down tod, thus producing an output of the same size as the input."
  },
  {
    "chunk_number": 52,
    "pages": [
      103,
      104,
      105
    ],
    "text": "Actual Attention: slightly more complicated•Instead of one attention head, we'll have lots of them!•Intuition: each head might be attending to the context for different purposes•Different linguistic relationships or patterns in the context9.1•ATTENTION7each headiin a self-attention layer has its own set of key, query and value matrices:WKi,WQiandWVi. These are used to project the inputs into separate key, value,and query embeddings for each head.When using multiple heads the model dimensiondis still used for the inputand output, the key and query embeddings have dimensionalitydk, and the valueembeddings are of dimensionalitydv(again, in the original transformer paperdk=dv=64,A=8, andd=512). Thus for each headi, we have weight layersWQiofshape[d⇥dk],WKiof shape[d⇥dk], andWViof shape[d⇥dv].Below are the equations for attention augmented with multiple heads; Fig.9.5shows an intuition.qci=xiWQc;kcj=xjWKc;vcj=xjWVc;8c1cA(9.15)scorec(xi,xj)=qci·kcjpdk(9.16)acij=softmax(scorec(xi,xj))8ji(9.17)headci=Xjiacijvcj(9.18)ai=(head1\u0000head2...\u0000headA)WO(9.19)MultiHeadAttention(xi,[x1,···,xN]) =ai(9.20)The output of each of theAheads is of shape 1⇥dv, and so the output of themulti-head layer withAheads consists ofAvectors of shape 1⇥dv. These areconcatenated to produce a single output with dimensionality 1⇥hdv. Then we useyet another linear projectionWO2RAdv⇥dto reshape it, resulting in the multi-headattention vectoraiwith the correct output shape[1⇥d]at each inputi.\naixi-1xixi-2xi-3WK1Head 1WV1WQ1……WK2Head 2WV2WQ2WK8Head 8WV8WQ8aiWO  [hdv x d][1 x dv ][1 x d]\n[1 x d][1 x hdv ]Project down to dConcatenate OutputsEach headattends diﬀerentlyto context…[1 x dv ]\nFigure 9.5The multi-head attention computation for inputxi, producing outputai. A multi-head attentionlayer hasAheads, each with its own key, query and value weight matrices. The outputs from each of the headsare concatenated and then projected down tod, thus producing an output of the same size as the input.\nMulti-head attention\naixi-1xixi-2xi-3WK1Head 1WV1WQ1……WK2Head 2WV2WQ2WK8Head 8WV8WQ8aiWO  [hdv x d][1 x dv ][1 x d]\n[1 x d][1 x hdv ]Project down to dConcatenate OutputsEach headattends diﬀerentlyto context…[1 x dv ]\nParallelizing computation using XFor attention/transformer block we've been computing a single output at a single time step i in a single residual stream. But we can pack the  N tokens of the input sequence into a single matrix X of size [N × d]. Each row of X is the embedding of one token of the input. X can have 1K-32K rows, each of the dimensionality of the embedding d (the model dimension)9.3•PARALLELIZING COMPUTATION USING A SINGLE MATRIXX11dimension).Parallelizing attentionLet’s ﬁrst see this for a single attention head and then turnto multiple heads, and then add in the rest of the components in the transformerblock. For one head we multiplyXby the key, query, and value matricesWQofshape[d⇥dk],WKof shape[d⇥dk], andWVof shape[d⇥dv], to produce matricesQof shape[N⇥dk],K2RN⇥dk, andV2RN⇥dv, containing all the key, query, andvalue vectors:Q=XWQ;K=XWK;V=XWV(9.31)Given these matrices we can compute all the requisite query-key comparisons simul-taneously by multiplyingQandK|in a single matrix multiplication. The product isof shapeN⇥N, visualized in Fig.9.9.q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NNq1•k2q1•k3q1•k4q2•k3q2•k4q3•k4Figure 9.8TheN⇥NQK|matrix showing how it computes allqi·kjcomparisons in asingle matrix multiple.Once we have thisQK|matrix, we can very efﬁciently scale these scores, takethe softmax, and then multiply the result byVresulting in a matrix of shapeN⇥d:a vector embedding representation for each token in the input. We’ve reduced theentire self-attention step for an entire sequence ofNtokens for one head to thefollowing computation:A=softmax✓mask✓QK|pdk◆◆V(9.32)Masking out the futureYou may have noticed that we introduced a mask functionin Eq.9.32above. This is because the self-attention computation as we’ve describedit has a problem: the calculation inQK|results in a score for each query valueto every key value,including those that follow the query. This is inappropriate inthe setting of language modeling: guessing the next word is pretty simple if youalready know it! To ﬁx this, the elements in the upper-triangular portion of thematrix are zeroed out (set to\u0000•), thus eliminating any knowledge of words thatfollow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000•8j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we’ll see in Chapter 11 how tomake use of words in the future for tasks that need it).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of tokens."
  },
  {
    "chunk_number": 53,
    "pages": [
      105,
      106,
      107
    ],
    "text": "Parallelizing computation using XFor attention/transformer block we've been computing a single output at a single time step i in a single residual stream. But we can pack the  N tokens of the input sequence into a single matrix X of size [N × d]. Each row of X is the embedding of one token of the input. X can have 1K-32K rows, each of the dimensionality of the embedding d (the model dimension)9.3•PARALLELIZING COMPUTATION USING A SINGLE MATRIXX11dimension).Parallelizing attentionLet’s ﬁrst see this for a single attention head and then turnto multiple heads, and then add in the rest of the components in the transformerblock. For one head we multiplyXby the key, query, and value matricesWQofshape[d⇥dk],WKof shape[d⇥dk], andWVof shape[d⇥dv], to produce matricesQof shape[N⇥dk],K2RN⇥dk, andV2RN⇥dv, containing all the key, query, andvalue vectors:Q=XWQ;K=XWK;V=XWV(9.31)Given these matrices we can compute all the requisite query-key comparisons simul-taneously by multiplyingQandK|in a single matrix multiplication. The product isof shapeN⇥N, visualized in Fig.9.9.q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NNq1•k2q1•k3q1•k4q2•k3q2•k4q3•k4Figure 9.8TheN⇥NQK|matrix showing how it computes allqi·kjcomparisons in asingle matrix multiple.Once we have thisQK|matrix, we can very efﬁciently scale these scores, takethe softmax, and then multiply the result byVresulting in a matrix of shapeN⇥d:a vector embedding representation for each token in the input. We’ve reduced theentire self-attention step for an entire sequence ofNtokens for one head to thefollowing computation:A=softmax✓mask✓QK|pdk◆◆V(9.32)Masking out the futureYou may have noticed that we introduced a mask functionin Eq.9.32above. This is because the self-attention computation as we’ve describedit has a problem: the calculation inQK|results in a score for each query valueto every key value,including those that follow the query. This is inappropriate inthe setting of language modeling: guessing the next word is pretty simple if youalready know it! To ﬁx this, the elements in the upper-triangular portion of thematrix are zeroed out (set to\u0000•), thus eliminating any knowledge of words thatfollow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000•8j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we’ll see in Chapter 11 how tomake use of words in the future for tasks that need it).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of tokens.\nQKTNow can do a single matrix multiply to combine Q and KTq1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NNq1•k2q1•k3q1•k4q2•k3q2•k4q3•k4\nParallelizing attention•Scale the  scores, take the softmax, and then multiply the result by V resulting in a matrix of shape N × d•An attention vector for each input token12CHAPTER9•THETRANSFORMERfollowing computation:head=softmax✓mask✓QK|pdk◆◆VA=head WO(9.33)Masking out the futureYou may have noticed that we introduced a mask functionin Eq.9.33above. This is because the self-attention computation as we’ve describedit has a problem: the calculation ofQK|results in a score for each query value toevery key value,including those that follow the query. This is inappropriate in thesetting of language modeling: guessing the next word is pretty simple if you alreadyknow it! To ﬁx this, the elements in the upper-triangular portion of the matrix are setto\u0000•, which the softmax will turn to zero, thus eliminating any knowledge of wordsthat follow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000•8j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we’ll see in Chapter 11 how tomake use of words in the future for tasks that need it).q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NN−∞−∞−∞−∞−∞−∞\nFigure 9.9TheN⇥NQK|matrix showing theqi·kjvalues, with the upper-triangle por-tion of the comparisons matrix zeroed out (set to\u0000•, which the softmax will turn to zero).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of tokens.Parallelizing multi-head attentionIn multi-head attention, as with self-attention,the input and output have the model dimensiond, the key and query embeddingshave dimensionalitydk, and the value embeddings are of dimensionalitydv(again,in the original transformer paperdk=dv=64,A=8, andd=512). Thus foreach headi, we have weight layersWQiof shape[d⇥dk],WKiof shape[d⇥dk],andWViof shape[d⇥dv], and these get multiplied by the inputs packed intoXto produceQof shape[N⇥dk],Kof shape[N⇥dk], andVof shape[N⇥dv].The output of each of theAheads is of shapeN⇥dv, and so the output of themulti-head layer withAheads consists ofAmatrices of shapeN⇥dv. To make useof these matrices in further processing, they are concatenated to produce a singleoutput with dimensionalityN⇥hdv. Finally, we use a ﬁnal linear projectionWOof shape[Adv⇥d], that reshape it to the original output dimension for each token.Multiplying the concatenatedN⇥hdvmatrix output byWOof shape[Adv⇥d]yields"
  },
  {
    "chunk_number": 54,
    "pages": [
      107,
      108,
      109
    ],
    "text": "Parallelizing attention•Scale the  scores, take the softmax, and then multiply the result by V resulting in a matrix of shape N × d•An attention vector for each input token12CHAPTER9•THETRANSFORMERfollowing computation:head=softmax✓mask✓QK|pdk◆◆VA=head WO(9.33)Masking out the futureYou may have noticed that we introduced a mask functionin Eq.9.33above. This is because the self-attention computation as we’ve describedit has a problem: the calculation ofQK|results in a score for each query value toevery key value,including those that follow the query. This is inappropriate in thesetting of language modeling: guessing the next word is pretty simple if you alreadyknow it! To ﬁx this, the elements in the upper-triangular portion of the matrix are setto\u0000•, which the softmax will turn to zero, thus eliminating any knowledge of wordsthat follow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000•8j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we’ll see in Chapter 11 how tomake use of words in the future for tasks that need it).q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NN−∞−∞−∞−∞−∞−∞\nFigure 9.9TheN⇥NQK|matrix showing theqi·kjvalues, with the upper-triangle por-tion of the comparisons matrix zeroed out (set to\u0000•, which the softmax will turn to zero).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of tokens.Parallelizing multi-head attentionIn multi-head attention, as with self-attention,the input and output have the model dimensiond, the key and query embeddingshave dimensionalitydk, and the value embeddings are of dimensionalitydv(again,in the original transformer paperdk=dv=64,A=8, andd=512). Thus foreach headi, we have weight layersWQiof shape[d⇥dk],WKiof shape[d⇥dk],andWViof shape[d⇥dv], and these get multiplied by the inputs packed intoXto produceQof shape[N⇥dk],Kof shape[N⇥dk], andVof shape[N⇥dv].The output of each of theAheads is of shapeN⇥dv, and so the output of themulti-head layer withAheads consists ofAmatrices of shapeN⇥dv. To make useof these matrices in further processing, they are concatenated to produce a singleoutput with dimensionalityN⇥hdv. Finally, we use a ﬁnal linear projectionWOof shape[Adv⇥d], that reshape it to the original output dimension for each token.Multiplying the concatenatedN⇥hdvmatrix output byWOof shape[Adv⇥d]yields\nMasking out the future•What is this mask function?QKT has a score for each query dot every key, including those that follow the query.•Guessing the next word is pretty simple if you already know it! 12CHAPTER9•THETRANSFORMERfollowing computation:head=softmax✓mask✓QK|pdk◆◆VA=head WO(9.33)Masking out the futureYou may have noticed that we introduced a mask functionin Eq.9.33above. This is because the self-attention computation as we’ve describedit has a problem: the calculation ofQK|results in a score for each query value toevery key value,including those that follow the query. This is inappropriate in thesetting of language modeling: guessing the next word is pretty simple if you alreadyknow it! To ﬁx this, the elements in the upper-triangular portion of the matrix are setto\u0000•, which the softmax will turn to zero, thus eliminating any knowledge of wordsthat follow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000•8j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we’ll see in Chapter 11 how tomake use of words in the future for tasks that need it).q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NN−∞−∞−∞−∞−∞−∞\nFigure 9.9TheN⇥NQK|matrix showing theqi·kjvalues, with the upper-triangle por-tion of the comparisons matrix zeroed out (set to\u0000•, which the softmax will turn to zero).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of tokens.Parallelizing multi-head attentionIn multi-head attention, as with self-attention,the input and output have the model dimensiond, the key and query embeddingshave dimensionalitydk, and the value embeddings are of dimensionalitydv(again,in the original transformer paperdk=dv=64,A=8, andd=512). Thus foreach headi, we have weight layersWQiof shape[d⇥dk],WKiof shape[d⇥dk],andWViof shape[d⇥dv], and these get multiplied by the inputs packed intoXto produceQof shape[N⇥dk],Kof shape[N⇥dk], andVof shape[N⇥dv].The output of each of theAheads is of shapeN⇥dv, and so the output of themulti-head layer withAheads consists ofAmatrices of shapeN⇥dv. To make useof these matrices in further processing, they are concatenated to produce a singleoutput with dimensionalityN⇥hdv. Finally, we use a ﬁnal linear projectionWOof shape[Adv⇥d], that reshape it to the original output dimension for each token.Multiplying the concatenatedN⇥hdvmatrix output byWOof shape[Adv⇥d]yields\nMasking out the futureAdd –∞ to cells in upper triangleThe softmax will turn it to 0q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NN−∞−∞−∞−∞−∞−∞12CHAPTER9•THETRANSFORMERfollowing computation:head=softmax✓mask✓QK|pdk◆◆VA=head WO(9.33)Masking out the futureYou may have noticed that we introduced a mask functionin Eq.9.33above. This is because the self-attention computation as we’ve describedit has a problem: the calculation ofQK|results in a score for each query value toevery key value,including those that follow the query. This is inappropriate in thesetting of language modeling: guessing the next word is pretty simple if you alreadyknow it! To ﬁx this, the elements in the upper-triangular portion of the matrix are setto\u0000•, which the softmax will turn to zero, thus eliminating any knowledge of wordsthat follow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000•8j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we’ll see in Chapter 11 how tomake use of words in the future for tasks that need it).q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NN−∞−∞−∞−∞−∞−∞\nFigure 9.9TheN⇥NQK|matrix showing theqi·kjvalues, with the upper-triangle por-tion of the comparisons matrix zeroed out (set to\u0000•, which the softmax will turn to zero).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of tokens.Parallelizing multi-head attentionIn multi-head attention, as with self-attention,the input and output have the model dimensiond, the key and query embeddingshave dimensionalitydk, and the value embeddings are of dimensionalitydv(again,in the original transformer paperdk=dv=64,A=8, andd=512). Thus foreach headi, we have weight layersWQiof shape[d⇥dk],WKiof shape[d⇥dk],andWViof shape[d⇥dv], and these get multiplied by the inputs packed intoXto produceQof shape[N⇥dk],Kof shape[N⇥dk], andVof shape[N⇥dv].The output of each of theAheads is of shapeN⇥dv, and so the output of themulti-head layer withAheads consists ofAmatrices of shapeN⇥dv. To make useof these matrices in further processing, they are concatenated to produce a singleoutput with dimensionalityN⇥hdv. Finally, we use a ﬁnal linear projectionWOof shape[Adv⇥d], that reshape it to the original output dimension for each token.Multiplying the concatenatedN⇥hdvmatrix output byWOof shape[Adv⇥d]yields"
  },
  {
    "chunk_number": 55,
    "pages": [
      109,
      110,
      111
    ],
    "text": "Masking out the futureAdd –∞ to cells in upper triangleThe softmax will turn it to 0q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NN−∞−∞−∞−∞−∞−∞12CHAPTER9•THETRANSFORMERfollowing computation:head=softmax✓mask✓QK|pdk◆◆VA=head WO(9.33)Masking out the futureYou may have noticed that we introduced a mask functionin Eq.9.33above. This is because the self-attention computation as we’ve describedit has a problem: the calculation ofQK|results in a score for each query value toevery key value,including those that follow the query. This is inappropriate in thesetting of language modeling: guessing the next word is pretty simple if you alreadyknow it! To ﬁx this, the elements in the upper-triangular portion of the matrix are setto\u0000•, which the softmax will turn to zero, thus eliminating any knowledge of wordsthat follow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000•8j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we’ll see in Chapter 11 how tomake use of words in the future for tasks that need it).q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NN−∞−∞−∞−∞−∞−∞\nFigure 9.9TheN⇥NQK|matrix showing theqi·kjvalues, with the upper-triangle por-tion of the comparisons matrix zeroed out (set to\u0000•, which the softmax will turn to zero).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of tokens.Parallelizing multi-head attentionIn multi-head attention, as with self-attention,the input and output have the model dimensiond, the key and query embeddingshave dimensionalitydk, and the value embeddings are of dimensionalitydv(again,in the original transformer paperdk=dv=64,A=8, andd=512). Thus foreach headi, we have weight layersWQiof shape[d⇥dk],WKiof shape[d⇥dk],andWViof shape[d⇥dv], and these get multiplied by the inputs packed intoXto produceQof shape[N⇥dk],Kof shape[N⇥dk], andVof shape[N⇥dv].The output of each of theAheads is of shapeN⇥dv, and so the output of themulti-head layer withAheads consists ofAmatrices of shapeN⇥dv. To make useof these matrices in further processing, they are concatenated to produce a singleoutput with dimensionalityN⇥hdv. Finally, we use a ﬁnal linear projectionWOof shape[Adv⇥d], that reshape it to the original output dimension for each token.Multiplying the concatenatedN⇥hdvmatrix output byWOof shape[Adv⇥d]yields\nAnother point: Attention is quadratic in length\nq1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NN−∞−∞−∞−∞−∞−∞12CHAPTER9•THETRANSFORMERfollowing computation:head=softmax✓mask✓QK|pdk◆◆VA=head WO(9.33)Masking out the futureYou may have noticed that we introduced a mask functionin Eq.9.33above. This is because the self-attention computation as we’ve describedit has a problem: the calculation ofQK|results in a score for each query value toevery key value,including those that follow the query. This is inappropriate in thesetting of language modeling: guessing the next word is pretty simple if you alreadyknow it! To ﬁx this, the elements in the upper-triangular portion of the matrix are setto\u0000•, which the softmax will turn to zero, thus eliminating any knowledge of wordsthat follow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000•8j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we’ll see in Chapter 11 how tomake use of words in the future for tasks that need it).q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NN−∞−∞−∞−∞−∞−∞\nFigure 9.9TheN⇥NQK|matrix showing theqi·kjvalues, with the upper-triangle por-tion of the comparisons matrix zeroed out (set to\u0000•, which the softmax will turn to zero).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of tokens.Parallelizing multi-head attentionIn multi-head attention, as with self-attention,the input and output have the model dimensiond, the key and query embeddingshave dimensionalitydk, and the value embeddings are of dimensionalitydv(again,in the original transformer paperdk=dv=64,A=8, andd=512). Thus foreach headi, we have weight layersWQiof shape[d⇥dk],WKiof shape[d⇥dk],andWViof shape[d⇥dv], and these get multiplied by the inputs packed intoXto produceQof shape[N⇥dk],Kof shape[N⇥dk], andVof shape[N⇥dv].The output of each of theAheads is of shapeN⇥dv, and so the output of themulti-head layer withAheads consists ofAmatrices of shapeN⇥dv. To make useof these matrices in further processing, they are concatenated to produce a singleoutput with dimensionalityN⇥hdv. Finally, we use a ﬁnal linear projectionWOof shape[Adv⇥d], that reshape it to the original output dimension for each token.Multiplying the concatenatedN⇥hdvmatrix output byWOof shape[Adv⇥d]yields\nAttention again\nq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dv\nq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dv\nq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dv"
  },
  {
    "chunk_number": 56,
    "pages": [
      111,
      112,
      113
    ],
    "text": "Attention again\nq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dv\nq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dv\nq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dv\nParallelizing Multi-head Attention9.4•THE INPUT:EMBEDDINGS FOR TOKEN AND POSITION13the self-attention outputAof shape [N⇥d].Qi=XWQi;Ki=XWKi;Vi=XWVi(9.33)headi=SelfAttention(Qi,Ki,Vi)=softmax✓QiKi|pdk◆Vi(9.34)MultiHeadAttention(X)=(head1\u0000head2...\u0000headh)WO(9.35)Putting it all together with the parallel input matrixXThe function computedin parallel by an entire layer ofNtransformer block over the entireNinput tokenscan be expressed as:O=LayerNorm(X+MultiHeadAttention(X))(9.36)H=LayerNorm(O+FFN(O))(9.37)Or we can break it down with one equation for each component computation, usingT(of shape[N⇥d]) to stand for transformer and superscripts to demarcate eachcomputation inside the block:T1=MultiHeadAttention(X)(9.38)T2=X+T1(9.39)T3=LayerNorm(T2)(9.40)T4=FFN(T3)(9.41)T5=T4+T3(9.42)H=LayerNorm(T5)(9.43)Here when we use a notation like FFN(T3)we mean that the same FFN is appliedin parallel to each of theNembedding vectors in the window. Similarly, each of theNtokens is normed in parallel in the LayerNorm. Crucially, the input and outputdimensions of transformer blocks are matched so they can be stacked. Since eachtokenxiat the input to the block has dimensionalityd, that means the inputXandoutputHare both of shape[N⇥d].9.4 The input: embeddings for token and positionLet’s talk about where the inputXcomes from. Given a sequence ofNtokens (Nisthe context length in tokens), the matrixXof shape[N⇥d]has anembeddingforembeddingeach word in the context. The transformer does this by separately computing twoembeddings: an input token embedding, and an input positional embedding.A token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di-mensiondthat will be our initial representation for the input token. (As we passvectors up through the transformer layers in the residual stream, this embeddingrepresentation will change and grow, incorporating context and playing a differentrole depending on the kind of language model we are building.) The set of initialembeddings are stored in the embedding matrixE, which has a row for each of the|V|tokens in the vocabulary. Thus each word is a row vector ofddimensions, andEhas shape[|V|⇥d].Given an input token string likeThanks for all thewe ﬁrst convert the tokensinto vocabulary indices (these were created when we ﬁrst tokenized the input using\nParallelizing Multi-head Attentionor9.4•THE INPUT:EMBEDDINGS FOR TOKEN AND POSITION13the self-attention outputAof shape [N⇥d].Qi=XWQi;Ki=XWKi;Vi=XWVi(9.33)headi=SelfAttention(Qi,Ki,Vi)=softmax✓QiKi|pdk◆Vi(9.34)MultiHeadAttention(X)=(head1\u0000head2...\u0000headh)WO(9.35)Putting it all together with the parallel input matrixXThe function computedin parallel by an entire layer ofNtransformer block over the entireNinput tokenscan be expressed as:O=LayerNorm(X+MultiHeadAttention(X))(9.36)H=LayerNorm(O+FFN(O))(9.37)Or we can break it down with one equation for each component computation, usingT(of shape[N⇥d]) to stand for transformer and superscripts to demarcate eachcomputation inside the block:T1=MultiHeadAttention(X)(9.38)T2=X+T1(9.39)T3=LayerNorm(T2)(9.40)T4=FFN(T3)(9.41)T5=T4+T3(9.42)H=LayerNorm(T5)(9.43)Here when we use a notation like FFN(T3)we mean that the same FFN is appliedin parallel to each of theNembedding vectors in the window. Similarly, each of theNtokens is normed in parallel in the LayerNorm. Crucially, the input and outputdimensions of transformer blocks are matched so they can be stacked. Since eachtokenxiat the input to the block has dimensionalityd, that means the inputXandoutputHare both of shape[N⇥d].9.4 The input: embeddings for token and positionLet’s talk about where the inputXcomes from. Given a sequence ofNtokens (Nisthe context length in tokens), the matrixXof shape[N⇥d]has anembeddingforembeddingeach word in the context. The transformer does this by separately computing twoembeddings: an input token embedding, and an input positional embedding.A token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di-mensiondthat will be our initial representation for the input token. (As we passvectors up through the transformer layers in the residual stream, this embeddingrepresentation will change and grow, incorporating context and playing a differentrole depending on the kind of language model we are building.) The set of initialembeddings are stored in the embedding matrixE, which has a row for each of the|V|tokens in the vocabulary. Thus each word is a row vector ofddimensions, andEhas shape[|V|⇥d].Given an input token string likeThanks for all thewe ﬁrst convert the tokensinto vocabulary indices (these were created when we ﬁrst tokenized the input using14CHAPTER9•THETRANSFORMERembedding:T1=LayerNorm(X)(9.39)T2=MultiHeadAttention(T1)(9.40)T3=T2+X(9.41)T4=LayerNorm(T3)(9.42)T5=FFN(T4)(9.43)H=T5+T3(9.44)Here when we use a notation like FFN(T3)we mean that the same FFN is appliedin parallel to each of theNembedding vectors in the window. Similarly, each of theNtokens is normed in parallel in the LayerNorm. Crucially, the input and outputdimensions of transformer blocks are matched so they can be stacked. Since eachtokenxiat the input to the block is represented by an embedding of dimensionality[1⇥d], that means the inputXand outputHare both of shape[N⇥d].9.4 The input: embeddings for token and positionLet’s talk about where the inputXcomes from. Given a sequence ofNtokens (Nisthe context length in tokens), the matrixXof shape[N⇥d]has anembeddingforembeddingeach word in the context. The transformer does this by separately computing twoembeddings: an input token embedding, and an input positional embedding.A token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di-mensiondthat will be our initial representation for the input token. (As we passvectors up through the transformer layers in the residual stream, this embeddingrepresentation will change and grow, incorporating context and playing a differentrole depending on the kind of language model we are building.) The set of initialembeddings are stored in the embedding matrixE, which has a row for each of the|V|tokens in the vocabulary. Thus each word is a row vector ofddimensions, andEhas shape[|V|⇥d].Given an input token string likeThanks for all thewe ﬁrst convert the tokensinto vocabulary indices (these were created when we ﬁrst tokenized the input usingBPE or SentencePiece). So the representation ofthanks for all themight bew=[5,4000,10532,2224]. Next we use indexing to select the corresponding rows fromE, (row 5, row 4000, row 10532, row 2224).Another way to think about selecting token embeddings from the embeddingmatrix is to represent tokens as one-hot vectors of shape[1⇥|V|], i.e., with onedimension for each word in the vocabulary. Recall that in aone-hot vectorall theone-hot vectorelements are 0 except one, the element whose dimension is the word’s index in thevocabulary, which has value 1. So if the word “thanks” has index 5 in the vocabulary,x5=1, andxi=08i6=5, as shown here:[0 0 0 0 1 0 0 ... 0 0 0 0]1 2 3 4 5 6 7 ... ... |V|Multiplying by a one-hot vector that has only one non-zero elementxi=1 simplyselects out the relevant row vector for wordi, resulting in the embedding for wordi,as depicted in Fig.9.11."
  },
  {
    "chunk_number": 57,
    "pages": [
      113,
      114
    ],
    "text": "Parallelizing Multi-head Attentionor9.4•THE INPUT:EMBEDDINGS FOR TOKEN AND POSITION13the self-attention outputAof shape [N⇥d].Qi=XWQi;Ki=XWKi;Vi=XWVi(9.33)headi=SelfAttention(Qi,Ki,Vi)=softmax✓QiKi|pdk◆Vi(9.34)MultiHeadAttention(X)=(head1\u0000head2...\u0000headh)WO(9.35)Putting it all together with the parallel input matrixXThe function computedin parallel by an entire layer ofNtransformer block over the entireNinput tokenscan be expressed as:O=LayerNorm(X+MultiHeadAttention(X))(9.36)H=LayerNorm(O+FFN(O))(9.37)Or we can break it down with one equation for each component computation, usingT(of shape[N⇥d]) to stand for transformer and superscripts to demarcate eachcomputation inside the block:T1=MultiHeadAttention(X)(9.38)T2=X+T1(9.39)T3=LayerNorm(T2)(9.40)T4=FFN(T3)(9.41)T5=T4+T3(9.42)H=LayerNorm(T5)(9.43)Here when we use a notation like FFN(T3)we mean that the same FFN is appliedin parallel to each of theNembedding vectors in the window. Similarly, each of theNtokens is normed in parallel in the LayerNorm. Crucially, the input and outputdimensions of transformer blocks are matched so they can be stacked. Since eachtokenxiat the input to the block has dimensionalityd, that means the inputXandoutputHare both of shape[N⇥d].9.4 The input: embeddings for token and positionLet’s talk about where the inputXcomes from. Given a sequence ofNtokens (Nisthe context length in tokens), the matrixXof shape[N⇥d]has anembeddingforembeddingeach word in the context. The transformer does this by separately computing twoembeddings: an input token embedding, and an input positional embedding.A token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di-mensiondthat will be our initial representation for the input token. (As we passvectors up through the transformer layers in the residual stream, this embeddingrepresentation will change and grow, incorporating context and playing a differentrole depending on the kind of language model we are building.) The set of initialembeddings are stored in the embedding matrixE, which has a row for each of the|V|tokens in the vocabulary. Thus each word is a row vector ofddimensions, andEhas shape[|V|⇥d].Given an input token string likeThanks for all thewe ﬁrst convert the tokensinto vocabulary indices (these were created when we ﬁrst tokenized the input using14CHAPTER9•THETRANSFORMERembedding:T1=LayerNorm(X)(9.39)T2=MultiHeadAttention(T1)(9.40)T3=T2+X(9.41)T4=LayerNorm(T3)(9.42)T5=FFN(T4)(9.43)H=T5+T3(9.44)Here when we use a notation like FFN(T3)we mean that the same FFN is appliedin parallel to each of theNembedding vectors in the window. Similarly, each of theNtokens is normed in parallel in the LayerNorm. Crucially, the input and outputdimensions of transformer blocks are matched so they can be stacked. Since eachtokenxiat the input to the block is represented by an embedding of dimensionality[1⇥d], that means the inputXand outputHare both of shape[N⇥d].9.4 The input: embeddings for token and positionLet’s talk about where the inputXcomes from. Given a sequence ofNtokens (Nisthe context length in tokens), the matrixXof shape[N⇥d]has anembeddingforembeddingeach word in the context. The transformer does this by separately computing twoembeddings: an input token embedding, and an input positional embedding.A token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di-mensiondthat will be our initial representation for the input token. (As we passvectors up through the transformer layers in the residual stream, this embeddingrepresentation will change and grow, incorporating context and playing a differentrole depending on the kind of language model we are building.) The set of initialembeddings are stored in the embedding matrixE, which has a row for each of the|V|tokens in the vocabulary. Thus each word is a row vector ofddimensions, andEhas shape[|V|⇥d].Given an input token string likeThanks for all thewe ﬁrst convert the tokensinto vocabulary indices (these were created when we ﬁrst tokenized the input usingBPE or SentencePiece). So the representation ofthanks for all themight bew=[5,4000,10532,2224]. Next we use indexing to select the corresponding rows fromE, (row 5, row 4000, row 10532, row 2224).Another way to think about selecting token embeddings from the embeddingmatrix is to represent tokens as one-hot vectors of shape[1⇥|V|], i.e., with onedimension for each word in the vocabulary. Recall that in aone-hot vectorall theone-hot vectorelements are 0 except one, the element whose dimension is the word’s index in thevocabulary, which has value 1. So if the word “thanks” has index 5 in the vocabulary,x5=1, andxi=08i6=5, as shown here:[0 0 0 0 1 0 0 ... 0 0 0 0]1 2 3 4 5 6 7 ... ... |V|Multiplying by a one-hot vector that has only one non-zero elementxi=1 simplyselects out the relevant row vector for wordi, resulting in the embedding for wordi,as depicted in Fig.9.11.\nTransformersParallelizing Attention Computation"
  }
]