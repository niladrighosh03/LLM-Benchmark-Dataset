{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415a6249",
   "metadata": {},
   "source": [
    "### Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3966751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_pdf_pages(pdf_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Extracts text from each page of a PDF.\n",
    "    Returns a list where each element is text from one page.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    pages = []\n",
    "\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        pages.append(text if text else \"\")\n",
    "\n",
    "    return pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7822ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_page_chunks(\n",
    "    pages: list,\n",
    "    chunk_size: int = 3,\n",
    "    overlap: int = 1\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Creates page chunks with overlap.\n",
    "    \n",
    "    Example:\n",
    "    chunk_size=3, overlap=1\n",
    "    Pages: [1,2,3] → [3,4,5] → ...\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap\n",
    "\n",
    "    chunk_number = 1\n",
    "    for start in range(0, len(pages), step):\n",
    "        end = start + chunk_size\n",
    "        chunk_pages = pages[start:end]\n",
    "\n",
    "        if not chunk_pages:\n",
    "            break\n",
    "\n",
    "        chunks.append({\n",
    "            \"chunk_number\": chunk_number,\n",
    "            \"pages\": list(range(start + 1, min(end + 1, len(pages) + 1))),\n",
    "            \"text\": \"\\n\".join(chunk_pages)\n",
    "        })\n",
    "\n",
    "        chunk_number += 1\n",
    "\n",
    "        if end >= len(pages):\n",
    "            break\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def save_chunks(chunks: list, output_path: str):\n",
    "    \"\"\"\n",
    "    Saves chunks to a JSON file.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "def load_chunks(path: str) -> list:\n",
    "    \"\"\"\n",
    "    Loads stored PDF chunks.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c08334",
   "metadata": {},
   "source": [
    "### Use LLM to generate MCQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b3bcc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mcq_prompt(text: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are an expert university-level instructor and question setter.\n",
    "\n",
    "Generate 5 high-quality multiple-choice questions (MCQs)\n",
    "STRICTLY based on the lecture content below.\n",
    "\n",
    "CRITICAL RULES (NO EXCEPTIONS):\n",
    "1. Use ONLY the provided lecture content.\n",
    "2. Each question must be conceptually meaningful and **non-generic**.\n",
    "3. Each question must be an **actual MCQ**, not a definition recall question.\n",
    "4. Avoid generic prompts such as \"What is X?\" or \"Define Y\".\n",
    "5. Questions must test **application, reasoning, interpretation, comparison, edge cases, numerical reasoning, or scenario-based understanding** explicitly grounded in the lecture.\n",
    "6. Each question must have EXACTLY 4 options.\n",
    "7. EXACTLY one option must be correct.\n",
    "8. Distractors must be **plausible and content-specific**, not obviously incorrect.\n",
    "9. DO NOT add explanations, comments, hints, or extra text.\n",
    "10. OUTPUT MUST BE VALID JSON ONLY.\n",
    "11. The output MUST contain EXACTLY 5 questions — no more, no less.\n",
    "\n",
    "OUTPUT FORMAT (STRICT JSON ARRAY):\n",
    "[\n",
    "  {{\n",
    "    \"question\": \"question text\",\n",
    "    \"options\": {{\n",
    "      \"A\": \"option text\",\n",
    "      \"B\": \"option text\",\n",
    "      \"C\": \"option text\",\n",
    "      \"D\": \"option text\"\n",
    "    }},\n",
    "    \"answer\": \"A\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "EXAMPLE OUTPUT (FORMAT ONLY):\n",
    "[\n",
    "  {{\n",
    "    \"question\": \"Who invented Gravity?\",\n",
    "    \"options\": {{\n",
    "      \"A\": \"Isaac Newton\",\n",
    "      \"B\": \"Albert Einstein\",\n",
    "      \"C\": \"Galileo Galilei\",\n",
    "      \"D\": \"Marrie Curie\"\n",
    "    }},\n",
    "    \"answer\": \"A\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "LECTURE CONTENT:\n",
    "<<<\n",
    "{text}\n",
    ">>>\n",
    "\n",
    "RETURN ONLY THE JSON ARRAY. NO MARKDOWN. NO TEXT OUTSIDE JSON.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d468fc",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">1) Mistral 7B Instruct</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483c3957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 19:02:16.646654: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-02 19:02:17.867979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-02 19:02:18.092618: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-02 19:02:18.135418: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-02 19:02:18.766567: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-02 19:02:22.740099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b13692b9f0c462cbe93e9fa8d16c20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generate(tokenizer, model, prompt: str, max_tokens: int = 1024):\n",
    "    messages = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "    inputs = tokenizer(messages, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id     # deterministic output\n",
    "    )\n",
    "\n",
    "    # Slice to get only the generated tokens (exclude prompt)\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508cb22",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">2) LLama 8B</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa08bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama8b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:26:20.516677: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-09 22:26:20.530578: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-09 22:26:20.547325: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-09 22:26:20.552226: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-09 22:26:20.564638: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-09 22:26:21.742362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6eef70ccfa8438a94f9891945bcaf05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "print(\"Loading llama8b model...\")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a218e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generate(tokenizer, model, prompt: str, max_tokens: int = 1024):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    encoded = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = encoded.to(model.device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,   # ✅ FIX\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_ids = outputs[0][input_ids.shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e83626",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">3) Phi 14B</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e6646cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:46:57.947876: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-09 22:46:57.961614: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-09 22:46:57.978365: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-09 22:46:57.983429: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-09 22:46:57.995897: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-09 22:47:00.401494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdeebb7a0bb4d789765baa3f04f9adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"microsoft/Phi-3-medium-4k-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "886e5068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generate(tokenizer, model, prompt: str, max_tokens: int = 1024):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,      # ✅ important\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_ids = outputs[0][input_ids.shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057a9b5",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">4) Qween 7B</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f594f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 23:48:32.991823: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-09 23:48:33.005838: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-09 23:48:33.023100: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-09 23:48:33.028221: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-09 23:48:33.040530: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-09 23:48:34.141629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1037f928afd4ed49b41af60f9adb17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c4f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def llm_generate(tokenizer, model, prompt: str, max_tokens: int = 1024):\n",
    "    # Build chat-style messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    # Tokenize using chat template\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Attention mask (important for some models)\n",
    "    attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Extract only newly generated tokens\n",
    "    generated_ids = outputs[0][input_ids.shape[1]:]\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return response.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b1b50f",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">5) Falcon 7B</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a79e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-25 22:52:03.711990: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-25 22:52:03.726094: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-25 22:52:03.740293: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-25 22:52:03.744536: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-25 22:52:03.757508: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-25 22:52:05.339296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9102a310469a428596a8a8ddba702ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"tiiuae/falcon-7b-instruct\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tiiuae/falcon-7b-instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44296f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generate(tokenizer, model, prompt: str, max_tokens: int = 1024):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=False,\n",
    "        use_cache=True,                    \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe2277a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- (a) the Prime Minister of India;\\n- (b) the Prime Minister of the State of India;\\n- (c) the Prime Minister of the Union Territory of Ladakh;\\n- (d) the Prime Minister of the Union Territory of Jammu and Kashmir;\\n- (e) the Prime Minister of the Union Territory of Dadra and Nagar Haveli and Daman and Diu;\\n- (f) the Prime Minister of the Union Territory of Lakshadweep;\\n- (g) the Prime Minister of the Union Territory of Puducherry;\\n- (h) the Prime Minister of the Union Territory of Dadra and Nagar Haveli and Daman and Diu;\\n- (i) the Prime Minister of the Union Territory of Jammu and Kashmir;\\n- (j) the Prime Minister of the Union Territory of Ladakh;\\n- (k) the Prime Minister of the Union Territory of Puducherry;\\n- (l) the Prime Minister of the Union Territory of Dadra and Nagar Haveli and Daman and Diu;\\n- (m) the Prime Minister of the Union Territory of Jammu and Kashmir;\\n- (n) the Prime Minister of the Union Territory of Ladakh;\\n- (o) the Prime Minister of the Union Territory of Puducherry;\\n- (p) the Prime Minister of the Union Territory of Dadra and Nagar Haveli and Daman and Diu;\\n- (q) the Prime Minister of the Union Territory of Jammu and Kashmir;\\n- (r) the Prime Minister of the Union Territory of Ladakh;\\n- (s) the Prime Minister of the Union Territory of Puducherry;\\n- (t) the Prime Minister of the Union Territory of Dadra and Nagar Haveli and Daman and Diu;\\n- (u) the Prime Minister of the Union Territory of Jammu and Kashmir;\\n- (v) the Prime Minister of the Union Territory of Ladakh;\\n- (w) the Prime Minister of the Union Territory of Puducherry;\\n- (x) the Prime Minister of the Union Territory of Dadra and Nagar Haveli and Daman and Diu;\\n- (y) the Prime Minister of the Union Territory of Jammu and Kashmir;\\n- (z) the Prime Minister of the Union Territory of Ladakh;\\n- (aa) the Prime Minister of the Union Territory of Dadra and Nagar Haveli and Daman and Diu;\\n- (bb) the Prime Minister of the Union Territory of Jammu and Kashmir;\\n- (cc) the Prime Minister of the Union Territory of Ladakh;\\n- (dd) the Prime Minister of the Union Territory of Puducherry;\\n- (ee) the Prime Minister of the Union Territory of Dadra and Nagar Haveli and Daman and Diu;\\n- (ff) the Prime Minister of the Union Territory of Jammu and Kashmir;\\n- (gg) the Prime Minister of the Union Territory of Ladakh;\\n- (hh) the Prime Minister of the Union Territory of Dadra and Nagar Haveli and Daman and Diu;\\n- (ii) the Prime Minister of the Union Territory of Jammu and Kashmir;\\n- (jj) the Prime Minister of the Union Territory of Ladakh;\\n- (kk) the Prime Minister of the Union Territory of Dadra and Nagar Haveli and Daman and Diu;\\n- (ll) the Prime Minister of the Union Territory of Jammu and Kashmir;\\n- (mm) the Prime Minister of the Union Territory of Ladakh;\\n- (nn) the Prime Minister of the Union Territory of Dadra and Nagar Haveli and Daman and Diu;\\n- (oo) the Prime Minister of the Union Territory of Jammu and Kashmir;\\n- (pp) the Prime Minister of the Union Territory of Ladakh;\\n- (qq) the Prime Minister of the Union Territory of Dadra and Nagar Haveli and Daman and Diu;\\n- (rr) the Prime Minister of the Union Territory of Jammu and Kashmir;\\n- (ss) the Prime Minister of the Union Territory of Ladakh;\\n- (tt) the Prime Minister of the Union Territory of Dadra and Nagar Haveli and Daman and Diu;\\n- (uu) the Prime Minister of the Union Territory of Jammu and Kashmir;\\n- (vv) the Prime Minister of the Union Territory of Ladakh;\\n- (w) the Prime Minister of the Union Territory of Dadra and Nagar Haveli and Daman and Diu;\\n- (xx) the Prime Minister of the Union Territory of Jammu and'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_generate(tokenizer, model, \"pm of india\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f68248a",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue;\">JSON Formatiing</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952819fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def clean_json_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the LLM response to ensure it's valid JSON.\n",
    "    \"\"\"\n",
    "    # Remove markdown code fences\n",
    "    cleaned = re.sub(r\"```(?:json)?\", \"\", response).strip()\n",
    "    \n",
    "    # Try to find the JSON array list\n",
    "    start = cleaned.find(\"[\")\n",
    "    end = cleaned.rfind(\"]\")\n",
    "    \n",
    "    if start != -1 and end != -1:\n",
    "        cleaned = cleaned[start : end + 1]\n",
    "        \n",
    "    return cleaned\n",
    "\n",
    "def generate_mcqs_from_chunks(\n",
    "    chunks: list,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    csv_path: str,\n",
    "    pdf_name: str\n",
    "):\n",
    "    \"\"\"Generate MCQs and write to CSV immediately after each MCQ\"\"\"\n",
    "    \n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    # Write CSV header first\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"pdf_name\",\n",
    "            \"chunk_number\",\n",
    "            \"total_chunks\",\n",
    "            \"pages\",\n",
    "            \"question\",\n",
    "            \"option_A\",\n",
    "            \"option_B\",\n",
    "            \"option_C\",\n",
    "            \"option_D\",\n",
    "            \"correct_answer\"\n",
    "        ])\n",
    "    \n",
    "    total_mcqs = 0\n",
    "    skipped_mcqs = 0\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_number = chunk.get(\"chunk_number\", i + 1)  # Use chunk_number from JSON or fallback\n",
    "        print(f\"\\nProcessing chunk {chunk_number}/{total_chunks}...\")\n",
    "\n",
    "        try:\n",
    "            prompt = build_mcq_prompt(\n",
    "                text=chunk[\"text\"]\n",
    "            )\n",
    "\n",
    "            response = llm_generate(tokenizer, model, prompt)\n",
    "\n",
    "            # --- JSON SAFETY ---\n",
    "            try:\n",
    "                cleaned_response = clean_json_response(response)\n",
    "                mcq_list = json.loads(cleaned_response)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[WARN] JSON parse failed for chunk {chunk_number}\")\n",
    "                print(f\"ERROR: {e}\")\n",
    "                print(f\"RAW RESPONSE:\\n{response}\\n\")\n",
    "                print(f\"Skipping chunk {chunk_number} and continuing...\")\n",
    "                continue\n",
    "\n",
    "            # Ensure mcq_list is actually a list\n",
    "            if not isinstance(mcq_list, list):\n",
    "                print(f\"[WARN] Response is not a list for chunk {chunk_number}\")\n",
    "                print(f\"Skipping chunk {chunk_number} and continuing...\")\n",
    "                continue\n",
    "\n",
    "            # Write each MCQ immediately to CSV\n",
    "            with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                \n",
    "                for mcq_idx, mcq in enumerate(mcq_list):\n",
    "                    try:\n",
    "                        # Validate MCQ structure\n",
    "                        if not isinstance(mcq, dict):\n",
    "                            print(f\"  [SKIP] MCQ {mcq_idx+1} is not a dict, skipping...\")\n",
    "                            skipped_mcqs += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Check required fields\n",
    "                        if \"question\" not in mcq or \"options\" not in mcq or \"answer\" not in mcq:\n",
    "                            print(f\"  [SKIP] MCQ {mcq_idx+1} missing required fields, skipping...\")\n",
    "                            skipped_mcqs += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Validate options\n",
    "                        options = mcq[\"options\"]\n",
    "                        if not isinstance(options, dict) or not all(key in options for key in [\"A\", \"B\", \"C\", \"D\"]):\n",
    "                            print(f\"  [SKIP] MCQ {mcq_idx+1} has invalid options structure, skipping...\")\n",
    "                            skipped_mcqs += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Validate answer key\n",
    "                        answer_key = mcq[\"answer\"]\n",
    "                        if answer_key not in options:\n",
    "                            print(f\"  [SKIP] MCQ {mcq_idx+1} has invalid answer key '{answer_key}', skipping...\")\n",
    "                            skipped_mcqs += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Format correct answer as \"D) Option Text\"\n",
    "                        answer_text = options[answer_key]\n",
    "                        formatted_answer = f\"{answer_key}) {answer_text}\"\n",
    "                        \n",
    "                        # Convert pages list to string\n",
    "                        pages_str = \", \".join(map(str, chunk[\"pages\"]))\n",
    "                        \n",
    "                        writer.writerow([\n",
    "                            pdf_name,\n",
    "                            chunk_number,\n",
    "                            pages_str,\n",
    "                            mcq[\"question\"],\n",
    "                            options[\"A\"],\n",
    "                            options[\"B\"],\n",
    "                            options[\"C\"],\n",
    "                            options[\"D\"],\n",
    "                            formatted_answer\n",
    "                        ])\n",
    "                        \n",
    "                        total_mcqs += 1\n",
    "                        # print(f\"  ✓ Generated MCQ #{total_mcqs}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"  [ERROR] Failed to process MCQ {mcq_idx+1}: {e}\")\n",
    "                        print(f\"  Skipping this MCQ and continuing...\")\n",
    "                        skipped_mcqs += 1\n",
    "                        continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process chunk {chunk_number}: {e}\")\n",
    "            print(f\"Skipping chunk {chunk_number} and continuing...\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✅ Completed!\")\n",
    "    print(f\"   Generated: {total_mcqs} MCQs\")\n",
    "    print(f\"   Skipped: {skipped_mcqs} MCQs\")\n",
    "    print(f\"   Processed: {total_chunks} chunks\")\n",
    "    return total_mcqs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a8e09",
   "metadata": {},
   "source": [
    "#### Inference the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "191a7298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 114 pages\n",
      "Created 57 chunks\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract pages from the PDF\n",
    "pdf_name = \"notes/LLM_cs124_week7_2025.pdf\"\n",
    "pages = extract_pdf_pages(pdf_name)\n",
    "\n",
    "# Step 2: Chunk (3 pages, 1-page overlap)\n",
    "chunks = create_page_chunks(\n",
    "    pages,\n",
    "    chunk_size=3,\n",
    "    overlap=1\n",
    ")\n",
    "\n",
    "# Step 3: Store to JSON\n",
    "save_chunks(chunks, \"results/without_image/metdata/qween_pdf_chunks.json\")\n",
    "\n",
    "print(f\"Extracted {len(pages)} pages\")\n",
    "print(f\"Created {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29c0c1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1/57...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 2/57...\n",
      "\n",
      "Processing chunk 3/57...\n",
      "\n",
      "Processing chunk 4/57...\n",
      "\n",
      "Processing chunk 5/57...\n",
      "\n",
      "Processing chunk 6/57...\n",
      "\n",
      "Processing chunk 7/57...\n",
      "\n",
      "Processing chunk 8/57...\n",
      "\n",
      "Processing chunk 9/57...\n",
      "\n",
      "Processing chunk 10/57...\n",
      "\n",
      "Processing chunk 11/57...\n",
      "\n",
      "Processing chunk 12/57...\n",
      "\n",
      "Processing chunk 13/57...\n",
      "\n",
      "Processing chunk 14/57...\n",
      "[WARN] JSON parse failed for chunk 14\n",
      "ERROR: Expecting value: line 46 column 12 (char 1812)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"If the dot product between the query vector qi and the key vector kj results in a high value, what does this indicate about the relationship between the two vectors?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"The vectors are dissimilar.\",\n",
      "      \"B\": \"The vectors are highly similar.\",\n",
      "      \"C\": \"The vectors are orthogonal.\",\n",
      "      \"D\": \"The vectors are identical.\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Given the sequence 'The chickendidn’tcrosstheroadbecauseitwastootired', which vector would be considered the current focus of attention when computing the attention score for the word 'crossthe'?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"x1\",\n",
      "      \"B\": \"x5\",\n",
      "      \"C\": \"x6\",\n",
      "      \"D\": \"x7\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"In the self-attention mechanism, how does the softmax function ensure that the weights assigned to the vectors are normalized?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"By ensuring all weights are equal.\",\n",
      "      \"B\": \"By summing the weights to 1.\",\n",
      "      \"C\": \"By setting the highest weight to 1.\",\n",
      "      \"D\": \"By multiplying the weights by the total number of vectors.\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is the role of the value vector vi in the self-attention mechanism?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"It determines the similarity between the query and key vectors.\",\n",
      "      \"B\": \"It is used to compute the final output after weighting and summing.\",\n",
      "      \"C\": \"It represents the current focus of attention.\",\n",
      "      \"D\": \"It serves as a key for comparing with the query vector.\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Why can the computations at each time step in the self-attention model be performed in parallel, unlike in RNNs?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"Because the computations depend on future inputs.\",\n",
      "      \"B\": Because the computations do not depend on the results of previous steps.\n",
      "      \"C\": \"Because the model uses a sequential processing approach.\",\n",
      "      \"D\": \"Because the model requires a fixed sequence length.\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 14 and continuing...\n",
      "\n",
      "Processing chunk 15/57...\n",
      "\n",
      "Processing chunk 16/57...\n",
      "[WARN] JSON parse failed for chunk 16\n",
      "ERROR: Invalid \\escape: line 3 column 50 (char 55)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What role does the query vector \\( q_i \\) play in the attention mechanism?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"It represents the current element being compared to preceding inputs.\",\n",
      "      \"B\": \"It represents the preceding input being compared to the current element.\",\n",
      "      \"C\": \"It represents the value of a preceding element.\",\n",
      "      \"D\": \"It scales the dot product between the current element and preceding elements.\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Why is the dot product between the query vector \\( q_i \\) and the key vector \\( k_j \\) scaled by \\( \\frac{1}{\\sqrt{d_k}} \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"To increase the numerical range of the dot product.\",\n",
      "      \"B\": \"To ensure the dot product remains within a certain range to prevent numerical issues.\",\n",
      "      \"C\": \"To decrease the importance of the query vector.\",\n",
      "      \"D\": \"To make the computation faster.\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which of the following correctly represents the computation of the attention score \\( a_{ij} \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"a_{ij} = softmax(q_i \\cdot k_j)\",\n",
      "      \"B\": \"a_{ij} = softmax(k_j \\cdot v_j)\",\n",
      "      \"C\": \"a_{ij} = softmax(v_j \\cdot q_i)\",\n",
      "      \"D\": \"a_{ij} = softmax(v_i \\cdot k_j)\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"How is the final output vector \\( a_i \\) computed from the attention mechanism?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"By taking the average of all value vectors \\( v_j \\).\",\n",
      "      \"B\": \"By summing the value vectors \\( v_j \\) weighted by their respective attention scores \\( a_{ij} \\).\",\n",
      "      \"C\": \"By multiplying the query vector \\( q_i \\) with the key vector \\( k_j \\).\",\n",
      "      \"D\": \"By subtracting the value vectors \\( v_j \\) from the query vector \\( q_i \\).\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What would be the impact if the softmax function in the attention mechanism was replaced with a linear function?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"It would simplify the computation but reduce the ability to model dependencies.\",\n",
      "      \"B\": \"It would increase the gradient flow during backpropagation.\",\n",
      "      \"C\": \"It would enhance the model's ability to learn complex patterns.\",\n",
      "      \"D\": \"It would make the model computationally more efficient without any loss of accuracy.\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 16 and continuing...\n",
      "\n",
      "Processing chunk 17/57...\n",
      "[WARN] JSON parse failed for chunk 17\n",
      "ERROR: Invalid \\escape: line 3 column 76 (char 81)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"When computing the similarity between the current element \\( x_i \\) and a prior element \\( x_j \\), which operation is used according to the lecture content?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"Dot product between \\( q_i \\) and \\( k_j \\)\",\n",
      "      \"B\": \"Dot product between \\( q_i \\) and \\( v_j \\)\",\n",
      "      \"C\": \"Dot product between \\( k_i \\) and \\( v_j \\)\",\n",
      "      \"D\": \"Dot product between \\( k_i \\) and \\( k_j \\)\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is the primary reason for scaling the dot product by \\( \\frac{1}{\\sqrt{d_k}} \\) in the attention mechanism?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"To ensure the softmax function always outputs a value between 0 and 1\",\n",
      "      \"B\": \"To prevent numerical issues due to large values\",\n",
      "      \"C\": \"To increase the computational complexity of the model\",\n",
      "      \"D\": \"To reduce the dimensionality of the input vectors\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which of the following correctly represents the final weighted sum of values for the attention mechanism output \\( a_i \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \" \\( a_i = \\sum_{j < i} a_{ij} k_j \\)\",\n",
      "      \"B\": \" \\( a_i = \\sum_{j < i} a_{ij} v_j \\)\",\n",
      "      \"C\": \" \\( a_i = \\sum_{j > i} a_{ij} v_j \\)\",\n",
      "      \"D\": \" \\( a_i = \\sum_{j > i} a_{ij} k_j \\)\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"If the query vector \\( q_i \\) and the key vector \\( k_j \\) are highly similar, what effect will this have on the computed attention score \\( a_{ij} \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"The attention score will be low, indicating poor similarity\",\n",
      "      \"B\": \"The attention score will be high, indicating strong similarity\",\n",
      "      \"C\": \"The attention score will be zero, indicating no similarity\",\n",
      "      \"D\": \"The attention score will be negative, indicating inverse similarity\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"In the context of the attention mechanism, what does the matrix \\( WQ \\) do to the input vector \\( x_i \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"Projects \\( x_i \\) into a key representation\",\n",
      "      \"B\": \"Projects \\( x_i \\) into a query representation\",\n",
      "      \"C\": \"Projects \\( x_i \\) into a value representation\",\n",
      "      \"D\": \"Normalizes \\( x_i \\) into a probability distribution\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 17 and continuing...\n",
      "\n",
      "Processing chunk 18/57...\n",
      "[WARN] JSON parse failed for chunk 18\n",
      "ERROR: Invalid \\escape: line 3 column 83 (char 88)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"In the self-attention mechanism, what role does the query vector \\( q_i \\) play in the computation of the similarity score between the current element and a prior element?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"It represents the current element being compared to the prior elements.\",\n",
      "      \"B\": \"It represents the prior elements being compared to the current element.\",\n",
      "      \"C\": \"It is used to weigh the value vectors after the softmax operation.\",\n",
      "      \"D\": \"It projects the input vector into a key representation.\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"How is the similarity score between the query vector \\( q_i \\) and the key vector \\( k_j \\) calculated in the self-attention mechanism?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"By taking the dot product of \\( q_i \\) and \\( k_j \\) without any scaling.\",\n",
      "      \"B\": \"By taking the dot product of \\( q_i \\) and \\( k_j \\) and then dividing by the dimensionality of the key vector \\( dk \\).\",\n",
      "      \"C\": \"By taking the dot product of \\( q_i \\) and \\( k_j \\) and then multiplying by the dimensionality of the key vector \\( dk \\).\",\n",
      "      \"D\": \"By taking the dot product of \\( q_i \\) and \\( k_j \\) and then applying a sigmoid function.\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which of the following equations correctly represents the calculation of the weighted sum of the value vectors \\( v \\) after the softmax operation in the self-attention mechanism?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"head_i = softmax(score(xi,xj)) * vj\",\n",
      "      \"B\": \"head_i = sum(vj / softmax(score(xi,xj)))\",\n",
      "      \"C\": \"head_i = sum(softmax(score(xi,xj)) * vj)\",\n",
      "      \"D\": \"head_i = sum(vj) / softmax(score(xi,xj))\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"In the multi-head attention mechanism, why is it beneficial to use multiple attention heads?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"To increase the computational complexity of the model.\",\n",
      "      \"B\": \"To allow the model to attend to different aspects of the context for different purposes.\",\n",
      "      \"C\": \"To reduce the dimensionality of the input vectors.\",\n",
      "      \"D\": \"To make the model more susceptible to overfitting.\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is the purpose of the matrix \\( W_O \\) in the self-attention mechanism?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"To project the input vectors into key representations.\",\n",
      "      \"B\": \"To project the output of the attention head back to the original dimensionality.\",\n",
      "      \"C\": \"To calculate the similarity scores between queries and keys.\",\n",
      "      \"D\": \"To apply the softmax function to the scores.\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 18 and continuing...\n",
      "\n",
      "Processing chunk 19/57...\n",
      "\n",
      "Processing chunk 20/57...\n",
      "\n",
      "Processing chunk 21/57...\n",
      "[WARN] JSON parse failed for chunk 21\n",
      "ERROR: Invalid \\escape: line 33 column 47 (char 1642)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"Which of the following best describes the role of the residual stream in the transformer architecture?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"It bypasses the attention and feedforward layers entirely.\",\n",
      "      \"B\": \"It allows tokens to be processed independently without any connections.\",\n",
      "      \"C\": \"It enables modifications to be passed up and modified by subsequent components before being added back into the stream.\",\n",
      "      \"D\": \"It is used to normalize the entire input sequence before processing.\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"In the context of the transformer block, what happens after the layer norm and attention layer computations?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"The output is discarded and not added back into the residual stream.\",\n",
      "      \"B\": \"The output is directly passed to the next layer without modification.\",\n",
      "      \"C\": \"The output is added back into the residual stream.\",\n",
      "      \"D\": \"The output is normalized again before being added to the stream.\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is the typical relationship between the dimensionality of the hidden layer in the feedforward network and the model dimensionality?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"The hidden layer dimensionality is smaller than the model dimensionality.\",\n",
      "      \"B\": \"The hidden layer dimensionality is equal to the model dimensionality.\",\n",
      "      \"C\": \"The hidden layer dimensionality is larger than the model dimensionality.\",\n",
      "      \"D\": \"The hidden layer dimensionality is unrelated to the model dimensionality.\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"How many times is the vector \\( x_i \\) normalized in the transformer block?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"Once\",\n",
      "      \"B\": \"Twice\",\n",
      "      \"C\": \"Three times\",\n",
      "      \"D\": \"Four times\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which formula correctly represents the feedforward layer computation in the transformer block?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"FFN(x_i) = x_iW_1 + b_1\",\n",
      "      \"B\": \"FFN(x_i) = ReLU(x_iW_1 + b_1)W_2 + b_2\",\n",
      "      \"C\": \"FFN(x_i) = x_iW_2 + b_2\",\n",
      "      \"D\": \"FFN(x_i) = ReLU(x_iW_1 + b_1)W_2\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 21 and continuing...\n",
      "\n",
      "Processing chunk 22/57...\n",
      "[WARN] JSON parse failed for chunk 22\n",
      "ERROR: Invalid \\escape: line 13 column 44 (char 584)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"In the context of a transformer block, what is the primary purpose of applying Layer Normalization after the MultiHeadAttention component?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"To introduce non-linearity into the model\",\n",
      "      \"B\": \"To normalize the input embeddings before feeding them into the MultiHeadAttention layer\",\n",
      "      \"C\": \"To ensure the input and output vectors have similar distributions, facilitating gradient flow\",\n",
      "      \"D\": \"To increase the sparsity of the output vectors\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Given an embedding vector \\( x \\) of dimensionality \\( d \\), which of the following correctly represents the first step in calculating the mean \\( \\mu \\) for Layer Normalization?\",\n",
      "    \"options\": {\n",
      "      \"A\": \" \\( \\mu = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu) \\)\",\n",
      "      \"B\": \" \\( \\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i \\)\",\n",
      "      \"C\": \" \\( \\mu = \\frac{1}{d} \\sum_{i=1}^{d} (x_i + \\mu) \\)\",\n",
      "      \"D\": \" \\( \\mu = \\frac{1}{d} \\sum_{i=1}^{d} (x_i^2 - \\mu) \\)\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which of the following equations correctly represents the Layer Normalization process for a given vector \\( x \\) of dimensionality \\( d \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \" \\( \\text{LayerNorm}(x) = g(x - \\mu) / s \\)\",\n",
      "      \"B\": \" \\( \\text{LayerNorm}(x) = g(x - \\mu) / b \\)\",\n",
      "      \"C\": \" \\( \\text{LayerNorm}(x) = g(x - \\mu) / s + b \\)\",\n",
      "      \"D\": \" \\( \\text{LayerNorm}(x) = g(x - s) / \\mu + b \\)\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"In the transformer block, after the Layer Normalization step \\( t1_i \\), what is the next computational step involving the MultiHeadAttention component?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"Adding the input vector \\( x_i \\) to the output of MultiHeadAttention\",\n",
      "      \"B\": \"Applying a feedforward network to the output of MultiHeadAttention\",\n",
      "      \"C\": \"Calculating the mean and standard deviation of the output of MultiHeadAttention\",\n",
      "      \"D\": \"Introducing learnable parameters \\( g \\) and \\( b \\) to the output of MultiHeadAttention\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which of the following best describes the role of the residual connection in the transformer block?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"It allows the model to learn long-range dependencies by skipping layers\",\n",
      "      \"B\": \"It normalizes the entire transformer layer by scaling and shifting the inputs\",\n",
      "      \"C\": \"It ensures that the input and output of the block have similar distributions, aiding in gradient flow\",\n",
      "      \"D\": \"It increases the depth of the model by adding multiple parallel paths\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 22 and continuing...\n",
      "\n",
      "Processing chunk 23/57...\n",
      "[WARN] JSON parse failed for chunk 23\n",
      "ERROR: Invalid \\escape: line 3 column 79 (char 84)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"In the context of layer normalization, what does the formula \\( \\hat{x} = \\frac{(x - \\mu)}{s} \\) represent?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"The process of calculating the mean of the vector elements\",\n",
      "      \"B\": \"The normalization step where each element is adjusted by subtracting the mean and dividing by the standard deviation\",\n",
      "      \"C\": \"The calculation of the standard deviation of the vector elements\",\n",
      "      \"D\": \"The final transformation including learnable parameters g and b\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which of the following correctly represents the computation inside a transformer block according to the given sequence?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"t1i = MultiHeadAttention(t1i, x11, ..., x1N)\",\n",
      "      \"B\": \"t3i = t2i + xi\",\n",
      "      \"C\": \"t5i = FFN(t4i)\",\n",
      "      \"D\": \"t4i = LayerNorm(t3i)\",\n",
      "      \"E\": \"hi = t5i * t3i\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"How does the multi-head attention mechanism within a transformer block utilize information from other tokens?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"By directly adding the attention output to the current token's embedding\",\n",
      "      \"B\": \"By multiplying the attention output with the current token's embedding\",\n",
      "      \"C\": \"By using the attention output to replace the current token's embedding\",\n",
      "      \"D\": \"By adding the attention output to the current token's embedding after normalization\"\n",
      "    },\n",
      "    \"answer\": \"D\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What role do the learnable parameters \\( g \\) and \\( b \\) play in the layer normalization process?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"They calculate the mean and standard deviation of the vector elements\",\n",
      "      \"B\": \"They are used to scale and shift the normalized vector components\",\n",
      "      \"C\": \"They are used to compute the multi-head attention scores\",\n",
      "      \"D\": \"They define the dimensions of the input vector\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"In the given sequence of operations, which step involves adding the output of the feedforward network to the residual connection of the previous block?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"t1i = LayerNorm(xi)\",\n",
      "      \"B\": \"t3i = t2i + xi\",\n",
      "      \"C\": \"t4i = LayerNorm(t3i)\",\n",
      "      \"D\": \"hi = t5i + t3i\",\n",
      "      \"E\": \"t5i = FFN(t4i)\"\n",
      "    },\n",
      "    \"answer\": \"D\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 23 and continuing...\n",
      "\n",
      "Processing chunk 24/57...\n",
      "\n",
      "Processing chunk 25/57...\n",
      "\n",
      "Processing chunk 26/57...\n",
      "\n",
      "Processing chunk 27/57...\n",
      "\n",
      "Processing chunk 28/57...\n",
      "\n",
      "Processing chunk 29/57...\n",
      "\n",
      "Processing chunk 30/57...\n",
      "\n",
      "Processing chunk 31/57...\n",
      "\n",
      "Processing chunk 32/57...\n",
      "\n",
      "Processing chunk 33/57...\n",
      "\n",
      "Processing chunk 34/57...\n",
      "[WARN] JSON parse failed for chunk 34\n",
      "ERROR: Invalid \\escape: line 3 column 101 (char 106)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"In the process of teacher forcing, what action is taken at the next token position \\( t+1 \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"We use the correct word \\( w_{t+1} \\) that was predicted by the model.\",\n",
      "      \"B\": \"We ignore the correct word \\( w_{t+1} \\) and let the model predict again.\",\n",
      "      \"C\": \"We use a random word from the vocabulary to continue the sequence.\",\n",
      "      \"D\": \"We compute the loss based on the predicted word \\( w_t \\) at position \\( t \\).\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which of the following best describes the purpose of input encoding in a Transformer model?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"To decode the final output into human-readable text.\",\n",
      "      \"B\": \"To convert raw input data into a format that can be processed by the Transformer blocks.\",\n",
      "      \"C\": \"To calculate the loss function for training the model.\",\n",
      "      \"D\": \"To predict the next token in the sequence without considering the previous context.\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which of the following datasets is primarily used for training large language models like those described in the lecture?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"IMDB Movie Reviews\",\n",
      "      \"B\": \"Common Crawl and Colossal Clean Crawled Corpus (C4)\",\n",
      "      \"C\": \"MNIST Digit Dataset\",\n",
      "      \"D\": \"CIFAR-10 Image Dataset\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"In the context of training a Transformer model, why might a dataset be filtered to remove boilerplate, adult content, and toxicity?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"To reduce the size of the dataset and speed up training.\",\n",
      "      \"B\": \"To improve the quality of the training data and avoid bias.\",\n",
      "      \"C\": \"To increase the diversity of the training data.\",\n",
      "      \"D\": \"To make the training process more complex and challenging.\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What type of tokens does the Colossal Clean Crawled Corpus (C4) predominantly consist of after filtering?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"Sports event descriptions\",\n",
      "      \"B\": \"Patent text documents, Wikipedia, and news sites\",\n",
      "      \"C\": \"Scientific research papers\",\n",
      "      \"D\": \"User-generated content from social media platforms\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 34 and continuing...\n",
      "\n",
      "Processing chunk 35/57...\n",
      "\n",
      "Processing chunk 36/57...\n",
      "\n",
      "Processing chunk 37/57...\n",
      "\n",
      "Processing chunk 38/57...\n",
      "\n",
      "Processing chunk 39/57...\n",
      "\n",
      "Processing chunk 40/57...\n",
      "\n",
      "Processing chunk 41/57...\n",
      "\n",
      "Processing chunk 42/57...\n",
      "\n",
      "Processing chunk 43/57...\n",
      "[WARN] JSON parse failed for chunk 43\n",
      "ERROR: Invalid \\escape: line 13 column 74 (char 492)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"In the context of updating network weights using backpropagation, what does the term 'blame' refer to?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"The amount by which a weight contributes to the loss\",\n",
      "      \"B\": \"The initial value of the weight before training\",\n",
      "      \"C\": \"The final value of the weight after training\",\n",
      "      \"D\": \"The learning rate used during training\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"If a weight in the network is updated using the formula \\( w_{\\text{new}} = w - \\eta \\frac{\\partial L}{\\partial w} \\), where \\(\\eta = 0.01\\), what does \\(\\eta\\) represent?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"The derivative of the loss function with respect to the weight\",\n",
      "      \"B\": \"The learning rate that controls the step size of weight updates\",\n",
      "      \"C\": \"The target value used for training\",\n",
      "      \"D\": \"The new value of the weight after the update\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which of the following best describes the process of running forward computation in a neural network?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"Updating the weights to minimize the loss\",\n",
      "      \"B\": \"Propagating the input through the network to produce an output\",\n",
      "      \"C\": \"Calculating the gradient of the loss function\",\n",
      "      \"D\": \"Determining the chain of blame for the current loss\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"During backpropagation, if the loss function is squared error, what would be the partial derivative of the loss with respect to the predicted output \\(y\\) when the true output \\(y_{true}\\) is 5 and the predicted output \\(y\\) is 3?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"2\",\n",
      "      \"B\": \"-2\",\n",
      "      \"C\": \"4\",\n",
      "      \"D\": \"-4\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"When assessing the blame for a current loss in a neural network, which mathematical concept is primarily used?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"Chain rule\",\n",
      "      \"B\": \"Forward propagation\",\n",
      "      \"C\": \"Backward propagation\",\n",
      "      \"D\": \"Gradient descent\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 43 and continuing...\n",
      "\n",
      "Processing chunk 44/57...\n",
      "[WARN] JSON parse failed for chunk 44\n",
      "ERROR: Invalid \\escape: line 3 column 47 (char 52)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What is the initial value of \\( y \\) after the forward pass with the given initial weights and input?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"5\",\n",
      "      \"B\": \"11\",\n",
      "      \"C\": \"12\",\n",
      "      \"D\": \"13\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"If the true output \\( y_{true} \\) is 10, what is the loss \\( L \\) before any weight updates?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"0\",\n",
      "      \"B\": \"4\",\n",
      "      \"C\": \"10\",\n",
      "      \"D\": \"16\"\n",
      "    },\n",
      "    \"answer\": \"D\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which of the following formulas correctly represents the update rule for a weight \\( w \\) during backpropagation?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"wnew = w - η * !\\\"!#\",\n",
      "      \"B\": \"wnew = w + η * !\\\"!#\",\n",
      "      \"C\": \"wnew = w - η / !\\\"!#\",\n",
      "      \"D\": \"wnew = w + η / !\\\"!#\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"In the context of this example, what does the term 'blame' refer to in the weight update process?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"The amount by which the weight should increase to reduce loss\",\n",
      "      \"B\": \"The amount by which the weight should decrease to reduce loss\",\n",
      "      \"C\": \"The total loss calculated from the model's prediction\",\n",
      "      \"D\": \"The difference between the predicted and true outputs\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Given that the loss function used is squared loss, what would be the new value of \\( y \\) after updating the weights if we assume the learning rate \\( η \\) is 0.01 and the partial derivative \\( !\\\"!# \\) for \\( w1 \\) is -0.5?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"11.5\",\n",
      "      \"B\": \"11.9\",\n",
      "      \"C\": \"12.1\",\n",
      "      \"D\": \"12.5\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 44 and continuing...\n",
      "\n",
      "Processing chunk 45/57...\n",
      "\n",
      "Processing chunk 46/57...\n",
      "[WARN] JSON parse failed for chunk 46\n",
      "ERROR: Invalid \\escape: line 3 column 95 (char 100)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"In the context of the computation graph, what role do the intermediate nodes \\(h1\\) and \\(h2\\) play during the backward pass?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"They store the final output before calculating the loss.\",\n",
      "      \"B\": \"They represent the gradients of the loss with respect to the inputs \\(x1\\) and \\(x2\\).\",\n",
      "      \"C\": \"They are used to compute the gradients of the loss with respect to the weights \\(w1\\) and \\(w2\\).\",\n",
      "      \"D\": \"They are placeholders for the true values of the labels.\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"If the loss function \\(L = (y_{true} - y)^2\\) and the output \\(y = h1 + h2 + b\\), how would you compute the gradient of \\(L\\) with respect to \\(b\\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"∂L/∂b = ∂(y_{true} - y)^2 / ∂b = 2(y - y_{true})\",\n",
      "      \"B\": \"∂L/∂b = ∂(y_{true} - y)^2 / ∂b = 2(y_{true} - h1 - h2)\",\n",
      "      \"C\": \"∂L/∂b = ∂(y_{true} - y)^2 / ∂b = 2(y - h1 - h2)\",\n",
      "      \"D\": \"∂L/∂b = ∂(y_{true} - y)^2 / ∂b = 2(h1 + h2 - y_{true})\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Given the computation graph where \\(h1 = w1x1\\) and \\(h2 = w2x2\\), how would you calculate the gradient of the loss \\(L\\) with respect to \\(w1\\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"∂L/∂w1 = ∂L/∂h1 * ∂h1/∂w1\",\n",
      "      \"B\": \"∂L/∂w1 = ∂L/∂h1 * ∂h1/∂x1\",\n",
      "      \"C\": \"∂L/∂w1 = ∂L/∂y * ∂y/∂h1 * ∂h1/∂w1\",\n",
      "      \"D\": \"∂L/∂w1 = ∂L/∂y * ∂y/∂h1 * ∂h1/∂x1\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"In the given computation graph, if the true label \\(y_{true} = 5\\), the predicted output \\(y = 3\\), and the intermediate values \\(h1 = 2\\) and \\(h2 = 1\\), what is the value of the loss \\(L\\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"L = 1\",\n",
      "      \"B\": \"L = 4\",\n",
      "      \"C\": \"L = 9\",\n",
      "      \"D\": \"L = 16\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Why might it be useful to include intermediate nodes like \\(h1\\) and \\(h2\\) in the computation graph when performing backpropagation?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"To make the graph more complex and harder to optimize.\",\n",
      "      \"B\": \"To allow for easier parallel processing of the inputs.\",\n",
      "      \"C\": \"To facilitate the calculation of gradients for the weights by breaking down the computation into smaller parts.\",\n",
      "      \"D\": \"To reduce the number of operations needed for the forward pass.\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 46 and continuing...\n",
      "\n",
      "Processing chunk 47/57...\n",
      "[WARN] JSON parse failed for chunk 47\n",
      "ERROR: Invalid \\escape: line 3 column 36 (char 41)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"If the true label \\( y_{true} \\) is 0.8 and the predicted label \\( y \\) is 0.6, what is the gradient of the loss with respect to \\( w_1 \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"0.8\",\n",
      "      \"B\": \"-0.4\",\n",
      "      \"C\": \"0.4\",\n",
      "      \"D\": \"-0.8\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Given the formula for the loss \\( L = (y_{true} - y)^2 \\), if the loss needs to be minimized, which of the following correctly represents the derivative of \\( L \\) with respect to \\( h_1 \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"2 * (y_{true} - y)\",\n",
      "      \"B\": \"2 * (y_{true} - y) * w_1\",\n",
      "      \"C\": \"2 * (y_{true} - y) * x_1\",\n",
      "      \"D\": \"2 * (y_{true} - y) * w_2\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"If the input \\( x_1 \\) is 2 and the weight \\( w_1 \\) is 0.5, what is the gradient of the loss with respect to \\( x_1 \\) when the predicted value \\( y \\) is 0.7 and the true value \\( y_{true} \\) is 0.6?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"0.2\",\n",
      "      \"B\": \"0.1\",\n",
      "      \"C\": \"-0.1\",\n",
      "      \"D\": \"-0.2\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"In the context of backpropagation, if the loss function \\( L \\) is defined as \\( (y_{true} - y)^2 \\), which of the following best describes the role of the term \\( 2(y_{true} - y) \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"It updates the bias \\( b \\).\",\n",
      "      \"B\": \"It propagates the error backward through the network.\",\n",
      "      \"C\": \"It calculates the initial loss value.\",\n",
      "      \"D\": \"It adjusts the learning rate.\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"If the hidden layer output \\( h_1 \\) is calculated as \\( w_1 x_1 + b \\) and the loss \\( L \\) is given by \\( (y_{true} - y)^2 \\), what is the gradient of the loss with respect to \\( h_1 \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"2 * (y_{true} - y)\",\n",
      "      \"B\": \"2 * (y_{true} - y) * w_1\",\n",
      "      \"C\": \"2 * (y_{true} - y) * x_1\",\n",
      "      \"D\": \"2 * (y_{true} - y) * b\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 47 and continuing...\n",
      "\n",
      "Processing chunk 48/57...\n",
      "[WARN] JSON parse failed for chunk 48\n",
      "ERROR: Invalid \\escape: line 3 column 37 (char 42)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"If the true output \\(y_{true}\\) is 10 and the predicted output \\(y\\) is 12, what is the gradient of the loss with respect to \\(w_1\\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"4\",\n",
      "      \"B\": \"-12\",\n",
      "      \"C\": \"8\",\n",
      "      \"D\": \"2\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Given \\(w_2 = -1\\), \\(x_2 = -3\\), and the true output \\(y_{true} = 10\\), what is the gradient of the loss with respect to \\(w_2\\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"4\",\n",
      "      \"B\": \"-12\",\n",
      "      \"C\": \"8\",\n",
      "      \"D\": \"2\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"If \\(x_1 = 4\\) and the gradient of the loss with respect to \\(w_1\\) is 4, what is the gradient of the loss with respect to \\(h_1\\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"16\",\n",
      "      \"B\": \"8\",\n",
      "      \"C\": \"4\",\n",
      "      \"D\": \"12\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"If \\(y = 12\\) and \\(y_{true} = 10\\), what is the value of the loss function \\(L\\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"4\",\n",
      "      \"B\": \"12\",\n",
      "      \"C\": \"4\",\n",
      "      \"D\": \"144\"\n",
      "    },\n",
      "    \"answer\": \"D\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"If \\(w_1 = 2\\), \\(w_2 = -1\\), and \\(b = 1\\), what is the predicted output \\(y\\) when \\(x_1 = 4\\) and \\(x_2 = -3\\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"12\",\n",
      "      \"B\": \"10\",\n",
      "      \"C\": \"14\",\n",
      "      \"D\": \"16\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 48 and continuing...\n",
      "\n",
      "Processing chunk 49/57...\n",
      "[WARN] JSON parse failed for chunk 49\n",
      "ERROR: Invalid \\escape: line 3 column 44 (char 49)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"Given the initial weights \\( w1 = 2 \\), \\( w2 = -1 \\), and \\( b = 1 \\), if the learning rate \\( \\eta = 0.01 \\), what will be the updated value of \\( w1 \\) after one backpropagation step?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"1.84\",\n",
      "      \"B\": \"1.96\",\n",
      "      \"C\": \"2.12\",\n",
      "      \"D\": \"1.72\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"If the true output \\( y_{true} = 10 \\) and the predicted output \\( y = 12 \\), what is the gradient of the loss function with respect to \\( w2 \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"4\",\n",
      "      \"B\": \"-12\",\n",
      "      \"C\": \"12\",\n",
      "      \"D\": \"-4\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"After computing the gradients, if the initial weight \\( w1 = 2 \\) and the learning rate \\( \\eta = 0.01 \\), what is the gradient of the loss function with respect to \\( w1 \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"16\",\n",
      "      \"B\": \"4\",\n",
      "      \"C\": \"8\",\n",
      "      \"D\": \"12\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"In the context of backpropagation, if \\( x1 = 4 \\), \\( x2 = -3 \\), \\( y_{true} = 10 \\), and \\( y = 12 \\), what is the gradient of the loss function with respect to the bias \\( b \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"4\",\n",
      "      \"B\": \"-12\",\n",
      "      \"C\": \"12\",\n",
      "      \"D\": \"-4\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"If the initial weights are \\( w1 = 2 \\), \\( w2 = -1 \\), and \\( b = 1 \\), and after one backpropagation step, the updated weights are \\( w1 = 1.84 \\), \\( w2 = -0.88 \\), and \\( b = 0.96 \\), what was the gradient of the loss function with respect to \\( w2 \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"4\",\n",
      "      \"B\": \"-12\",\n",
      "      \"C\": \"12\",\n",
      "      \"D\": \"-4\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 49 and continuing...\n",
      "\n",
      "Processing chunk 50/57...\n",
      "[WARN] JSON parse failed for chunk 50\n",
      "ERROR: Invalid \\escape: line 3 column 87 (char 92)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"After one iteration of backpropagation, what is the updated value of \\( w_1 \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"1.84\",\n",
      "      \"B\": \"2.00\",\n",
      "      \"C\": \"1.00\",\n",
      "      \"D\": \"0.96\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is the partial derivative of the loss function with respect to \\( w_2 \\) after the first update?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"12\",\n",
      "      \"B\": \"-12\",\n",
      "      \"C\": \"4\",\n",
      "      \"D\": \"-3\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"If the learning rate \\( \\eta \\) was increased to 0.1, what would be the updated value of \\( b \\) after one iteration of backpropagation?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"0.96\",\n",
      "      \"B\": \"0.90\",\n",
      "      \"C\": \"1.00\",\n",
      "      \"D\": \"0.84\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"After one iteration of backpropagation, which of the following represents the new prediction for \\( y \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"10.96\",\n",
      "      \"B\": \"10.00\",\n",
      "      \"C\": \"9.56\",\n",
      "      \"D\": \"11.44\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What would be the updated value of \\( w_2 \\) if the initial values were \\( w_1 = 3 \\), \\( w_2 = -2 \\), \\( b = 2 \\), and the learning rate \\( \\eta = 0.01 \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"-1.98\",\n",
      "      \"B\": \"-1.12\",\n",
      "      \"C\": \"-2.00\",\n",
      "      \"D\": \"-0.98\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 50 and continuing...\n",
      "\n",
      "Processing chunk 51/57...\n",
      "\n",
      "Processing chunk 52/57...\n",
      "\n",
      "Processing chunk 53/57...\n",
      "[WARN] JSON parse failed for chunk 53\n",
      "ERROR: Invalid \\escape: line 3 column 84 (char 89)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"If the input sequence has 1024 tokens and the embedding dimension \\( d \\) is 512, what is the shape of the matrix \\( X \\)?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"[1024 x 512]\",\n",
      "      \"B\": \"[512 x 1024]\",\n",
      "      \"C\": \"[1024 x 1024]\",\n",
      "      \"D\": \"[512 x 512]\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"How does masking ensure that the model does not use future tokens during the attention computation?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"By setting the corresponding elements in the QK| matrix to zero for queries that precede keys.\",\n",
      "      \"B\": \"By setting the corresponding elements in the QK| matrix to zero for queries that follow keys.\",\n",
      "      \"C\": \"By setting the corresponding elements in the QK| matrix to negative infinity for queries that follow keys.\",\n",
      "      \"D\": \"By setting the corresponding elements in the QK| matrix to negative infinity for queries that precede keys.\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is the shape of the output matrix after applying the softmax function to the scaled QK| matrix and multiplying it by V in a single attention head?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"[N x d]\",\n",
      "      \"B\": \"[N x dk]\",\n",
      "      \"C\": \"[dk x dv]\",\n",
      "      \"D\": \"[N x dv]\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"In the context of parallelizing multi-head attention, if there are 8 heads and the model dimension \\( d \\) is 512, what is the shape of the final concatenated output before the final linear projection?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"[N x 512]\",\n",
      "      \"B\": \"[N x 64]\",\n",
      "      \"C\": \"[N x 4096]\",\n",
      "      \"D\": \"[N x 8]\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Why is the self-attention mechanism considered quadratic in the length of the input sequence?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"Because each token needs to compute dot products with every other token in the sequence.\",\n",
      "      \"B\": \"Because the number of parameters in the model grows linearly with the input sequence length.\",\n",
      "      \"C\": \"Because the number of heads used in multi-head attention scales with the input sequence length.\",\n",
      "      \"D\": \"Because the embedding dimension \\( d \\) is fixed and does not change with the input sequence length.\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 53 and continuing...\n",
      "\n",
      "Processing chunk 54/57...\n",
      "\n",
      "Processing chunk 55/57...\n",
      "\n",
      "Processing chunk 56/57...\n",
      "[WARN] JSON parse failed for chunk 56\n",
      "ERROR: Expecting ',' delimiter: line 33 column 38 (char 1198)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"If the input to a transformer block is a sequence of 10 tokens each with a dimensionality of 512, what would be the dimensions of the output after passing through the multi-head attention mechanism?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"[10, 512]\",\n",
      "      \"B\": \"[512, 10]\",\n",
      "      \"C\": \"[10, 1024]\",\n",
      "      \"D\": \"[512, 512]\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Given a transformer block with a multi-head attention mechanism, if the query, key, and value matrices are transformed using weight matrices WQ, WK, and WV respectively, what is the shape of these weight matrices assuming the embedding dimension is 512?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"[512, 512]\",\n",
      "      \"B\": \"[10, 512]\",\n",
      "      \"C\": \"[512, 10]\",\n",
      "      \"D\": \"[10, 10]\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"In the context of the multi-head attention mechanism, how are the heads combined to form the final output?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"By summing the heads\",\n",
      "      \"B\": \"By concatenating the heads and then applying a linear transformation\",\n",
      "      \"C\": \"By averaging the heads\",\n",
      "      \"D\": \"By multiplying the heads\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"If the input token \"Thanks\" has an index of 5 in the vocabulary and the embedding matrix E has a shape of [10000, 512], what is the shape of the token embedding vector for \"Thanks\"?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"[1, 10000]\",\n",
      "      \"B\": \"[5, 512]\",\n",
      "      \"C\": \"[1, 512]\",\n",
      "      \"D\": \"[10000, 1]\"\n",
      "    },\n",
      "    \"answer\": \"C\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"When converting an input token string \"Thanks for all the\" into vocabulary indices, if the first three tokens are converted to indices 5, 4000, and 10532 respectively, what would be the shape of the resulting embedding matrix E before applying the token embeddings?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"[3, 512]\",\n",
      "      \"B\": \"[1, 512]\",\n",
      "      \"C\": \"[512, 3]\",\n",
      "      \"D\": \"[10000, 512]\"\n",
      "    },\n",
      "    \"answer\": \"D\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 56 and continuing...\n",
      "\n",
      "Processing chunk 57/57...\n",
      "[WARN] JSON parse failed for chunk 57\n",
      "ERROR: Invalid \\escape: line 3 column 66 (char 71)\n",
      "RAW RESPONSE:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"If the input to a transformer block is a matrix \\(X\\) of shape \\([N \\times d]\\), what is the shape of the output after applying the multi-head attention mechanism followed by LayerNorm?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"[N \\times d]\",\n",
      "      \"B\": \"[N \\times 2d]\",\n",
      "      \"C\": \"[2N \\times d]\",\n",
      "      \"D\": \"[N \\times d/2]\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"In the context of the transformer block, what does the operation \\(T1 = \\text{MultiHeadAttention}(X)\\) imply regarding the processing of the input embeddings?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"It computes the positional encodings for each token.\",\n",
      "      \"B\": \"It applies the multi-head attention mechanism to the input embeddings in parallel.\",\n",
      "      \"C\": \"It concatenates the input embeddings into a single vector.\",\n",
      "      \"D\": \"It normalizes the input embeddings.\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Given the formula \\(T5 = T4 + T3\\), where \\(T4 = \\text{FFN}(T3)\\), what does this operation signify in the transformer block?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"It adds the output of the FFN to the normalized input embeddings.\",\n",
      "      \"B\": \"It applies the FFN to the sum of the input embeddings and the FFN output.\",\n",
      "      \"C\": \"It concatenates the input embeddings with the FFN output.\",\n",
      "      \"D\": \"It subtracts the FFN output from the normalized input embeddings.\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"How are the positional embeddings incorporated into the input \\(X\\) of a transformer block?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"By directly adding them to the input token embeddings.\",\n",
      "      \"B\": \"By multiplying them with the input token embeddings.\",\n",
      "      \"C\": \"By replacing the input token embeddings with them.\",\n",
      "      \"D\": \"By concatenating them with the input token embeddings.\"\n",
      "    },\n",
      "    \"answer\": \"A\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"If the input sequence length \\(N\\) is 100 and the embedding dimension \\(d\\) is 512, what would be the dimensions of the output after passing through a transformer block with the given operations?\",\n",
      "    \"options\": {\n",
      "      \"A\": \"[100 \\times 1024]\",\n",
      "      \"B\": \"[100 \\times 512]\",\n",
      "      \"C\": \"[200 \\times 512]\",\n",
      "      \"D\": \"[100 \\times 256]\"\n",
      "    },\n",
      "    \"answer\": \"B\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Skipping chunk 57 and continuing...\n",
      "\n",
      "✅ Completed!\n",
      "   Generated: 195 MCQs\n",
      "   Skipped: 0 MCQs\n",
      "   Processed: 57 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunks = load_chunks( \"results/without_image/metdata/qween_pdf_chunks.json\")\n",
    "\n",
    "# Generate MCQs and write to CSV on the go\n",
    "total_mcqs = generate_mcqs_from_chunks(\n",
    "    chunks,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    csv_path=\"results/without_image/qween_mcq_dataset.csv\",\n",
    "    pdf_name=pdf_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b2d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salesbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
