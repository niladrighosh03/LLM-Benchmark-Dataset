{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415a6249",
   "metadata": {},
   "source": [
    "### Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3966751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_pdf_pages(pdf_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Extracts text from each page of a PDF.\n",
    "    Returns a list where each element is text from one page.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    pages = []\n",
    "\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        pages.append(text if text else \"\")\n",
    "\n",
    "    return pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7822ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_page_chunks(\n",
    "    pages: list,\n",
    "    chunk_size: int = 3,\n",
    "    overlap: int = 1\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Creates page chunks with overlap.\n",
    "    \n",
    "    Example:\n",
    "    chunk_size=3, overlap=1\n",
    "    Pages: [1,2,3] → [3,4,5] → ...\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap\n",
    "\n",
    "    chunk_number = 1\n",
    "    for start in range(0, len(pages), step):\n",
    "        end = start + chunk_size\n",
    "        chunk_pages = pages[start:end]\n",
    "\n",
    "        if not chunk_pages:\n",
    "            break\n",
    "\n",
    "        chunks.append({\n",
    "            \"chunk_number\": chunk_number,\n",
    "            \"pages\": list(range(start + 1, min(end + 1, len(pages) + 1))),\n",
    "            \"text\": \"\\n\".join(chunk_pages)\n",
    "        })\n",
    "\n",
    "        chunk_number += 1\n",
    "\n",
    "        if end >= len(pages):\n",
    "            break\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def save_chunks(chunks: list, output_path: str):\n",
    "    \"\"\"\n",
    "    Saves chunks to a JSON file.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "def load_chunks(path: str) -> list:\n",
    "    \"\"\"\n",
    "    Loads stored PDF chunks.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c08334",
   "metadata": {},
   "source": [
    "### Use LLM to generate MCQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3bcc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mcq_prompt(text: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are an expert university-level instructor and question setter.\n",
    "\n",
    "Generate high-quality multiple-choice questions (MCQs)\n",
    "STRICTLY based on the lecture content below.\n",
    "\n",
    "CRITICAL RULES (NO EXCEPTIONS):\n",
    "1. Use ONLY the provided lecture content.\n",
    "2. Each question must be conceptually meaningful and **non-generic**.\n",
    "3. Each question must be an **actual MCQ**, not a definition recall question.\n",
    "4. Avoid generic prompts such as \"What is X?\" or \"Define Y\".\n",
    "5. Questions must test **application, reasoning, interpretation, comparison, edge cases, numerical reasoning, or scenario-based understanding** explicitly grounded in the lecture.\n",
    "6. Each question must have EXACTLY 4 options.\n",
    "7. EXACTLY one option must be correct.\n",
    "8. Distractors must be **plausible and content-specific**, not obviously incorrect.\n",
    "9. DO NOT add explanations, comments, hints, or extra text.\n",
    "10. OUTPUT MUST BE VALID JSON ONLY.\n",
    "\n",
    "\n",
    "OUTPUT FORMAT (STRICT JSON ARRAY):\n",
    "[\n",
    "  {{\n",
    "    \"question\": \"question text\",\n",
    "    \"options\": {{\n",
    "      \"A\": \"option text\",\n",
    "      \"B\": \"option text\",\n",
    "      \"C\": \"option text\",\n",
    "      \"D\": \"option text\"\n",
    "    }},\n",
    "    \"answer\": \"A\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "EXAMPLE OUTPUT (FORMAT ONLY):\n",
    "[\n",
    "  {{\n",
    "    \"question\": \"Who invented Gravity?\",\n",
    "    \"options\": {{\n",
    "      \"A\": \"Isaac Newton\",\n",
    "      \"B\": \"Albert Einstein\",\n",
    "      \"C\": \"Galileo Galilei\",\n",
    "      \"D\": \"Marrie Curie\"\n",
    "    }},\n",
    "    \"answer\": \"A\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "LECTURE CONTENT:\n",
    "<<<\n",
    "{text}\n",
    ">>>\n",
    "\n",
    "RETURN ONLY THE JSON ARRAY. NO MARKDOWN. NO TEXT OUTSIDE JSON.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d468fc",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">1) Mistral 7B Instruct</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c3957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generate(tokenizer, model, prompt: str, max_tokens: int = 1024):\n",
    "    messages = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "    inputs = tokenizer(messages, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id     # deterministic output\n",
    "    )\n",
    "\n",
    "    # Slice to get only the generated tokens (exclude prompt)\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116b3f3",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">2) Flacon 7B</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c769f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"tiiuae/falcon-7b\",\n",
    "    trust_remote_code=True,\n",
    "    force_download=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tiiuae/falcon-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    force_download=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e04415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generate(tokenizer, model, prompt: str, max_tokens: int = 1024):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=False,\n",
    "        use_cache=False,                    \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508cb22",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">2) LLama 8B</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa08bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama8b model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d333889d9d9d45129cfaaf1077f6c33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ccb987dc60486fa81874ff15c2dceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e2cade1bc7460d95c400d3a4f99cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b27ca8d7b084d299cb80ce91850867d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc682be2cd8647198a78375afc7f3d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ee726028694ab18a7137d86efb4047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10dcda70e8f4994a4fdeef4e685498e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81651721f3de4b01830ac1d396e249c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432041616a0644a59e6cf63d59c112f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f37a64ed9954d8f9083e3a4b2a81e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15006c94cf764291a2ea636126fdf22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d8547b245444dab70b2eb3bb5d162d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "print(\"Loading llama8b model...\")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7a218e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generate(tokenizer, model, prompt: str, max_tokens: int = 1024):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    encoded = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = encoded.to(model.device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,   # ✅ FIX\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_ids = outputs[0][input_ids.shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e83626",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">3) Phi 14B</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e6646cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-19 22:51:14.169724: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-19 22:51:14.183787: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-19 22:51:14.200138: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-19 22:51:14.204915: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-19 22:51:14.217491: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-19 22:51:15.396428: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b374729fbaa4f4886fbfc1997bb2b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"microsoft/Phi-3-medium-4k-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "886e5068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generate(tokenizer, model, prompt: str, max_tokens: int = 1024):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,      # ✅ important\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_ids = outputs[0][input_ids.shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057a9b5",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">4) Qween 7B</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f594f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-25 21:22:24.105817: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-25 21:22:24.120781: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-25 21:22:24.182414: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-25 21:22:24.195164: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-25 21:22:24.235392: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-25 21:22:25.407314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016fac371ebf4b39b863fd69f3a8f3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50c4f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def llm_generate(tokenizer, model, prompt: str, max_tokens: int = 1024):\n",
    "    # Build chat-style messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    # Tokenize using chat template\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Attention mask (important for some models)\n",
    "    attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Extract only newly generated tokens\n",
    "    generated_ids = outputs[0][input_ids.shape[1]:]\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return response.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f68248a",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue;\">JSON Formatiing</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "952819fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def clean_json_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the LLM response to ensure it's valid JSON.\n",
    "    \"\"\"\n",
    "    # Remove markdown code fences\n",
    "    cleaned = re.sub(r\"```(?:json)?\", \"\", response).strip()\n",
    "    \n",
    "    # Try to find the JSON array list\n",
    "    start = cleaned.find(\"[\")\n",
    "    end = cleaned.rfind(\"]\")\n",
    "    \n",
    "    if start != -1 and end != -1:\n",
    "        cleaned = cleaned[start : end + 1]\n",
    "        \n",
    "    return cleaned\n",
    "\n",
    "def generate_mcqs_from_chunks(\n",
    "    chunks: list,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    csv_path: str,\n",
    "    pdf_name: str\n",
    "):\n",
    "    \"\"\"Generate MCQs and write to CSV immediately after each MCQ\"\"\"\n",
    "    \n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    # Write CSV header first\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"pdf_name\",\n",
    "            \"chunk_number\",\n",
    "            \"total_chunks\",\n",
    "            \"pages\",\n",
    "            \"question\",\n",
    "            \"option_A\",\n",
    "            \"option_B\",\n",
    "            \"option_C\",\n",
    "            \"option_D\",\n",
    "            \"correct_answer\"\n",
    "        ])\n",
    "    \n",
    "    total_mcqs = 0\n",
    "    skipped_mcqs = 0\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_number = chunk.get(\"chunk_number\", i + 1)  # Use chunk_number from JSON or fallback\n",
    "        print(f\"\\nProcessing chunk {chunk_number}/{total_chunks}...\")\n",
    "\n",
    "        try:\n",
    "            prompt = build_mcq_prompt(\n",
    "                text=chunk[\"text\"]\n",
    "            )\n",
    "\n",
    "            response = llm_generate(tokenizer, model, prompt)\n",
    "\n",
    "            # --- JSON SAFETY ---\n",
    "            try:\n",
    "                cleaned_response = clean_json_response(response)\n",
    "                mcq_list = json.loads(cleaned_response)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[WARN] JSON parse failed for chunk {chunk_number}\")\n",
    "                print(f\"ERROR: {e}\")\n",
    "                print(f\"Skipping chunk {chunk_number} and continuing...\")\n",
    "                continue\n",
    "\n",
    "            # Ensure mcq_list is actually a list\n",
    "            if not isinstance(mcq_list, list):\n",
    "                print(f\"[WARN] Response is not a list for chunk {chunk_number}\")\n",
    "                print(f\"Skipping chunk {chunk_number} and continuing...\")\n",
    "                continue\n",
    "\n",
    "            # Write each MCQ immediately to CSV\n",
    "            with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                \n",
    "                for mcq_idx, mcq in enumerate(mcq_list):\n",
    "                    try:\n",
    "                        # Validate MCQ structure\n",
    "                        if not isinstance(mcq, dict):\n",
    "                            print(f\"  [SKIP] MCQ {mcq_idx+1} is not a dict, skipping...\")\n",
    "                            skipped_mcqs += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Check required fields\n",
    "                        if \"question\" not in mcq or \"options\" not in mcq or \"answer\" not in mcq:\n",
    "                            print(f\"  [SKIP] MCQ {mcq_idx+1} missing required fields, skipping...\")\n",
    "                            skipped_mcqs += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Validate options\n",
    "                        options = mcq[\"options\"]\n",
    "                        if not isinstance(options, dict) or not all(key in options for key in [\"A\", \"B\", \"C\", \"D\"]):\n",
    "                            print(f\"  [SKIP] MCQ {mcq_idx+1} has invalid options structure, skipping...\")\n",
    "                            skipped_mcqs += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Validate answer key\n",
    "                        answer_key = mcq[\"answer\"]\n",
    "                        if answer_key not in options:\n",
    "                            print(f\"  [SKIP] MCQ {mcq_idx+1} has invalid answer key '{answer_key}', skipping...\")\n",
    "                            skipped_mcqs += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Format correct answer as \"D) Option Text\"\n",
    "                        answer_text = options[answer_key]\n",
    "                        formatted_answer = f\"{answer_key}) {answer_text}\"\n",
    "                        \n",
    "                        # Convert pages list to string\n",
    "                        pages_str = \", \".join(map(str, chunk[\"pages\"]))\n",
    "                        \n",
    "                        writer.writerow([\n",
    "                            pdf_name,\n",
    "                            chunk_number,\n",
    "                            pages_str,\n",
    "                            mcq[\"question\"],\n",
    "                            options[\"A\"],\n",
    "                            options[\"B\"],\n",
    "                            options[\"C\"],\n",
    "                            options[\"D\"],\n",
    "                            formatted_answer\n",
    "                        ])\n",
    "                        \n",
    "                        total_mcqs += 1\n",
    "                        # print(f\"  ✓ Generated MCQ #{total_mcqs}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"  [ERROR] Failed to process MCQ {mcq_idx+1}: {e}\")\n",
    "                        print(f\"  Skipping this MCQ and continuing...\")\n",
    "                        skipped_mcqs += 1\n",
    "                        continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process chunk {chunk_number}: {e}\")\n",
    "            print(f\"Skipping chunk {chunk_number} and continuing...\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✅ Completed!\")\n",
    "    print(f\"   Generated: {total_mcqs} MCQs\")\n",
    "    print(f\"   Skipped: {skipped_mcqs} MCQs\")\n",
    "    print(f\"   Processed: {total_chunks} chunks\")\n",
    "    return total_mcqs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a8e09",
   "metadata": {},
   "source": [
    "#### Inference the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "191a7298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 114 pages\n",
      "Created 57 chunks\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract pages from the PDF\n",
    "pdf_name = \"notes/LLM_cs124_week7_2025.pdf\"\n",
    "pages = extract_pdf_pages(pdf_name)\n",
    "\n",
    "# Step 2: Chunk (3 pages, 1-page overlap)\n",
    "chunks = create_page_chunks(\n",
    "    pages,\n",
    "    chunk_size=3,\n",
    "    overlap=1\n",
    ")\n",
    "\n",
    "# Step 3: Store to JSON\n",
    "save_chunks(chunks, \"results/without_image/metdata/qween_pdf_chunks.json\")\n",
    "\n",
    "print(f\"Extracted {len(pages)} pages\")\n",
    "print(f\"Created {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29c0c1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1/57...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 2/57...\n",
      "\n",
      "Processing chunk 3/57...\n",
      "\n",
      "Processing chunk 4/57...\n",
      "\n",
      "Processing chunk 5/57...\n",
      "\n",
      "Processing chunk 6/57...\n",
      "[WARN] JSON parse failed for chunk 6\n",
      "ERROR: Expecting ',' delimiter: line 13 column 143 (char 475)\n",
      "Skipping chunk 6 and continuing...\n",
      "\n",
      "Processing chunk 7/57...\n",
      "\n",
      "Processing chunk 8/57...\n",
      "\n",
      "Processing chunk 9/57...\n",
      "\n",
      "Processing chunk 10/57...\n",
      "\n",
      "Processing chunk 11/57...\n",
      "\n",
      "Processing chunk 12/57...\n",
      "\n",
      "Processing chunk 13/57...\n",
      "\n",
      "Processing chunk 14/57...\n",
      "\n",
      "Processing chunk 15/57...\n",
      "[WARN] JSON parse failed for chunk 15\n",
      "ERROR: Invalid \\escape: line 3 column 114 (char 119)\n",
      "Skipping chunk 15 and continuing...\n",
      "\n",
      "Processing chunk 16/57...\n",
      "\n",
      "Processing chunk 17/57...\n",
      "[WARN] JSON parse failed for chunk 17\n",
      "ERROR: Invalid \\escape: line 3 column 76 (char 81)\n",
      "Skipping chunk 17 and continuing...\n",
      "\n",
      "Processing chunk 18/57...\n",
      "[WARN] JSON parse failed for chunk 18\n",
      "ERROR: Invalid \\escape: line 16 column 88 (char 788)\n",
      "Skipping chunk 18 and continuing...\n",
      "\n",
      "Processing chunk 19/57...\n",
      "\n",
      "Processing chunk 20/57...\n",
      "\n",
      "Processing chunk 21/57...\n",
      "\n",
      "Processing chunk 22/57...\n",
      "[WARN] JSON parse failed for chunk 22\n",
      "ERROR: Invalid \\escape: line 3 column 92 (char 97)\n",
      "Skipping chunk 22 and continuing...\n",
      "\n",
      "Processing chunk 23/57...\n",
      "\n",
      "Processing chunk 24/57...\n",
      "\n",
      "Processing chunk 25/57...\n",
      "\n",
      "Processing chunk 26/57...\n",
      "\n",
      "Processing chunk 27/57...\n",
      "\n",
      "Processing chunk 28/57...\n",
      "\n",
      "Processing chunk 29/57...\n",
      "\n",
      "Processing chunk 30/57...\n",
      "\n",
      "Processing chunk 31/57...\n",
      "\n",
      "Processing chunk 32/57...\n",
      "\n",
      "Processing chunk 33/57...\n",
      "\n",
      "Processing chunk 34/57...\n",
      "\n",
      "Processing chunk 35/57...\n",
      "\n",
      "Processing chunk 36/57...\n",
      "\n",
      "Processing chunk 37/57...\n",
      "\n",
      "Processing chunk 38/57...\n",
      "\n",
      "Processing chunk 39/57...\n",
      "\n",
      "Processing chunk 40/57...\n",
      "\n",
      "Processing chunk 41/57...\n",
      "\n",
      "Processing chunk 42/57...\n",
      "\n",
      "Processing chunk 43/57...\n",
      "[WARN] JSON parse failed for chunk 43\n",
      "ERROR: Invalid \\escape: line 13 column 34 (char 549)\n",
      "Skipping chunk 43 and continuing...\n",
      "\n",
      "Processing chunk 44/57...\n",
      "[WARN] JSON parse failed for chunk 44\n",
      "ERROR: Invalid \\escape: line 3 column 56 (char 61)\n",
      "Skipping chunk 44 and continuing...\n",
      "\n",
      "Processing chunk 45/57...\n",
      "\n",
      "Processing chunk 46/57...\n",
      "\n",
      "Processing chunk 47/57...\n",
      "[WARN] JSON parse failed for chunk 47\n",
      "ERROR: Invalid \\escape: line 3 column 42 (char 47)\n",
      "Skipping chunk 47 and continuing...\n",
      "\n",
      "Processing chunk 48/57...\n",
      "[WARN] JSON parse failed for chunk 48\n",
      "ERROR: Invalid \\escape: line 3 column 37 (char 42)\n",
      "Skipping chunk 48 and continuing...\n",
      "\n",
      "Processing chunk 49/57...\n",
      "[WARN] JSON parse failed for chunk 49\n",
      "ERROR: Invalid \\escape: line 3 column 48 (char 55)\n",
      "Skipping chunk 49 and continuing...\n",
      "\n",
      "Processing chunk 50/57...\n",
      "[WARN] JSON parse failed for chunk 50\n",
      "ERROR: Invalid \\escape: line 3 column 47 (char 52)\n",
      "Skipping chunk 50 and continuing...\n",
      "\n",
      "Processing chunk 51/57...\n",
      "\n",
      "Processing chunk 52/57...\n",
      "\n",
      "Processing chunk 53/57...\n",
      "\n",
      "Processing chunk 54/57...\n",
      "\n",
      "Processing chunk 55/57...\n",
      "\n",
      "Processing chunk 56/57...\n",
      "[WARN] JSON parse failed for chunk 56\n",
      "ERROR: Invalid \\escape: line 13 column 61 (char 381)\n",
      "Skipping chunk 56 and continuing...\n",
      "\n",
      "Processing chunk 57/57...\n",
      "\n",
      "✅ Completed!\n",
      "   Generated: 176 MCQs\n",
      "   Skipped: 0 MCQs\n",
      "   Processed: 57 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunks = load_chunks( \"results/without_image/metdata/qween_pdf_chunks.json\")\n",
    "\n",
    "# Generate MCQs and write to CSV on the go\n",
    "total_mcqs = generate_mcqs_from_chunks(\n",
    "    chunks,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    csv_path=\"results/without_image/qween_mcq_dataset.csv\",\n",
    "    pdf_name=pdf_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b2d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salesbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
